Topic,Count,Name,Representation,KeyBERT,Flan-T5,Llama2,Representative_Docs
-1,24377,-1_data_learning_model_models,"['data', 'learning', 'model', 'models', 'based', 'neural', 'training', 'network', 'networks', 'deep']","['neural network', 'neural networks', 'deep learning', 'convolutional', 'networks', 'neural', 'models', 'machine learning', 'model', 'representations']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['A novel architecture for deep convolutional neural networks', '', '', '', '', '', '', '', '', '']","['  The history of deep learning has shown that human-designed problem-specific\nnetworks can greatly improve the classification performance of general neural\nmodels. In most practical cases, however, choosing the optimal architecture for\na given task remains a challenging problem. Recent architecture-search methods\nare able to automatically build neural models with strong performance but fail\nto fully appreciate the interaction between neural architecture and weights.\nThis work investigates the problem of disentangling the role of the neural\nstructure and its edge weights, by showing that well-trained architectures may\nnot need any link-specific fine-tuning of the weights. We compare the\nperformance of such weight-free networks (in our case these are binary networks\nwith {0, 1}-valued weights) with random, weight-agnostic, pruned and standard\nfully connected networks. To find the optimal weight-agnostic network, we use a\nnovel and computationally efficient method that translates the hard\narchitecture-search problem into a feasible optimization problem.More\nspecifically, we look at the optimal task-specific architectures as the optimal\nconfiguration of binary networks with {0, 1}-valued weights, which can be found\nthrough an approximate gradient descent strategy. Theoretical convergence\nguarantees of the proposed algorithm are obtained by bounding the error in the\ngradient approximation and its practical performance is evaluated on two\nreal-world data sets. For measuring the structural similarities between\ndifferent architectures, we use a novel spectral approach that allows us to\nunderline the intrinsic differences between real-valued networks and\nweight-free architectures.\n', '  Transfer learning for feature extraction can be used to exploit deep\nrepresentations in contexts where there is very few training data, where there\nare limited computational resources, or when tuning the hyper-parameters needed\nfor training is not an option. While previous contributions to feature\nextraction propose embeddings based on a single layer of the network, in this\npaper we propose a full-network embedding which successfully integrates\nconvolutional and fully connected features, coming from all layers of a deep\nconvolutional neural network. To do so, the embedding normalizes features in\nthe context of the problem, and discretizes their values to reduce noise and\nregularize the embedding space. Significantly, this also reduces the\ncomputational cost of processing the resultant representations. The proposed\nmethod is shown to outperform single layer embeddings on several image\nclassification tasks, while also being more robust to the choice of the\npre-trained model used for obtaining the initial features. The performance gap\nin classification accuracy between thoroughly tuned solutions and the\nfull-network embedding is also reduced, which makes of the proposed approach a\ncompetitive solution for a large set of applications.\n', '  Recently, with convolutional neural networks gaining significant achievements\nin many challenging machine learning fields, hand-crafted neural networks no\nlonger satisfy our requirements as designing a network will cost a lot, and\nautomatically generating architectures has attracted increasingly more\nattention and focus. Some research on auto-generated networks has achieved\npromising results. However, they mainly aim at picking a series of single\nlayers such as convolution or pooling layers one by one. There are many elegant\nand creative designs in the carefully hand-crafted neural networks, such as\nInception-block in GoogLeNet, residual block in residual network and dense\nblock in dense convolutional network. Based on reinforcement learning and\ntaking advantages of the superiority of these networks, we propose a novel\nautomatic process to design a multi-block neural network, whose architecture\ncontains multiple types of blocks mentioned above, with the purpose to do\nstructure learning of deep neural networks and explore the possibility whether\ndifferent blocks can be composed together to form a well-behaved neural\nnetwork. The optimal network is created by the Q-learning agent who is trained\nto sequentially pick different types of blocks. To verify the validity of our\nproposed method, we use the auto-generated multi-block neural network to\nconduct experiments on image benchmark datasets MNIST, SVHN and CIFAR-10 image\nclassification task with restricted computational resources. The results\ndemonstrate that our method is very effective, achieving comparable or better\nperformance than hand-crafted networks and advanced auto-generated neural\nnetworks.\n']"
0,6727,0_policy_reinforcement_reinforcement learning_learning,"['policy', 'reinforcement', 'reinforcement learning', 'learning', 'rl', 'agent', 'control', 'agents', 'reward', 'policies']","['reinforcement learning', 'policy gradient', 'imitation learning', 'learning rl', 'markov decision', 'deep reinforcement', 'multi agent', 'model based', 'dynamics', 'reinforcement']","['Reinforcement Learning', '', '', '', '', '', '', '', '', '']","['Reinforcement Learning', '', '', '', '', '', '', '', '', '']","[""  Reinforcement learning (RL) methods often rely on massive exploration data to\nsearch optimal policies, and suffer from poor sampling efficiency. This paper\npresents a mixed reinforcement learning (mixed RL) algorithm by simultaneously\nusing dual representations of environmental dynamics to search the optimal\npolicy with the purpose of improving both learning accuracy and training speed.\nThe dual representations indicate the environmental model and the state-action\ndata: the former can accelerate the learning process of RL, while its inherent\nmodel uncertainty generally leads to worse policy accuracy than the latter,\nwhich comes from direct measurements of states and actions. In the framework\ndesign of the mixed RL, the compensation of the additive stochastic model\nuncertainty is embedded inside the policy iteration RL framework by using\nexplored state-action data via iterative Bayesian estimator (IBE). The optimal\npolicy is then computed in an iterative way by alternating between policy\nevaluation (PEV) and policy improvement (PIM). The convergence of the mixed RL\nis proved using the Bellman's principle of optimality, and the recursive\nstability of the generated policy is proved via the Lyapunov's direct method.\nThe effectiveness of the mixed RL is demonstrated by a typical optimal control\nproblem of stochastic non-affine nonlinear systems (i.e., double lane change\ntask with an automated vehicle).\n"", ""  Reinforcement Learning (RL) based methods have seen their paramount successes\nin solving serial decision-making and control problems in recent years. For\nconventional RL formulations, Markov Decision Process (MDP) and\nstate-action-value function are the basis for the problem modeling and policy\nevaluation. However, several challenging issues still remain. Among most cited\nissues, the enormity of state/action space is an important factor that causes\ninefficiency in accurately approximating the state-action-value function. We\nobserve that although actions directly define the agents' behaviors, for many\nproblems the next state after a state transition matters more than the action\ntaken, in determining the return of such a state transition. In this regard, we\npropose a new learning paradigm, State Action Separable Reinforcement Learning\n(sasRL), wherein the action space is decoupled from the value function learning\nprocess for higher efficiency. Then, a light-weight transition model is learned\nto assist the agent to determine the action that triggers the associated state\ntransition. In addition, our convergence analysis reveals that under certain\nconditions, the convergence time of sasRL is $O(T^{1/k})$, where $T$ is the\nconvergence time for updating the value function in the MDP-based formulation\nand $k$ is a weighting factor. Experiments on several gaming scenarios show\nthat sasRL outperforms state-of-the-art MDP-based RL algorithms by up to\n$75\\%$.\n"", ""  Model-free deep reinforcement learning (RL) methods have been successful in a\nwide variety of simulated domains. However, a major obstacle facing deep RL in\nthe real world is their high sample complexity. Batch policy gradient methods\noffer stable learning, but at the cost of high variance, which often requires\nlarge batches. TD-style methods, such as off-policy actor-critic and\nQ-learning, are more sample-efficient but biased, and often require costly\nhyperparameter sweeps to stabilize. In this work, we aim to develop methods\nthat combine the stability of policy gradients with the efficiency of\noff-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor\nexpansion of the off-policy critic as a control variate. Q-Prop is both sample\nefficient and stable, and effectively combines the benefits of on-policy and\noff-policy methods. We analyze the connection between Q-Prop and existing\nmodel-free algorithms, and use control variate theory to derive two variants of\nQ-Prop with conservative and aggressive adaptation. We show that conservative\nQ-Prop provides substantial gains in sample efficiency over trust region policy\noptimization (TRPO) with generalized advantage estimation (GAE), and improves\nstability over deep deterministic policy gradient (DDPG), the state-of-the-art\non-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control\nenvironments.\n""]"
1,2456,1_speech_audio_speaker_music,"['speech', 'audio', 'speaker', 'music', 'acoustic', 'recognition', 'asr', 'model', 'speech recognition', 'end']","['speech enhancement', 'speech recognition', 'automatic speech', 'speaker verification', 'encoder', 'convolutional', 'neural networks', 'neural network', 'voice', 'speech']","['A.W.A.E.A.S.A.', '', '', '', '', '', '', '', '', '']","['Speech enhancement', '', '', '', '', '', '', '', '', '']","['  End-to-end models for robust automatic speech recognition (ASR) have not been\nsufficiently well-explored in prior work. With end-to-end models, one could\nchoose to preprocess the input speech using speech enhancement techniques and\ntrain the model using enhanced speech. Another alternative is to pass the noisy\nspeech as input and modify the model architecture to adapt to noisy speech. A\nsystematic comparison of these two approaches for end-to-end robust ASR has not\nbeen attempted before. We address this gap and present a detailed comparison of\nspeech enhancement-based techniques and three different model-based adaptation\ntechniques covering data augmentation, multi-task learning, and adversarial\nlearning for robust ASR. While adversarial learning is the best-performing\ntechnique on certain noise types, it comes at the cost of degrading clean\nspeech WER. On other relatively stationary noise types, a new speech\nenhancement technique outperformed all the model-based adaptation techniques.\nThis suggests that knowledge of the underlying noise type can meaningfully\ninform the choice of adaptation technique.\n', ""  This paper proposes speaker-adaptive neural vocoders for parametric\ntext-to-speech (TTS) systems. Recently proposed WaveNet-based neural vocoding\nsystems successfully generate a time sequence of speech signal with an\nautoregressive framework. However, it remains a challenge to synthesize\nhigh-quality speech when the amount of a target speaker's training data is\ninsufficient. To generate more natural speech signals with the constraint of\nlimited training data, we propose a speaker adaptation task with an effective\nvariation of neural vocoding models. In the proposed method, a\nspeaker-independent training method is applied to capture universal attributes\nembedded in multiple speakers, and the trained model is then optimized to\nrepresent the specific characteristics of the target speaker. Experimental\nresults verify that the proposed TTS systems with speaker-adaptive neural\nvocoders outperform those with traditional source-filter model-based vocoders\nand those with WaveNet vocoders, trained either speaker-dependently or\nspeaker-independently. In particular, our TTS system achieves 3.80 and 3.77 MOS\nfor the Korean male and Korean female speakers, respectively, even though we\nuse only ten minutes' speech corpus for training the model.\n"", '  Variational auto-encoders (VAEs) are deep generative latent variable models\nthat can be used for learning the distribution of complex data. VAEs have been\nsuccessfully used to learn a probabilistic prior over speech signals, which is\nthen used to perform speech enhancement. One advantage of this generative\napproach is that it does not require pairs of clean and noisy speech signals at\ntraining. In this paper, we propose audio-visual variants of VAEs for\nsingle-channel and speaker-independent speech enhancement. We develop a\nconditional VAE (CVAE) where the audio speech generative process is conditioned\non visual information of the lip region. At test time, the audio-visual speech\ngenerative model is combined with a noise model based on nonnegative matrix\nfactorization, and speech enhancement relies on a Monte Carlo\nexpectation-maximization algorithm. Experiments are conducted with the recently\npublished NTCD-TIMIT dataset as well as the GRID corpus. The results confirm\nthat the proposed audio-visual CVAE effectively fuses audio and visual\ninformation, and it improves the speech enhancement performance compared with\nthe audio-only VAE model, especially when the speech signal is highly corrupted\nby noise. We also show that the proposed unsupervised audio-visual speech\nenhancement approach outperforms a state-of-the-art supervised deep learning\nmethod.\n']"
2,1785,2_label_labels_classification_multi label,"['label', 'labels', 'classification', 'multi label', 'data', 'classifier', 'learning', 'classifiers', 'class', 'ensemble']","['label learning', 'label classification', 'supervised learning', 'multi label', 'semi supervised', 'supervised', 'labeled data', 'random forest', 'classifiers', 'machine learning']","['Multi label classification', '', '', '', '', '', '', '', '', '']","['Multi-label classification', '', '', '', '', '', '', '', '', '']","['  In multi-label classification, where the evaluation of predictions is less\nstraightforward than in single-label classification, various meaningful, though\ndifferent, loss functions have been proposed. Ideally, the learning algorithm\nshould be customizable towards a specific choice of the performance measure.\nModern implementations of boosting, most prominently gradient boosted decision\ntrees, appear to be appealing from this point of view. However, they are mostly\nlimited to single-label classification, and hence not amenable to multi-label\nlosses unless these are label-wise decomposable. In this work, we develop a\ngeneralization of the gradient boosting framework to multi-output problems and\npropose an algorithm for learning multi-label classification rules that is able\nto minimize decomposable as well as non-decomposable loss functions. Using the\nwell-known Hamming loss and subset 0/1 loss as representatives, we analyze the\nabilities and limitations of our approach on synthetic data and evaluate its\npredictive performance on multi-label benchmarks.\n', '  Multi-label classification (MLC) is a generalization of standard\nclassification where multiple labels may be assigned to a given sample. In the\nreal world, it is more common to deal with noisy datasets than clean datasets,\ngiven how modern datasets are labeled by a large group of annotators on\ncrowdsourcing platforms, but little attention has been given to evaluating\nmulti-label classifiers with noisy labels. Exploiting label correlations now\nbecomes a standard component of a multi-label classifier to achieve competitive\nperformance. However, this component makes the classifier more prone to poor\ngeneralization - it overfits labels as well as label dependencies. We identify\nthree common real-world label noise scenarios and show how previous approaches\nper-form poorly with noisy labels. To address this issue, we present a\nContext-Based Multi-LabelClassifier (CbMLC) that effectively handles noisy\nlabels when learning label dependencies, without requiring additional\nsupervision. We compare CbMLC against other domain-specific state-of-the-art\nmodels on a variety of datasets, under both the clean and the noisy settings.\nWe show CbMLC yields substantial improvements over the previous methods in most\ncases.\n', '  Multi-label learning deals with the classification problems where each\ninstance can be assigned with multiple labels simultaneously. Conventional\nmulti-label learning approaches mainly focus on exploiting label correlations.\nIt is usually assumed, explicitly or implicitly, that the label sets for\ntraining instances are fully labeled without any missing labels. However, in\nmany real-world multi-label datasets, the label assignments for training\ninstances can be incomplete. Some ground-truth labels can be missed by the\nlabeler from the label set. This problem is especially typical when the number\ninstances is very large, and the labeling cost is very high, which makes it\nalmost impossible to get a fully labeled training set. In this paper, we study\nthe problem of large-scale multi-label learning with incomplete label\nassignments. We propose an approach, called MPU, based upon positive and\nunlabeled stochastic gradient descent and stacked models. Unlike prior works,\nour method can effectively and efficiently consider missing labels and label\ncorrelations simultaneously, and is very scalable, that has linear time\ncomplexities over the size of the data. Extensive experiments on two real-world\nmulti-label datasets show that our MPU model consistently outperform other\ncommonly-used baselines.\n']"
3,1611,3_segmentation_images_image_medical,"['segmentation', 'images', 'image', 'medical', 'imaging', 'cancer', 'mri', 'tumor', 'deep', 'ct']","['image segmentation', 'segmentation', 'convolutional neural', 'mri', 'medical imaging', 'deep learning', 'convolutional', 'imaging', 'medical images', 'medical image']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['A novel architecture for deep learning for medical image segmentation', '', '', '', '', '', '', '', '', '']","['  Medical image segmentation is routinely performed to isolate regions of\ninterest, such as organs and lesions. Currently, deep learning is the state of\nthe art for automatic segmentation, but is usually limited by the need for\nsupervised training with large datasets that have been manually segmented by\ntrained clinicians. The goal of semi-superised and unsupervised image\nsegmentation is to greatly reduce, or even eliminate, the need for training\ndata and therefore to minimze the burden on clinicians when training\nsegmentation models. To this end we introduce a novel network architecture for\ncapable of unsupervised and semi-supervised image segmentation called\nTricycleGAN. This approach uses three generative models to learn translations\nbetween medical images and segmentation maps using edge maps as an intermediate\nstep. Distinct from other approaches based on generative networks, TricycleGAN\nrelies on shape priors rather than colour and texture priors. As such, it is\nparticularly well-suited for several domains of medical imaging, such as\nultrasound imaging, where commonly used visual cues may be absent. We present\nexperiments with TricycleGAN on a clinical dataset of kidney ultrasound images\nand the benchmark ISIC 2018 skin lesion dataset.\n', ""  Multi-modality is widely used in medical imaging, because it can provide\nmultiinformation about a target (tumor, organ or tissue). Segmentation using\nmultimodality consists of fusing multi-information to improve the segmentation.\nRecently, deep learning-based approaches have presented the state-of-the-art\nperformance in image classification, segmentation, object detection and\ntracking tasks. Due to their self-learning and generalization ability over\nlarge amounts of data, deep learning recently has also gained great interest in\nmulti-modal medical image segmentation. In this paper, we give an overview of\ndeep learning-based approaches for multi-modal medical image segmentation task.\nFirstly, we introduce the general principle of deep learning and multi-modal\nmedical image segmentation. Secondly, we present different deep learning\nnetwork architectures, then analyze their fusion strategies and compare their\nresults. The earlier fusion is commonly used, since it's simple and it focuses\non the subsequent segmentation network architecture. However, the later fusion\ngives more attention on fusion strategy to learn the complex relationship\nbetween different modalities. In general, compared to the earlier fusion, the\nlater fusion can give more accurate result if the fusion method is effective\nenough. We also discuss some common problems in medical image segmentation.\nFinally, we summarize and provide some perspectives on the future research.\n"", '  Creating large scale high-quality annotations is a known challenge in medical\nimaging. In this work, based on the CycleGAN algorithm, we propose leveraging\nannotations from one modality to be useful in other modalities. More\nspecifically, the proposed algorithm creates highly realistic synthetic CT\nimages (SynCT) from prostate MR images using unpaired data sets. By using SynCT\nimages (without segmentation labels) and MR images (with segmentation labels\navailable), we have trained a deep segmentation network for precise delineation\nof prostate from real CT scans. For the generator in our CycleGAN, the cycle\nconsistency term is used to guarantee that SynCT shares the identical\nmanually-drawn, high-quality masks originally delineated on MR images. Further,\nwe introduce a cost function based on structural similarity index (SSIM) to\nimprove the anatomical similarity between real and synthetic images. For\nsegmentation followed by the SynCT generation from CycleGAN, automatic\ndelineation is achieved through a 2.5D Residual U-Net. Quantitative evaluation\ndemonstrates comparable segmentation results between our SynCT and radiologist\ndrawn masks for real CT images, solving an important problem in medical image\nsegmentation field when ground truth annotations are not available for the\nmodality of interest.\n']"
4,1602,4_adversarial_attacks_adversarial examples_attack,"['adversarial', 'attacks', 'adversarial examples', 'attack', 'robustness', 'examples', 'perturbations', 'adversarial attacks', 'defense', 'adversarial training']","['adversarial training', 'adversarial perturbations', 'adversarial examples', 'adversarial attack', 'adversarial attacks', 'adversarial', 'adversarial example', 'adversarial robustness', 'vulnerable adversarial', 'adversarial samples']","['A adversarial attack', '', '', '', '', '', '', '', '', '']","['A adversarial example', '', '', '', '', '', '', '', '', '']","['  Deep Neural Networks (DNNs) have recently led to significant improvements in\nmany fields. However, DNNs are vulnerable to adversarial examples which are\nsamples with imperceptible perturbations while dramatically misleading the\nDNNs. Moreover, adversarial examples can be used to perform an attack on\nvarious kinds of DNN based systems, even if the adversary has no access to the\nunderlying model. Many defense methods have been proposed, such as obfuscating\ngradients of the networks or detecting adversarial examples. However it is\nproved out that these defense methods are not effective or cannot resist\nsecondary adversarial attacks. In this paper, we point out that steganalysis\ncan be applied to adversarial examples detection, and propose a method to\nenhance steganalysis features by estimating the probability of modifications\ncaused by adversarial attacks. Experimental results show that the proposed\nmethod can accurately detect adversarial examples. Moreover, secondary\nadversarial attacks cannot be directly performed to our method because our\nmethod is not based on a neural network but based on high-dimensional\nartificial features and FLD (Fisher Linear Discriminant) ensemble.\n', '  Recent works have demonstrated convolutional neural networks are vulnerable\nto adversarial examples, i.e., inputs to machine learning models that an\nattacker has intentionally designed to cause the models to make a mistake. To\nimprove the adversarial robustness of neural networks, adversarial training has\nbeen proposed to train networks by injecting adversarial examples into the\ntraining data. However, adversarial training could overfit to a specific type\nof adversarial attack and also lead to standard accuracy drop on clean images.\nTo this end, we propose a novel Class-Aware Domain Adaptation (CADA) method for\nadversarial defense without directly applying adversarial training.\nSpecifically, we propose to learn domain-invariant features for adversarial\nexamples and clean images via a domain discriminator. Furthermore, we introduce\na class-aware component into the discriminator to increase the discriminative\npower of the network for adversarial examples. We evaluate our newly proposed\napproach using multiple benchmark datasets. The results demonstrate that our\nmethod can significantly improve the state-of-the-art of adversarial robustness\nfor various attacks and maintain high performances on clean images.\n', '  Most of the works follow such definition of adversarial example that is\nimperceptible to humans but can fool the deep neural networks (DNNs). Some\nworks find another interesting form of adversarial examples such as one which\nis unrecognizable to humans, but DNNs classify it as one class with high\nconfidence and adversarial patch. Based on this phenomenon, in this paper, from\nthe perspective of cognition of humans and machines, we propose a new\ndefinition of adversarial examples. We show that imperceptible adversarial\nexamples, unrecognizable adversarial examples, and adversarial patches are\nderivates of generalized adversarial examples. Then, we propose three types of\nadversarial attacks based on the generalized definition. Finally, we propose a\ndefence mechanism that achieves state-of-the-art performance. We construct a\nlossy compression function to filter out the redundant features generated by\nthe network. In this process, the perturbation produced by the attacker will be\nfiltered out. Therefore, the defence mechanism can effectively improve the\nrobustness of the model. The experiments show that our attack methods can\neffectively generate adversarial examples, and our defence method can\nsignificantly improve the adversarial robustness of DNNs compared with\nadversarial training. As far as we know, our defending method achieves the best\nperformance even though we do not adopt adversarial training.\n']"
5,1532,5_traffic_driving_vehicle_road,"['traffic', 'driving', 'vehicle', 'road', 'vehicles', 'autonomous', 'prediction', 'trajectory', 'urban', 'data']","['traffic flow', 'traffic', 'trajectory prediction', 'road network', 'autonomous driving', 'autonomous vehicles', 'self driving', 'trajectories', 'driving', 'reinforcement learning']","['Traffic', '', '', '', '', '', '', '', '', '']","['Traffic', '', '', '', '', '', '', '', '', '']","['  Nowadays, in developing countries including Iran, the number of vehicles is\nincreasing due to growing population. This has recently led to waste time\ngetting stuck in traffic, take more time for daily commute, and increase\naccidents. So it is necessary to control traffic congestion by traffic police\nofficers, expand paths efficiently and choose the best way for decreasing the\ntraffic by citizens. Therefore, it is important to have the knowledge of\ninstant traffic in each lane. Todays, many traffic organization services such\nas traffic police officer and urban traffic control system use traffic cameras,\ninductive sensors, satellite images, radar sensors, ultrasonic technology and\nradio-frequency identification (RFID) for urban traffic diagnosis. But this\nmethod has some problems such as inefficiency in heavy traffic influenced by\ncondition of the air and inability to detect parallel traffic. Our method\nsuggested in this article detects traffic congestion based on IOT containing a\nsmart system that gives us traffic congestion by calculating the air pollution\namount in that area. According to conducted experiment, the results were\nsatisfied.\n', ""  Urban traffic systems worldwide are suffering from severe traffic safety\nproblems. Traffic safety is affected by many complex factors, and heavily\nrelated to all drivers' behaviors involved in traffic system. Drivers with\naggressive driving behaviors increase the risk of traffic accidents. In order\nto manage the safety level of traffic system, we propose Driving Safety Credit\ninspired by credit score in financial security field, and design a scoring\nmethod using trajectory data and violation records. First, we extract driving\nhabits, aggressive driving behaviors and traffic violation behaviors from\ndriver's trajectories and traffic violation records. Next, we train a\nclassification model to filtered out irrelevant features. And at last, we score\neach driver with selected features. We verify our proposed scoring method using\n40 days of traffic simulation, and proves the effectiveness of our scoring\nmethod.\n"", '  Traditional traffic optimization solutions assume that the graph structure of\nroad networks is static, missing opportunities for further traffic flow\noptimization. We are interested in optimizing traffic flows as a new type of\ngraph-based problem, where the graph structure of a road network can adapt to\ntraffic conditions in real time. In particular, we focus on the dynamic\nconfiguration of traffic-lane directions, which can help balance the usage of\ntraffic lanes in opposite directions. The rise of connected autonomous vehicles\noffers an opportunity to apply this type of dynamic traffic optimization at a\nlarge scale. The existing techniques for optimizing lane-directions are however\nnot suitable for dynamic traffic environments due to their high computational\ncomplexity and the static nature.\n  In this paper, we propose an efficient traffic optimization solution, called\nCoordinated Learning-based Lane Allocation (CLLA), which is suitable for\ndynamic configuration of lane-directions. CLLA consists of a two-layer\nmulti-agent architecture, where the bottom-layer agents use a machine learning\ntechnique to find a suitable configuration of lane-directions around individual\nroad intersections. The lane-direction changes proposed by the learning agents\nare then coordinated at a higher level to reduce the negative impact of the\nchanges on other parts of the road network. Our experimental results show that\nCLLA can reduce the average travel time significantly in congested road\nnetworks. We believe our method is general enough to be applied to other types\nof networks as well.\n']"
6,1330,6_protein_molecular_drug_molecules,"['protein', 'molecular', 'drug', 'molecules', 'chemical', 'prediction', 'graph', 'data', 'cancer', 'gene']","['drug discovery', 'molecular', 'ligand', 'drug', 'molecules', 'computational', 'compounds', 'molecule', 'neural network', 'neural networks']","['CogMol', '', '', '', '', '', '', '', '', '']","['CogMol (software)', '', '', '', '', '', '', '', '', '']","['  Recently, deep generative models for molecular graphs are gaining more and\nmore attention in the field of de novo drug design. A variety of models have\nbeen developed to generate topological structures of drug-like molecules, but\nexplorations in generating three-dimensional structures are still limited.\nExisting methods have either focused on low molecular weight compounds without\nconsidering drug-likeness or generate 3D structures indirectly using atom\ndensity maps. In this work, we introduce Ligand Neural Network (L-Net), a novel\ngraph generative model for designing drug-like molecules with high-quality 3D\nstructures. L-Net directly outputs the topological and 3D structure of\nmolecules (including hydrogen atoms), without the need for additional atom\nplacement or bond order inference algorithm. The architecture of L-Net is\nspecifically optimized for drug-like molecules, and a set of metrics is\nassembled to comprehensively evaluate its performance. The results show that\nL-Net is capable of generating chemically correct, conformationally valid, and\nhighly druglike molecules. Finally, to demonstrate its potential in\nstructure-based molecular design, we combine L-Net with MCTS and test its\nability to generate potential inhibitors targeting ABL1 kinase.\n', '  Learning accurate drug representation is essential for tasks such as\ncomputational drug repositioning and prediction of drug side-effects. A drug\nhierarchy is a valuable source that encodes human knowledge of drug relations\nin a tree-like structure where drugs that act on the same organs, treat the\nsame disease, or bind to the same biological target are grouped together.\nHowever, its utility in learning drug representations has not yet been\nexplored, and currently described drug representations cannot place novel\nmolecules in a drug hierarchy. Here, we develop a semi-supervised drug\nembedding that incorporates two sources of information: (1) underlying chemical\ngrammar that is inferred from molecular structures of drugs and drug-like\nmolecules (unsupervised), and (2) hierarchical relations that are encoded in an\nexpert-crafted hierarchy of approved drugs (supervised). We use the Variational\nAuto-Encoder (VAE) framework to encode the chemical structures of molecules and\nuse the knowledge-based drug-drug similarity to induce the clustering of drugs\nin hyperbolic space. The hyperbolic space is amenable for encoding hierarchical\nconcepts. Both quantitative and qualitative results support that the learned\ndrug embedding can accurately reproduce the chemical structure and induce the\nhierarchical relations among drugs. Furthermore, our approach can infer the\npharmacological properties of novel molecules by retrieving similar drugs from\nthe embedding space. We demonstrate that the learned drug embedding can be used\nto find new uses for existing drugs and to discover side-effects. We show that\nit significantly outperforms baselines in both tasks.\n', '  The novel nature of SARS-CoV-2 calls for the development of efficient de novo\ndrug design approaches. In this study, we propose an end-to-end framework,\nnamed CogMol (Controlled Generation of Molecules), for designing new drug-like\nsmall molecules targeting novel viral proteins with high affinity and\noff-target selectivity. CogMol combines adaptive pre-training of a molecular\nSMILES Variational Autoencoder (VAE) and an efficient multi-attribute\ncontrolled sampling scheme that uses guidance from attribute predictors trained\non latent features. To generate novel and optimal drug-like molecules for\nunseen viral targets, CogMol leverages a protein-molecule binding affinity\npredictor that is trained using SMILES VAE embeddings and protein sequence\nembeddings learned unsupervised from a large corpus. CogMol framework is\napplied to three SARS-CoV-2 target proteins: main protease, receptor-binding\ndomain of the spike protein, and non-structural protein 9 replicase. The\ngenerated candidates are novel at both molecular and chemical scaffold levels\nwhen compared to the training data. CogMol also includes insilico screening for\nassessing toxicity of parent molecules and their metabolites with a multi-task\ntoxicity classifier, synthetic feasibility with a chemical retrosynthesis\npredictor, and target structure binding with docking simulations. Docking\nreveals favorable binding of generated molecules to the target protein\nstructure, where 87-95 % of high affinity molecules showed docking free energy\n< -6 kcal/mol. When compared to approved drugs, the majority of designed\ncompounds show low parent molecule and metabolite toxicity and high synthetic\nfeasibility. In summary, CogMol handles multi-constraint design of\nsynthesizable, low-toxic, drug-like molecules with high target specificity and\nselectivity, and does not need target-dependent fine-tuning of the framework or\ntarget structure information.\n']"
7,1328,7_networks_neural_neural networks_relu,"['networks', 'neural', 'neural networks', 'relu', 'network', 'deep', 'functions', 'layer', 'activation', 'function']","['neural networks', 'neural network', 'deep networks', 'relu networks', 'layer neural', 'gradient descent', 'deep neural', 'deep learning', 'dnns', 'regularization']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['Deep neural networks', '', '', '', '', '', '', '', '', '']","['  We study the training and generalization of deep neural networks (DNNs) in\nthe over-parameterized regime, where the network width (i.e., number of hidden\nnodes per layer) is much larger than the number of training data points. We\nshow that, the expected $0$-$1$ loss of a wide enough ReLU network trained with\nstochastic gradient descent (SGD) and random initialization can be bounded by\nthe training loss of a random feature model induced by the network gradient at\ninitialization, which we call a neural tangent random feature (NTRF) model. For\ndata distributions that can be classified by NTRF model with sufficiently small\nerror, our result yields a generalization error bound in the order of\n$\\tilde{\\mathcal{O}}(n^{-1/2})$ that is independent of the network width. Our\nresult is more general and sharper than many existing generalization error\nbounds for over-parameterized neural networks. In addition, we establish a\nstrong connection between our generalization error bound and the neural tangent\nkernel (NTK) proposed in recent work.\n', '  Neural networks with REctified Linear Unit (ReLU) activation functions\n(a.k.a. ReLU networks) have achieved great empirical success in various\ndomains. Nonetheless, existing results for learning ReLU networks either pose\nassumptions on the underlying data distribution being e.g. Gaussian, or require\nthe network size and/or training size to be sufficiently large. In this\ncontext, the problem of learning a two-layer ReLU network is approached in a\nbinary classification setting, where the data are linearly separable and a\nhinge loss criterion is adopted. Leveraging the power of random noise\nperturbation, this paper presents a novel stochastic gradient descent (SGD)\nalgorithm, which can \\emph{provably} train any single-hidden-layer ReLU network\nto attain global optimality, despite the presence of infinitely many bad local\nminima, maxima, and saddle points in general. This result is the first of its\nkind, requiring no assumptions on the data distribution, training/network size,\nor initialization. Convergence of the resultant iterative algorithm to a global\nminimum is analyzed by establishing both an upper bound and a lower bound on\nthe number of non-zero updates to be performed. Moreover, generalization\nguarantees are developed for ReLU networks trained with the novel SGD\nleveraging classic compression bounds. These guarantees highlight a key\ndifference (at least in the worst case) between reliably learning a ReLU\nnetwork as well as a leaky ReLU network in terms of sample complexity.\nNumerical tests using both synthetic data and real images validate the\neffectiveness of the algorithm and the practical merits of the theory.\n', '  Deep neural networks (DNNs) have demonstrated dominating performance in many\nfields; since AlexNet, networks used in practice are going wider and deeper. On\nthe theoretical side, a long line of works has been focusing on training neural\nnetworks with one hidden layer. The theory of multi-layer networks remains\nlargely unsettled.\n  In this work, we prove why stochastic gradient descent (SGD) can find\n$\\textit{global minima}$ on the training objective of DNNs in\n$\\textit{polynomial time}$. We only make two assumptions: the inputs are\nnon-degenerate and the network is over-parameterized. The latter means the\nnetwork width is sufficiently large: $\\textit{polynomial}$ in $L$, the number\nof layers and in $n$, the number of samples.\n  Our key technique is to derive that, in a sufficiently large neighborhood of\nthe random initialization, the optimization landscape is almost-convex and\nsemi-smooth even with ReLU activations. This implies an equivalence between\nover-parameterized neural networks and neural tangent kernel (NTK) in the\nfinite (and polynomial) width setting.\n  As concrete examples, starting from randomly initialized weights, we prove\nthat SGD can attain 100% training accuracy in classification tasks, or minimize\nregression loss in linear convergence speed, with running time polynomial in\n$n,L$. Our theory applies to the widely-used but non-smooth ReLU activation,\nand to any smooth and possibly non-convex loss functions. In terms of network\narchitectures, our theory at least applies to fully-connected neural networks,\nconvolutional neural networks (CNN), and residual neural networks (ResNet).\n']"
8,1281,8_graph_node_graphs_nodes,"['graph', 'node', 'graphs', 'nodes', 'gnns', 'embedding', 'networks', 'network', 'graph neural', 'gnn']","['graph embedding', 'graph convolutional', 'network embedding', 'graph convolution', 'networks gnns', 'graph neural', 'graph representation', 'node embeddings', 'node representations', 'graph classification']","['graph neural networks', '', '', '', '', '', '', '', '', '']","['graph neural network', '', '', '', '', '', '', '', '', '']","[""  Graph data widely exist in many high-impact applications. Inspired by the\nsuccess of deep learning in grid-structured data, graph neural network models\nhave been proposed to learn powerful node-level or graph-level representation.\nHowever, most of the existing graph neural networks suffer from the following\nlimitations: (1) there is limited analysis regarding the graph convolution\nproperties, such as seed-oriented, degree-aware and order-free; (2) the node's\ndegree-specific graph structure is not explicitly expressed in graph\nconvolution for distinguishing structure-aware node neighborhoods; (3) the\ntheoretical explanation regarding the graph-level pooling schemes is unclear.\n  To address these problems, we propose a generic degree-specific graph neural\nnetwork named DEMO-Net motivated by Weisfeiler-Lehman graph isomorphism test\nthat recursively identifies 1-hop neighborhood structures. In order to\nexplicitly capture the graph topology integrated with node attributes, we argue\nthat graph convolution should have three properties: seed-oriented,\ndegree-aware, order-free. To this end, we propose multi-task graph convolution\nwhere each task represents node representation learning for nodes with a\nspecific degree value, thus leading to preserving the degree-specific graph\nstructure. In particular, we design two multi-task learning methods:\ndegree-specific weight and hashing functions for graph convolution. In\naddition, we propose a novel graph-level pooling/readout scheme for learning\ngraph representation provably lying in a degree-specific Hilbert kernel space.\nThe experimental results on several node and graph classification benchmark\ndata sets demonstrate the effectiveness and efficiency of our proposed DEMO-Net\nover state-of-the-art graph neural network models.\n"", '  Graph neural networks, which generalize deep neural network models to graph\nstructured data, have attracted increasing attention in recent years. They\nusually learn node representations by transforming, propagating and aggregating\nnode features and have been proven to improve the performance of many graph\nrelated tasks such as node classification and link prediction. To apply graph\nneural networks for the graph classification task, approaches to generate the\n\\textit{graph representation} from node representations are demanded. A common\nway is to globally combine the node representations. However, rich structural\ninformation is overlooked. Thus a hierarchical pooling procedure is desired to\npreserve the graph structure during the graph representation learning. There\nare some recent works on hierarchically learning graph representation analogous\nto the pooling step in conventional convolutional neural (CNN) networks.\nHowever, the local structural information is still largely neglected during the\npooling process. In this paper, we introduce a pooling operator $\\pooling$\nbased on graph Fourier transform, which can utilize the node features and local\nstructures during the pooling process. We then design pooling layers based on\nthe pooling operator, which are further combined with traditional GCN\nconvolutional layers to form a graph neural network framework $\\m$ for graph\nclassification. Theoretical analysis is provided to understand $\\pooling$ from\nboth local and global perspectives. Experimental results of the graph\nclassification task on $6$ commonly used benchmarks demonstrate the\neffectiveness of the proposed framework.\n', '  Graph, as an important data representation, is ubiquitous in many real world\napplications ranging from social network analysis to biology. How to correctly\nand effectively learn and extract information from graph is essential for a\nlarge number of machine learning tasks. Graph embedding is a way to transform\nand encode the data structure in high dimensional and non-Euclidean feature\nspace to a low dimensional and structural space, which is easily exploited by\nother machine learning algorithms. We have witnessed a huge surge of such\nembedding methods, from statistical approaches to recent deep learning methods\nsuch as the graph convolutional networks (GCN). Deep learning approaches\nusually outperform the traditional methods in most graph learning benchmarks by\nbuilding an end-to-end learning framework to optimize the loss function\ndirectly. However, most of the existing GCN methods can only perform\nconvolution operations with node features, while ignoring the handy information\nin edge features, such as relations in knowledge graphs. To address this\nproblem, we present CensNet, Convolution with Edge-Node Switching graph neural\nnetwork, for learning tasks in graph-structured data with both node and edge\nfeatures. CensNet is a general graph embedding framework, which embeds both\nnodes and edges to a latent feature space. By using line graph of the original\nundirected graph, the role of nodes and edges are switched, and two novel graph\nconvolution operations are proposed for feature propagation. Experimental\nresults on real-world academic citation networks and quantum chemistry graphs\nshow that our approach achieves or matches the state-of-the-art performance in\nfour graph learning tasks, including semi-supervised node classification,\nmulti-task graph classification, graph regression, and link prediction.\n']"
9,1229,9_gradient_stochastic_sgd_convergence,"['gradient', 'stochastic', 'sgd', 'convergence', 'convex', 'descent', 'optimization', 'gradient descent', 'stochastic gradient', 'rate']","['stochastic gradient', 'stochastic optimization', 'gradient methods', 'gradient descent', 'convex optimization', 'optimization algorithms', 'minimization', 'linear convergence', 'convergence rates', 'convergence rate']","['Stochastic gradient descent', '', '', '', '', '', '', '', '', '']","['Stochastic gradient descent', '', '', '', '', '', '', '', '', '']","['  The learning rate is perhaps the single most important parameter in the\ntraining of neural networks and, more broadly, in stochastic (nonconvex)\noptimization. Accordingly, there are numerous effective, but poorly understood,\ntechniques for tuning the learning rate, including learning rate decay, which\nstarts with a large initial learning rate that is gradually decreased. In this\npaper, we present a general theoretical analysis of the effect of the learning\nrate in stochastic gradient descent (SGD). Our analysis is based on the use of\na learning-rate-dependent stochastic differential equation (lr-dependent SDE)\nthat serves as a surrogate for SGD. For a broad class of objective functions,\nwe establish a linear rate of convergence for this continuous-time formulation\nof SGD, highlighting the fundamental importance of the learning rate in SGD,\nand contrasting to gradient descent and stochastic gradient Langevin dynamics.\nMoreover, we obtain an explicit expression for the optimal linear rate by\nanalyzing the spectrum of the Witten-Laplacian, a special case of the\nSchr\\""odinger operator associated with the lr-dependent SDE. Strikingly, this\nexpression clearly reveals the dependence of the linear convergence rate on the\nlearning rate -- the linear rate decreases rapidly to zero as the learning rate\ntends to zero for a broad class of nonconvex functions, whereas it stays\nconstant for strongly convex functions. Based on this sharp distinction between\nnonconvex and convex problems, we provide a mathematical interpretation of the\nbenefits of using learning rate decay for nonconvex optimization.\n', '  We study finite-sum nonconvex optimization problems, where the objective\nfunction is an average of $n$ nonconvex functions. We propose a new stochastic\ngradient descent algorithm based on nested variance reduction. Compared with\nconventional stochastic variance reduced gradient (SVRG) algorithm that uses\ntwo reference points to construct a semi-stochastic gradient with diminishing\nvariance in each iteration, our algorithm uses $K+1$ nested reference points to\nbuild a semi-stochastic gradient to further reduce its variance in each\niteration. For smooth nonconvex functions, the proposed algorithm converges to\nan $\\epsilon$-approximate first-order stationary point (i.e., $\\|\\nabla\nF(\\mathbf{x})\\|_2\\leq \\epsilon$) within $\\tilde O(n\\land\n\\epsilon^{-2}+\\epsilon^{-3}\\land n^{1/2}\\epsilon^{-2})$ number of stochastic\ngradient evaluations. This improves the best known gradient complexity of SVRG\n$O(n+n^{2/3}\\epsilon^{-2})$ and that of SCSG $O(n\\land\n\\epsilon^{-2}+\\epsilon^{-10/3}\\land n^{2/3}\\epsilon^{-2})$. For gradient\ndominated functions, our algorithm also achieves better gradient complexity\nthan the state-of-the-art algorithms. Thorough experimental results on\ndifferent nonconvex optimization problems back up our theory.\n', ""  Recent works have shown that stochastic gradient descent (SGD) achieves the\nfast convergence rates of full-batch gradient descent for over-parameterized\nmodels satisfying certain interpolation conditions. However, the step-size used\nin these works depends on unknown quantities and SGD's practical performance\nheavily relies on the choice of this step-size. We propose to use line-search\ntechniques to automatically set the step-size when training models that can\ninterpolate the data. In the interpolation setting, we prove that SGD with a\nstochastic variant of the classic Armijo line-search attains the deterministic\nconvergence rates for both convex and strongly-convex functions. Under\nadditional assumptions, SGD with Armijo line-search is shown to achieve fast\nconvergence for non-convex functions. Furthermore, we show that stochastic\nextra-gradient with a Lipschitz line-search attains linear convergence for an\nimportant class of non-convex functions and saddle-point problems satisfying\ninterpolation. To improve the proposed methods' practical performance, we give\nheuristics to use larger step-sizes and acceleration. We compare the proposed\nalgorithms against numerous optimization methods on standard classification\ntasks using both kernel methods and deep networks. The proposed methods result\nin competitive performance across all models and datasets, while being robust\nto the precise choices of hyper-parameters. For multi-class classification\nusing deep networks, SGD with Armijo line-search results in both faster\nconvergence and better generalization.\n""]"
10,1124,10_recommendation_user_item_recommender,"['recommendation', 'user', 'item', 'recommender', 'items', 'users', 'recommender systems', 'recommendations', 'collaborative', 'systems']","['recommender systems', 'recommendation systems', 'based recommendation', 'collaborative filtering', 'recommender', 'matrix factorization', 'embedding', 'embeddings', 'ctr prediction', 'recommendation']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['Science/Tech', '', '', '', '', '', '', '', '', '']","[""  In recommender systems, cold-start issues are situations where no previous\nevents, e.g. ratings, are known for certain users or items. In this paper, we\nfocus on the item cold-start problem. Both content information (e.g. item\nattributes) and initial user ratings are valuable for seizing users'\npreferences on a new item. However, previous methods for the item cold-start\nproblem either 1) incorporate content information into collaborative filtering\nto perform hybrid recommendation, or 2) actively select users to rate the new\nitem without considering content information and then do collaborative\nfiltering. In this paper, we propose a novel recommendation scheme for the item\ncold-start problem by leverage both active learning and items' attribute\ninformation. Specifically, we design useful user selection criteria based on\nitems' attributes and users' rating history, and combine the criteria in an\noptimization framework for selecting users. By exploiting the feedback ratings,\nusers' previous ratings and items' attributes, we then generate accurate rating\npredictions for the other unselected users. Experimental results on two\nreal-world datasets show the superiority of our proposed method over\ntraditional methods.\n"", ""  Among various recommender techniques, collaborative filtering (CF) is the\nmost successful one. And a key problem in CF is how to represent users and\nitems. Previous works usually represent a user (an item) as a vector of latent\nfactors (aka. \\textit{embedding}) and then model the interactions between users\nand items based on the representations. Despite its effectiveness, we argue\nthat it's insufficient to yield satisfactory embeddings for collaborative\nfiltering. Inspired by the idea of SVD++ that represents users based on\nthemselves and their interacted items, we propose a general collaborative\nfiltering framework named DNCF, short for Dual-embedding based Neural\nCollaborative Filtering, to utilize historical interactions to enhance the\nrepresentation. In addition to learning the primitive embedding for a user (an\nitem), we introduce an additional embedding from the perspective of the\ninteracted items (users) to augment the user (item) representation. Extensive\nexperiments on four publicly datasets demonstrated the effectiveness of our\nproposed DNCF framework by comparing its performance with several traditional\nmatrix factorization models and other state-of-the-art deep learning based\nrecommender models.\n"", ""  In recommender systems, the user-item interaction data is usually sparse and\nnot sufficient for learning comprehensive user/item representations for\nrecommendation. To address this problem, we propose a novel dual-bridging\nrecommendation model (DBRec). DBRec performs latent user/item group discovery\nsimultaneously with collaborative filtering, and interacts group information\nwith users/items for bridging similar users/items. Therefore, a user's\npreference over an unobserved item, in DBRec, can be bridged by the users\nwithin the same group who have rated the item, or the user-rated items that\nshare the same group with the unobserved item. In addition, we propose to\njointly learn user-user group (item-item group) hierarchies, so that we can\neffectively discover latent groups and learn compact user/item representations.\nWe jointly integrate collaborative filtering, latent group discovering and\nhierarchical modelling into a unified framework, so that all the model\nparameters can be learned toward the optimization of the objective function. We\nvalidate the effectiveness of the proposed model with two real datasets, and\ndemonstrate its advantage over the state-of-the-art recommendation models with\nextensive experiments.\n""]"
11,1093,11_equations_differential_neural_differential equations,"['equations', 'differential', 'neural', 'differential equations', 'pdes', 'equation', 'physical', 'physics', 'dynamical', 'dynamics']","['equations pdes', 'pdes', 'pde', 'neural networks', 'dynamical systems', 'neural network', 'dynamics', 'differential equations', 'deep learning', 'modeling']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['Deep Learning for Prediction of Dynamics', '', '', '', '', '', '', '', '', '']","['  In this paper, we consider the problem of learning prediction models for\nspatiotemporal physical processes driven by unknown partial differential\nequations (PDEs). We propose a deep learning framework that learns the\nunderlying dynamics and predicts its evolution using sparsely distributed data\nsites. Deep learning has shown promising results in modeling physical dynamics\nin recent years. However, most of the existing deep learning methods for\nmodeling physical dynamics either focus on solving known PDEs or require data\nin a dense grid when the governing PDEs are unknown. In contrast, our method\nfocuses on learning prediction models for unknown PDE-driven dynamics only from\nsparsely observed data. The proposed method is spatial dimension-independent\nand geometrically flexible. We demonstrate our method in the forecasting task\nfor the two-dimensional wave equation and the Burgers-Fisher equation in\nmultiple geometries with different boundary conditions, and the ten-dimensional\nheat equation.\n', '  We introduce a class of Sparse, Physics-based, and partially Interpretable\nNeural Networks (SPINN) for solving ordinary and partial differential equations\n(PDEs). By reinterpreting a traditional meshless representation of solutions of\nPDEs we develop a class of sparse neural network architectures that are\npartially interpretable. The SPINN model we propose here serves as a seamless\nbridge between two extreme modeling tools for PDEs, namely dense neural network\nbased methods like Physics Informed Neural Networks (PINNs) and traditional\nmesh-free numerical methods, thereby providing a novel means to develop a new\nclass of hybrid algorithms that build on the best of both these viewpoints. A\nunique feature of the SPINN model that distinguishes it from other neural\nnetwork based approximations proposed earlier is that it is (i) interpretable,\nin a particular sense made precise in the work, and (ii) sparse in the sense\nthat it has much fewer connections than typical dense neural networks used for\nPDEs. Further, the SPINN algorithm implicitly encodes mesh adaptivity and is\nable to handle discontinuities in the solutions. In addition, we demonstrate\nthat Fourier series representations can also be expressed as a special class of\nSPINN and propose generalized neural network analogues of Fourier\nrepresentations. We illustrate the utility of the proposed method with a\nvariety of examples involving ordinary differential equations, elliptic,\nparabolic, hyperbolic and nonlinear partial differential equations, and an\nexample in fluid dynamics.\n', '  In recent years, data-driven methods have been developed to learn dynamical\nsystems and partial differential equations (PDE). The goal of such work is\ndiscovering unknown physics and the corresponding equations. However, prior to\nachieving this goal, major challenges remain to be resolved, including learning\nPDE under noisy data and limited discrete data. To overcome these challenges,\nin this work, a deep-learning based data-driven method, called DL-PDE, is\ndeveloped to discover the governing PDEs of underlying physical processes. The\nDL-PDE method combines deep learning via neural networks and data-driven\ndiscovery of PDE via sparse regressions. In the DL-PDE, a neural network is\nfirst trained, and then a large amount of meta-data is generated, and the\nrequired derivatives are calculated by automatic differentiation. Finally, the\nform of PDE is discovered by sparse regression. The proposed method is tested\nwith physical processes, governed by groundwater flow equation,\nconvection-diffusion equation, Burgers equation and Korteweg-de Vries (KdV)\nequation, for proof-of-concept and applications in real-world engineering\nsettings. The proposed method achieves satisfactory results when data are noisy\nand limited.\n']"
12,1048,12_clinical_patient_health_patients,"['clinical', 'patient', 'health', 'patients', 'medical', 'healthcare', 'ehr', 'data', 'records', 'care']","['electronic health', 'ehr data', 'ehr', 'ehrs', 'predictive', 'patients', 'clinicians', 'healthcare', 'mortality', 'clinical']","['health care', '', '', '', '', '', '', '', '', '']","['Science/Tech', '', '', '', '', '', '', '', '', '']","[""  A crucial step within secondary analysis of electronic health records (EHRs)\nis to identify the patient cohort under investigation. While EHRs contain\nmedical billing codes that aim to represent the conditions and treatments\npatients may have, much of the information is only present in the patient\nnotes. Therefore, it is critical to develop robust algorithms to infer\npatients' conditions and treatments from their written notes. In this paper, we\nintroduce a dataset for patient phenotyping, a task that is defined as the\nidentification of whether a patient has a given medical condition (also\nreferred to as clinical indication or phenotype) based on their patient note.\nNursing Progress Notes and Discharge Summaries from the Intensive Care Unit of\na large tertiary care hospital were manually annotated for the presence of\nseveral high-context phenotypes relevant to treatment and risk of\nre-hospitalization. This dataset contains 1102 Discharge Summaries and 1000\nNursing Progress Notes. Each Discharge Summary and Progress Note has been\nannotated by at least two expert human annotators (one clinical researcher and\none resident physician). Annotated phenotypes include treatment non-adherence,\nchronic pain, advanced/metastatic cancer, as well as 10 other phenotypes. This\ndataset can be utilized for academic and industrial research in medicine and\ncomputer science, particularly within the field of medical natural language\nprocessing.\n"", ""  Electronic health records (EHRs) are longitudinal records of a patient's\ninteractions with healthcare systems. A patient's EHR data is organized as a\nthree-level hierarchy from top to bottom: patient journey - all the experiences\nof diagnoses and treatments over a period of time; individual visit - a set of\nmedical codes in a particular visit; and medical code - a specific record in\nthe form of medical codes. As EHRs begin to amass in millions, the potential\nbenefits, which these data might hold for medical research and medical outcome\nprediction, are staggering - including, for example, predicting future\nadmissions to hospitals, diagnosing illnesses or determining the efficacy of\nmedical treatments. Each of these analytics tasks requires a domain knowledge\nextraction method to transform the hierarchical patient journey into a vector\nrepresentation for further prediction procedure. The representations should\nembed a sequence of visits and a set of medical codes with a specific\ntimestamp, which are crucial to any downstream prediction tasks. Hence,\nexpressively powerful representations are appealing to boost learning\nperformance. To this end, we propose a novel self-attention mechanism that\ncaptures the contextual dependency and temporal relationships within a\npatient's healthcare journey. An end-to-end bidirectional temporal encoder\nnetwork (BiteNet) then learns representations of the patient's journeys, based\nsolely on the proposed attention mechanism. We have evaluated the effectiveness\nof our methods on two supervised prediction and two unsupervised clustering\ntasks with a real-world EHR dataset. The empirical results demonstrate the\nproposed BiteNet model produces higher-quality representations than\nstate-of-the-art baseline methods.\n"", '  In recent years, increasingly augmentation of health data, such as patient\nElectronic Health Records (EHR), are becoming readily available. This provides\nan unprecedented opportunity for knowledge discovery and data mining algorithms\nto dig insights from them, which can, later on, be helpful to the improvement\nof the quality of care delivery. Predictive modeling of clinical risk,\nincluding in-hospital mortality, hospital readmission, chronic disease onset,\ncondition exacerbation, etc., from patient EHR, is one of the health data\nanalytic problems that attract most of the interests. The reason is not only\nbecause the problem is important in clinical settings, but also there are\nchallenges working with EHR such as sparsity, irregularity, temporality, etc.\nDifferent from applications in other domains such as computer vision and\nnatural language processing, the labeled data samples in medicine (patients)\nare relatively limited, which creates lots of troubles for effective predictive\nmodel learning, especially for complicated models such as deep learning. In\nthis paper, we propose MetaPred, a meta-learning for clinical risk prediction\nfrom longitudinal patient EHRs. In particular, in order to predict the target\nrisk where there are limited data samples, we train a meta-learner from a set\nof related risk prediction tasks which learns how a good predictor is learned.\nThe meta-learned can then be directly used in target risk prediction, and the\nlimited available samples can be used for further fine-tuning the model\nperformance. The effectiveness of MetaPred is tested on a real patient EHR\nrepository from Oregon Health & Science University. We are able to demonstrate\nthat with CNN and RNN as base predictors, MetaPred can achieve much better\nperformance for predicting target risk with low resources comparing with the\npredictor trained on the limited samples available for this risk.\n']"
13,1032,13_channel_wireless_network_radio,"['channel', 'wireless', 'network', 'radio', 'based', 'communication', 'learning', 'proposed', 'mimo', 'allocation']","['wireless networks', 'deep learning', 'networks', 'neural network', 'beamforming', 'channel state', 'reinforcement learning', 'resource allocation', 'mimo', 'mmwave']","['D2D Networks', '', '', '', '', '', '', '', '', '']","['A novel neural network architecture for wireless channel modeling', '', '', '', '', '', '', '', '', '']","['  The standardization process of the fifth generation (5G) wireless\ncommunications has recently been accelerated and the first commercial 5G\nservices would be provided as early as in 2018. The increasing of enormous\nsmartphones, new complex scenarios, large frequency bands, massive antenna\nelements, and dense small cells will generate big datasets and bring 5G\ncommunications to the era of big data. This paper investigates various\napplications of big data analytics, especially machine learning algorithms in\nwireless communications and channel modeling. We propose a big data and machine\nlearning enabled wireless channel model framework. The proposed channel model\nis based on artificial neural networks (ANNs), including feed-forward neural\nnetwork (FNN) and radial basis function neural network (RBF-NN). The input\nparameters are transmitter (Tx) and receiver (Rx) coordinates, Tx-Rx distance,\nand carrier frequency, while the output parameters are channel statistical\nproperties, including the received power, root mean square (RMS) delay spread\n(DS), and RMS angle spreads (ASs). Datasets used to train and test the ANNs are\ncollected from both real channel measurements and a geometry based stochastic\nmodel (GBSM). Simulation results show good performance and indicate that\nmachine learning algorithms can be powerful analytical tools for future\nmeasurement-based wireless channel modeling.\n', '  Channel allocation is the task of assigning channels to users such that some\nobjective (e.g., sum-rate) is maximized. In centralized networks such as\ncellular networks, this task is carried by the base station which gathers the\nchannel state information (CSI) from the users and computes the optimal\nsolution. In distributed networks such as ad-hoc and device-to-device (D2D)\nnetworks, no base station exists and conveying global CSI between users is\ncostly or simply impractical. When the CSI is time varying and unknown to the\nusers, the users face the challenge of both learning the channel statistics\nonline and converge to a good channel allocation. This introduces a multi-armed\nbandit (MAB) scenario with multiple decision makers. If two users or more\nchoose the same channel, a collision occurs and they all receive zero reward.\nWe propose a distributed channel allocation algorithm that each user runs and\nconverges to the optimal allocation while achieving an order optimal regret of\nO\\left(\\log T\\right). The algorithm is based on a carrier sensing multiple\naccess (CSMA) implementation of the distributed auction algorithm. It does not\nrequire any exchange of information between users. Users need only to observe a\nsingle channel at a time and sense if there is a transmission on that channel,\nwithout decoding the transmissions or identifying the transmitting users. We\ndemonstrate the performance of our algorithm using simulated LTE and 5G\nchannels.\n', '  Hybrid beamformer design plays very crucial role in the next generation\nmillimeter-wave (mm-Wave) massive MIMO (multiple-input multiple-output)\nsystems. Previous works assume the perfect channel state information (CSI)\nwhich results heavy feedback overhead. To lower complexity, channel statistics\ncan be utilized such that only infrequent update of the channel information is\nneeded. To reduce the complexity and provide robustness, in this work, we\npropose a deep learning (DL) framework to deal with both hybrid beamforming and\nchannel estimation. For this purpose, we introduce three deep convolutional\nneural network (CNN) architectures. We assume that the base station (BS) has\nthe channel statistics only and feeds the channel covariance matrix into a CNN\nto obtain the hybrid precoders. At the receiver, two CNNs are employed. The\nfirst one is used for channel estimation purposes and the another is employed\nto design the hybrid combiners. The proposed DL framework does not require the\ninstantaneous feedback of the CSI at the BS. We have shown that the proposed\napproach has higher spectral efficiency with comparison to the conventional\ntechniques. The trained CNN structures do not need to be re-trained due to the\nchanges in the propagation environment such as the deviations in the number of\nreceived paths and the fluctuations in the received path angles up to 4\ndegrees. Also, the proposed DL framework exhibits at least 10 times lower\ncomputational complexity as compared to the conventional optimization-based\napproaches.\n']"
14,977,14_gans_gan_generative_generative adversarial,"['gans', 'gan', 'generative', 'generative adversarial', 'discriminator', 'generator', 'adversarial', 'adversarial networks', 'image', 'networks gans']","['training gans', 'generative adversarial', 'gans', 'networks gans', 'adversarial network', 'gan training', 'adversarial networks', 'adversarial', 'generative models', 'generative model']","['GANs', '', '', '', '', '', '', '', '', '']","['GAN training', '', '', '', '', '', '', '', '', '']","['  Unconditional image generation has recently been dominated by generative\nadversarial networks (GANs). GAN methods train a generator which regresses\nimages from random noise vectors, as well as a discriminator that attempts to\ndifferentiate between the generated images and a training set of real images.\nGANs have shown amazing results at generating realistic looking images. Despite\ntheir success, GANs suffer from critical drawbacks including: unstable training\nand mode-dropping. The weaknesses in GANs have motivated research into\nalternatives including: variational auto-encoders (VAEs), latent embedding\nlearning methods (e.g. GLO) and nearest-neighbor based implicit maximum\nlikelihood estimation (IMLE). Unfortunately at the moment, GANs still\nsignificantly outperform the alternative methods for image generation. In this\nwork, we present a novel method - Generative Latent Nearest Neighbors (GLANN) -\nfor training generative models without adversarial training. GLANN combines the\nstrengths of IMLE and GLO in a way that overcomes the main drawbacks of each\nmethod. Consequently, GLANN generates images that are far better than GLO and\nIMLE. Our method does not suffer from mode collapse which plagues GAN training\nand is much more stable. Qualitative results show that GLANN outperforms a\nbaseline consisting of 800 GANs and VAEs on commonly used datasets. Our models\nare also shown to be effective for training truly non-adversarial unsupervised\nimage translation.\n', '  Generative Adversarial Networks (GANs) have achieved great success in\ngenerating realistic images. Most of these are conditional models, although\nacquisition of class labels is expensive and time-consuming in practice. To\nreduce the dependence on labeled data, we propose an un-conditional generative\nadversarial model, called K-Means-GAN (KM-GAN), which incorporates the idea of\nupdating centers in K-Means into GANs. Specifically, we redesign the framework\nof GANs by applying K-Means on the features extracted from the discriminator.\nWith obtained labels from K-Means, we propose new objective functions from the\nperspective of deep metric learning (DML). Distinct from previous works, the\ndiscriminator is treated as a feature extractor rather than a classifier in\nKM-GAN, meanwhile utilization of K-Means makes features of the discriminator\nmore representative. Experiments are conducted on various datasets, such as\nMNIST, Fashion-10, CIFAR-10 and CelebA, and show that the quality of samples\ngenerated by KM-GAN is comparable to some conditional generative adversarial\nmodels.\n', '  Generative Adversarial Networks (GANs) are the most popular image generation\nmodels that have achieved remarkable performance on various tasks. However,\ntraining instability is still one of the open problems for all GAN-based\nalgorithms. In order to stabilize GANs training, some regularization and\nnormalization techniques have been proposed to make the discriminator meet the\nLipschitz continuity. In this paper, a new approach inspired by works on\nadversarial is proposed to stabilize the training process of GANs. It is found\nthat sometimes the images generated by the generator play a role just like\nadversarial examples for discriminator during the training process, which might\nbe a part of the reason for the unstable training of GANs. With this discovery,\nwe propose a Direct Adversarial Training (DAT) method for the training process\nof GANs to improve its performance. We prove that the DAT method can minimize\nthe Lipschitz constant of the discriminator adaptively. The advanced\nperformance of the proposed method is verified on multiple baseline and SOTA\nnetworks, such as DCGAN, Spectral Normalization GAN, Self-supervised GAN, and\nInformation Maximum GAN. Code will be available at\n\\url{https://github.com/iceli1007/DAT-GAN}\n']"
15,945,15_regret_bandit_arms_arm,"['regret', 'bandit', 'arms', 'arm', 'bandits', 'armed', 'multi armed', 'reward', 'bound', 'algorithm']","['contextual bandits', 'bandit algorithm', 'bandit algorithms', 'linear bandits', 'linear bandit', 'optimal regret', 'contextual bandit', 'armed bandits', 'regret bounds', 'bandit problem']","['Learning algorithms for multi armed bandits', '', '', '', '', '', '', '', '', '']","['Learning algorithms for multi-armed bandits', '', '', '', '', '', '', '', '', '']","['  We consider stochastic multi-armed bandits where the expected reward is a\nunimodal function over partially ordered arms. This important class of problems\nhas been recently investigated in (Cope 2009, Yu 2011). The set of arms is\neither discrete, in which case arms correspond to the vertices of a finite\ngraph whose structure represents similarity in rewards, or continuous, in which\ncase arms belong to a bounded interval. For discrete unimodal bandits, we\nderive asymptotic lower bounds for the regret achieved under any algorithm, and\npropose OSUB, an algorithm whose regret matches this lower bound. Our algorithm\noptimally exploits the unimodal structure of the problem, and surprisingly, its\nasymptotic regret does not depend on the number of arms. We also provide a\nregret upper bound for OSUB in non-stationary environments where the expected\nrewards smoothly evolve over time. The analytical results are supported by\nnumerical experiments showing that OSUB performs significantly better than the\nstate-of-the-art algorithms. For continuous sets of arms, we provide a brief\ndiscussion. We show that combining an appropriate discretization of the set of\narms with the UCB algorithm yields an order-optimal regret, and in practice,\noutperforms recently proposed algorithms designed to exploit the unimodal\nstructure.\n', ""  We present a formal model of human decision-making in explore-exploit tasks\nusing the context of multi-armed bandit problems, where the decision-maker must\nchoose among multiple options with uncertain rewards. We address the standard\nmulti-armed bandit problem, the multi-armed bandit problem with transition\ncosts, and the multi-armed bandit problem on graphs. We focus on the case of\nGaussian rewards in a setting where the decision-maker uses Bayesian inference\nto estimate the reward values. We model the decision-maker's prior knowledge\nwith the Bayesian prior on the mean reward. We develop the upper credible limit\n(UCL) algorithm for the standard multi-armed bandit problem and show that this\ndeterministic algorithm achieves logarithmic cumulative expected regret, which\nis optimal performance for uninformative priors. We show how good priors and\ngood assumptions on the correlation structure among arms can greatly enhance\ndecision-making performance, even over short time horizons. We extend to the\nstochastic UCL algorithm and draw several connections to human decision-making\nbehavior. We present empirical data from human experiments and show that human\nperformance is efficiently captured by the stochastic UCL algorithm with\nappropriate parameters. For the multi-armed bandit problem with transition\ncosts and the multi-armed bandit problem on graphs, we generalize the UCL\nalgorithm to the block UCL algorithm and the graphical block UCL algorithm,\nrespectively. We show that these algorithms also achieve logarithmic cumulative\nexpected regret and require a sub-logarithmic expected number of transitions\namong arms. We further illustrate the performance of these algorithms with\nnumerical examples. NB: Appendix G included in this version details minor\nmodifications that correct for an oversight in the previously-published proofs.\nThe remainder of the text reflects the published work.\n"", '  We study a constrained contextual linear bandit setting, where the goal of\nthe agent is to produce a sequence of policies, whose expected cumulative\nreward over the course of $T$ rounds is maximum, and each has an expected cost\nbelow a certain threshold $\\tau$. We propose an upper-confidence bound\nalgorithm for this problem, called optimistic pessimistic linear bandit (OPLB),\nand prove an $\\widetilde{\\mathcal{O}}(\\frac{d\\sqrt{T}}{\\tau-c_0})$ bound on its\n$T$-round regret, where the denominator is the difference between the\nconstraint threshold and the cost of a known feasible action. We further\nspecialize our results to multi-armed bandits and propose a computationally\nefficient algorithm for this setting. We prove a regret bound of\n$\\widetilde{\\mathcal{O}}(\\frac{\\sqrt{KT}}{\\tau - c_0})$ for this algorithm in\n$K$-armed bandits, which is a $\\sqrt{K}$ improvement over the regret bound we\nobtain by simply casting multi-armed bandits as an instance of contextual\nlinear bandits and using the regret bound of OPLB. We also prove a lower-bound\nfor the problem studied in the paper and provide simulations to validate our\ntheoretical results.\n']"
16,879,16_3d_point_shape_object,"['3d', 'point', 'shape', 'object', 'depth', 'point clouds', 'clouds', 'point cloud', 'scene', 'cloud']","['point clouds', 'depth estimation', '3d point', '3d shape', 'point cloud', '3d shapes', 'lidar', '3d object', 'convolutional', '3d']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['3D point clouds', '', '', '', '', '', '', '', '', '']","['  3D point cloud is an efficient and flexible representation of 3D structures.\nRecently, neural networks operating on point clouds have shown superior\nperformance on 3D understanding tasks such as shape classification and part\nsegmentation. However, performance on such tasks is evaluated on complete\nshapes aligned in a canonical frame, while real world 3D data are partial and\nunaligned. A key challenge in learning from partial, unaligned point cloud data\nis to learn features that are invariant or equivariant with respect to\ngeometric transformations. To address this challenge, we propose the Iterative\nTransformer Network (IT-Net), a network module that canonicalizes the pose of a\npartial object with a series of 3D rigid transformations predicted in an\niterative fashion. We demonstrate the efficacy of IT-Net as an anytime pose\nestimator from partial point clouds without using complete object models.\nFurther, we show that IT-Net achieves superior performance over alternative 3D\ntransformer networks on various tasks, such as partial shape classification and\nobject part segmentation.\n', '  Understanding 3D object structure from a single image is an important but\nchallenging task in computer vision, mostly due to the lack of 3D object\nannotations to real images. Previous research tackled this problem by either\nsearching for a 3D shape that best explains 2D annotations, or training purely\non synthetic data with ground truth 3D information. In this work, we propose 3D\nINterpreter Networks (3D-INN), an end-to-end trainable framework that\nsequentially estimates 2D keypoint heatmaps and 3D object skeletons and poses.\nOur system learns from both 2D-annotated real images and synthetic 3D data.\nThis is made possible mainly by two technical innovations. First, heatmaps of\n2D keypoints serve as an intermediate representation to connect real and\nsynthetic data. 3D-INN is trained on real images to estimate 2D keypoint\nheatmaps from an input image; it then predicts 3D object structure from\nheatmaps using knowledge learned from synthetic 3D shapes. By doing so, 3D-INN\nbenefits from the variation and abundance of synthetic 3D objects, without\nsuffering from the domain difference between real and synthesized images, often\ndue to imperfect rendering. Second, we propose a Projection Layer, mapping\nestimated 3D structure back to 2D. During training, it ensures 3D-INN to\npredict 3D structure whose projection is consistent with the 2D annotations to\nreal images. Experiments show that the proposed system performs well on both 2D\nkeypoint estimation and 3D structure recovery. We also demonstrate that the\nrecovered 3D information has wide vision applications, such as image retrieval.\n', '  LiDAR-based SLAM algorithms are extensively studied to providing robust and\naccurate positioning for autonomous driving vehicles (ADV) in the past decades.\nSatisfactory performance can be obtained using high-grade 3D LiDAR with 64\nchannels, which can provide dense point clouds. Unfortunately, the high price\nsignificantly prevents its extensive commercialization in ADV. The\ncost-effective 3D LiDAR with 16 channels is a promising replacement. However,\nonly limited and sparse point clouds can be provided by the 16 channels LiDAR,\nwhich cannot guarantee sufficient positioning accuracy for ADV in challenging\ndynamic environments. The high-resolution image from the low-cost camera can\nprovide ample information about the surroundings. However, the explicit depth\ninformation is not available from the image. Inspired by the complementariness\nof 3D LiDAR and camera, this paper proposes to make use of the high-resolution\nimages from a camera to enrich the raw 3D point clouds from the low-cost 16\nchannels LiDAR based on a state-of-the-art deep learning algorithm. An ERFNet\nis firstly employed to segment the image with the aid of the raw sparse 3D\npoint clouds. Meanwhile, the sparse convolutional neural network is employed to\npredict the dense point clouds based on raw sparse 3D point clouds. Then, the\npredicted dense point clouds are fused with the segmentation outputs from\nERFnet using a novel multi-layer convolutional neural network to refine the\npredicted 3D point clouds. Finally, the enriched point clouds are employed to\nperform LiDAR SLAM based on the state-of-the-art normal distribution transform\n(NDT). We tested our approach on the re-edited KITTI datasets: (1)the sparse 3D\npoint clouds are significantly enriched with a mean square error of 1.1m MSE.\n(2)the map generated from the LiDAR SLAM is denser which includes more details\nwithout significant accuracy loss.\n']"
17,796,17_clustering_means_clusters_cluster,"['clustering', 'means', 'clusters', 'cluster', 'algorithm', 'data', 'algorithms', 'clustering algorithm', 'clustering algorithms', 'points']","['clustering methods', 'clustering algorithms', 'clustering algorithm', 'based clustering', 'clustering method', 'data clustering', 'clustering results', 'clustering', 'clusterings', 'hierarchical clustering']","['Clustering', '', '', '', '', '', '', '', '', '']","['Clustering', '', '', '', '', '', '', '', '', '']","['  Recent spectral clustering methods are a propular and powerful technique for\ndata clustering. These methods need to solve the eigenproblem whose\ncomputational complexity is $O(n^3)$, where $n$ is the number of data samples.\nIn this paper, a non-eigenproblem based clustering method is proposed to deal\nwith the clustering problem. Its performance is comparable to the spectral\nclustering algorithms but it is more efficient with computational complexity\n$O(n^2)$. We show that with a transitive distance and an observed property,\ncalled K-means duality, our algorithm can be used to handle data sets with\ncomplex cluster shapes, multi-scale clusters, and noise. Moreover, no\nparameters except the number of clusters need to be set in our algorithm.\n', '  Clustering is an unsupervised learning method that constitutes a cornerstone\nof an intelligent data analysis process. It is used for the exploration of\ninter-relationships among a collection of patterns, by organizing them into\nhomogeneous clusters. Clustering has been dynamically applied to a variety of\ntasks in the field of Information Retrieval (IR). Clustering has become one of\nthe most active area of research and the development. Clustering attempts to\ndiscover the set of consequential groups where those within each group are more\nclosely related to one another than the others assigned to different groups.\nThe resultant clusters can provide a structure for organizing large bodies of\ntext for efficient browsing and searching. There exists a wide variety of\nclustering algorithms that has been intensively studied in the clustering\nproblem. Among the algorithms that remain the most common and effectual, the\niterative optimization clustering algorithms have been demonstrated reasonable\nperformance for clustering, e.g. the Expectation Maximization (EM) algorithm\nand its variants, and the well known k-means algorithm. This paper presents an\nanalysis on how partition method clustering techniques - EM, K -means and K*\nMeans algorithm work on heartspect dataset with below mentioned features -\nPurity, Entropy, CPU time, Cluster wise analysis, Mean value analysis and inter\ncluster distance. Thus the paper finally provides the experimental results of\ndatasets for five clusters to strengthen the results that the quality of the\nbehavior in clusters in EM algorithm is far better than k-means algorithm and\nk*means algorithm.\n', '  Convex clustering, a convex relaxation of k-means clustering and hierarchical\nclustering, has drawn recent attentions since it nicely addresses the\ninstability issue of traditional nonconvex clustering methods. Although its\ncomputational and statistical properties have been recently studied, the\nperformance of convex clustering has not yet been investigated in the\nhigh-dimensional clustering scenario, where the data contains a large number of\nfeatures and many of them carry no information about the clustering structure.\nIn this paper, we demonstrate that the performance of convex clustering could\nbe distorted when the uninformative features are included in the clustering. To\novercome it, we introduce a new clustering method, referred to as Sparse Convex\nClustering, to simultaneously cluster observations and conduct feature\nselection. The key idea is to formulate convex clustering in a form of\nregularization, with an adaptive group-lasso penalty term on cluster centers.\nIn order to optimally balance the tradeoff between the cluster fitting and\nsparsity, a tuning criterion based on clustering stability is developed. In\ntheory, we provide an unbiased estimator for the degrees of freedom of the\nproposed sparse convex clustering method. Finally, the effectiveness of the\nsparse convex clustering is examined through a variety of numerical experiments\nand a real data application.\n']"
18,796,18_optimization_bayesian optimization_bayesian_bo,"['optimization', 'bayesian optimization', 'bayesian', 'bo', 'problems', 'hyperparameter', 'function', 'search', 'problem', 'hyperparameters']","['bayesian optimization', 'global optimization', 'hyperparameter optimization', 'surrogate model', 'optimization bo', 'optimization', 'optimisation', 'combinatorial optimization', 'bayesian', 'hyperparameter tuning']","['Bayesian optimization', '', '', '', '', '', '', '', '', '']","['Bayesian optimization', '', '', '', '', '', '', '', '', '']","['  Parameter settings profoundly impact the performance of machine learning\nalgorithms and laboratory experiments. The classical grid search or trial-error\nmethods are exponentially expensive in large parameter spaces, and Bayesian\noptimization (BO) offers an elegant alternative for global optimization of\nblack box functions. In situations where the black box function can be\nevaluated at multiple points simultaneously, batch Bayesian optimization is\nused. Current batch BO approaches are restrictive in that they fix the number\nof evaluations per batch, and this can be wasteful when the number of specified\nevaluations is larger than the number of real maxima in the underlying\nacquisition function. We present the Budgeted Batch Bayesian Optimization (B3O)\nfor hyper-parameter tuning and experimental design - we identify the\nappropriate batch size for each iteration in an elegant way. To set the batch\nsize flexible, we use the infinite Gaussian mixture model (IGMM) for\nautomatically identifying the number of peaks in the underlying acquisition\nfunctions. We solve the intractability of estimating the IGMM directly from the\nacquisition function by formulating the batch generalized slice sampling to\nefficiently draw samples from the acquisition function. We perform extensive\nexperiments for both synthetic functions and two real world applications -\nmachine learning hyper-parameter tuning and experimental design for alloy\nhardening. We show empirically that the proposed B3O outperforms the existing\nfixed batch BO approaches in finding the optimum whilst requiring a fewer\nnumber of evaluations, thus saving cost and time.\n', '  Bayesian optimization is an effective method to efficiently optimize unknown\nobjective functions with high evaluation costs. Traditional Bayesian\noptimization algorithms select one point per iteration for single objective\nfunction, whereas in recent years, Bayesian optimization for multi-objective\noptimization or multi-point search per iteration have been proposed. However,\nBayesian optimization that can deal with them at the same time in non-heuristic\nway is not known at present. We propose a Bayesian optimization algorithm that\ncan deal with multi-objective optimization and multi-point search at the same\ntime. First, we define an acquisition function that considers both\nmulti-objective and multi-point search problems. It is difficult to\nanalytically maximize the acquisition function as the computational cost is\nprohibitive even when approximate calculations such as sampling approximation\nare performed; therefore, we propose an accurate and computationally efficient\nmethod for estimating gradient of the acquisition function, and develop an\nalgorithm for Bayesian optimization with multi-objective and multi-point\nsearch. It is shown via numerical experiments that the performance of the\nproposed method is comparable or superior to those of heuristic methods.\n', '  Bayesian optimization has become a fundamental global optimization algorithm\nin many problems where sample efficiency is of paramount importance. Recently,\nthere has been proposed a large number of new applications in fields such as\nrobotics, machine learning, experimental design, simulation, etc. In this\npaper, we focus on several problems that appear in robotics and autonomous\nsystems: algorithm tuning, automatic control and intelligent design. All those\nproblems can be mapped to global optimization problems. However, they become\nhard optimization problems. Bayesian optimization internally uses a\nprobabilistic surrogate model (e.g.: Gaussian process) to learn from the\nprocess and reduce the number of samples required. In order to generalize to\nunknown functions in a black-box fashion, the common assumption is that the\nunderlying function can be modeled with a stationary process. Nonstationary\nGaussian process regression cannot generalize easily and it typically requires\nprior knowledge of the function. Some works have designed techniques to\ngeneralize Bayesian optimization to nonstationary functions in an indirect way,\nbut using techniques originally designed for regression, where the objective is\nto improve the quality of the surrogate model everywhere. Instead optimization\nshould focus on improving the surrogate model near the optimum. In this paper,\nwe present a novel kernel function specially designed for Bayesian\noptimization, that allows nonstationary behavior of the surrogate model in an\nadaptive local region. In our experiments, we found that this new kernel\nresults in an improved local search (exploitation), without penalizing the\nglobal search (exploration). We provide results in well-known benchmarks and\nreal applications. The new method outperforms the state of the art in Bayesian\noptimization both in stationary and nonstationary problems.\n']"
19,743,19_financial_stock_market_trading,"['financial', 'stock', 'market', 'trading', 'credit', 'price', 'fraud', 'data', 'model', 'prices']","['volatility', 'stock market', 'financial markets', 'time series', 'stock prices', 'forecasting', 'predictive', 'investors', 'predicting', 'lstm']","['Financial Markets', '', '', '', '', '', '', '', '', '']","['Financial Market Trend Forecasting', '', '', '', '', '', '', '', '', '']","['  Prediction of stock price and stock price movement patterns has always been a\ncritical area of research. While the well-known efficient market hypothesis\nrules out any possibility of accurate prediction of stock prices, there are\nformal propositions in the literature demonstrating accurate modeling of the\npredictive systems that can enable us to predict stock prices with a very high\nlevel of accuracy. In this paper, we present a suite of deep learning-based\nregression models that yields a very high level of accuracy in stock price\nprediction. To build our predictive models, we use the historical stock price\ndata of a well-known company listed in the National Stock Exchange (NSE) of\nIndia during the period December 31, 2012 to January 9, 2015. The stock prices\nare recorded at five minutes intervals of time during each working day in a\nweek. Using these extremely granular stock price data, we build four\nconvolutional neural network (CNN) and five long- and short-term memory\n(LSTM)-based deep learning models for accurate forecasting of the future stock\nprices. We provide detailed results on the forecasting accuracies of all our\nproposed models based on their execution time and their root mean square error\n(RMSE) values.\n', '  Stock market forecasting is very important in the planning of business\nactivities. Stock price prediction has attracted many researchers in multiple\ndisciplines including computer science, statistics, economics, finance, and\noperations research. Recent studies have shown that the vast amount of online\ninformation in the public domain such as Wikipedia usage pattern, news stories\nfrom the mainstream media, and social media discussions can have an observable\neffect on investors opinions towards financial markets. The reliability of the\ncomputational models on stock market prediction is important as it is very\nsensitive to the economy and can directly lead to financial loss. In this\npaper, we retrieved, extracted, and analyzed the effects of news sentiments on\nthe stock market. Our main contributions include the development of a sentiment\nanalysis dictionary for the financial sector, the development of a\ndictionary-based sentiment analysis model, and the evaluation of the model for\ngauging the effects of news sentiments on stocks for the pharmaceutical market.\nUsing only news sentiments, we achieved a directional accuracy of 70.59% in\npredicting the trends in short-term stock price movement.\n', '  The financial market trend forecasting method is emerging as a hot topic in\nfinancial markets today. Many challenges still currently remain, and various\nresearches related thereto have been actively conducted. Especially, recent\nresearch of neural network-based financial market trend prediction has\nattracted much attention. However, previous researches do not deal with the\nfinancial market forecasting method based on LSTM which has good performance in\ntime series data. There is also a lack of comparative analysis in the\nperformance of neural network-based prediction techniques and traditional\nprediction techniques. In this paper, we propose a financial market trend\nforecasting method using LSTM and analyze the performance with existing\nfinancial market trend forecasting methods through experiments. This method\nprepares the input data set through the data preprocessing process so as to\nreflect all the fundamental data, technical data and qualitative data used in\nthe financial data analysis, and makes comprehensive financial market analysis\nthrough LSTM. In this paper, we experiment and compare performances of existing\nfinancial market trend forecasting models, and performance according to the\nfinancial market environment. In addition, we implement the proposed method\nusing open sources and platform and forecast financial market trends using\nvarious financial data indicators.\n']"
20,723,20_matrix_rank_pca_low rank,"['matrix', 'rank', 'pca', 'low rank', 'matrix completion', 'low', 'nmf', 'completion', 'principal', 'matrices']","['matrix completion', 'matrix factorization', 'rank matrices', 'sparse pca', 'rank approximation', 'rank matrix', 'robust pca', 'nonnegative matrix', 'matrices', 'matrix']","['Mathematical problems', '', '', '', '', '', '', '', '', '']","['Robust matrix completion', '', '', '', '', '', '', '', '', '']","['  Most recent results in matrix completion assume that the matrix under\nconsideration is low-rank or that the columns are in a union of low-rank\nsubspaces. In real-world settings, however, the linear structure underlying\nthese models is distorted by a (typically unknown) nonlinear transformation.\nThis paper addresses the challenge of matrix completion in the face of such\nnonlinearities. Given a few observations of a matrix that are obtained by\napplying a Lipschitz, monotonic function to a low rank matrix, our task is to\nestimate the remaining unobserved entries. We propose a novel matrix completion\nmethod that alternates between low-rank matrix estimation and monotonic\nfunction estimation to estimate the missing matrix elements. Mean squared error\nbounds provide insight into how well the matrix can be estimated based on the\nsize, rank of the matrix and properties of the nonlinear transformation.\nEmpirical results on synthetic and real-world datasets demonstrate the\ncompetitiveness of the proposed approach.\n', ""  Consider a movie recommendation system where apart from the ratings\ninformation, side information such as user's age or movie's genre is also\navailable. Unlike standard matrix completion, in this setting one should be\nable to predict inductively on new users/movies. In this paper, we study the\nproblem of inductive matrix completion in the exact recovery setting. That is,\nwe assume that the ratings matrix is generated by applying feature vectors to a\nlow-rank matrix and the goal is to recover back the underlying matrix.\nFurthermore, we generalize the problem to that of low-rank matrix estimation\nusing rank-1 measurements. We study this generic problem and provide conditions\nthat the set of measurements should satisfy so that the alternating\nminimization method (which otherwise is a non-convex method with no convergence\nguarantees) is able to recover back the {\\em exact} underlying low-rank matrix.\n  In addition to inductive matrix completion, we show that two other low-rank\nestimation problems can be studied in our framework: a) general low-rank matrix\nsensing using rank-1 measurements, and b) multi-label regression with missing\nlabels. For both the problems, we provide novel and interesting bounds on the\nnumber of measurements required by alternating minimization to provably\nconverges to the {\\em exact} low-rank matrix. In particular, our analysis for\nthe general low rank matrix sensing problem significantly improves the required\nstorage and computational cost than that required by the RIP-based matrix\nsensing methods \\cite{RechtFP2007}. Finally, we provide empirical validation of\nour approach and demonstrate that alternating minimization is able to recover\nthe true matrix for the above mentioned problems using a small number of\nmeasurements.\n"", '  Robust low-rank matrix completion (RMC), or robust principal component\nanalysis with partially observed data, has been studied extensively for\ncomputer vision, signal processing and machine learning applications. This\nproblem aims to decompose a partially observed matrix into the superposition of\na low-rank matrix and a sparse matrix, where the sparse matrix captures the\ngrossly corrupted entries of the matrix. A widely used approach to tackle RMC\nis to consider a convex formulation, which minimizes the nuclear norm of the\nlow-rank matrix (to promote low-rankness) and the l1 norm of the sparse matrix\n(to promote sparsity). In this paper, motivated by some recent works on\nlow-rank matrix completion and Riemannian optimization, we formulate this\nproblem as a nonsmooth Riemannian optimization problem over Grassmann manifold.\nThis new formulation is scalable because the low-rank matrix is factorized to\nthe multiplication of two much smaller matrices. We then propose an alternating\nmanifold proximal gradient continuation (AManPGC) method to solve the proposed\nnew formulation. The convergence rate of the proposed algorithm is rigorously\nanalyzed. Numerical results on both synthetic data and real data on background\nextraction from surveillance videos are reported to demonstrate the advantages\nof the proposed new formulation and algorithm over several popular existing\napproaches.\n']"
21,705,21_explanations_explanation_interpretability_interpretable,"['explanations', 'explanation', 'interpretability', 'interpretable', 'counterfactual', 'models', 'model', 'machine', 'black', 'explainability']","['counterfactual explanations', 'explainable ai', 'learning models', 'model agnostic', 'neural networks', 'explanation methods', 'explanations', 'explainability', 'prediction', 'predictive']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['Science/Tech', '', '', '', '', '', '', '', '', '']","[""  With machine learning models being increasingly applied to various\ndecision-making scenarios, people have spent growing efforts to make machine\nlearning models more transparent and explainable. Among various explanation\ntechniques, counterfactual explanations have the advantages of being\nhuman-friendly and actionable -- a counterfactual explanation tells the user\nhow to gain the desired prediction with minimal changes to the input. Besides,\ncounterfactual explanations can also serve as efficient probes to the models'\ndecisions. In this work, we exploit the potential of counterfactual\nexplanations to understand and explore the behavior of machine learning models.\nWe design DECE, an interactive visualization system that helps understand and\nexplore a model's decisions on individual instances and data subsets,\nsupporting users ranging from decision-subjects to model developers. DECE\nsupports exploratory analysis of model decisions by combining the strengths of\ncounterfactual explanations at instance- and subgroup-levels. We also introduce\na set of interactions that enable users to customize the generation of\ncounterfactual explanations to find more actionable ones that can suit their\nneeds. Through three use cases and an expert interview, we demonstrate the\neffectiveness of DECE in supporting decision exploration tasks and instance\nexplanations.\n"", '  Systems based on artificial intelligence and machine learning models should\nbe transparent, in the sense of being capable of explaining their decisions to\ngain humans\' approval and trust. While there are a number of explainability\ntechniques that can be used to this end, many of them are only capable of\noutputting a single one-size-fits-all explanation that simply cannot address\nall of the explainees\' diverse needs. In this work we introduce a\nmodel-agnostic and post-hoc local explainability technique for black-box\npredictions called LIMEtree, which employs surrogate multi-output regression\ntrees. We validate our algorithm on a deep neural network trained for object\ndetection in images and compare it against Local Interpretable Model-agnostic\nExplanations (LIME). Our method comes with local fidelity guarantees and can\nproduce a range of diverse explanation types, including contrastive and\ncounterfactual explanations praised in the literature. Some of these\nexplanations can be interactively personalised to create bespoke, meaningful\nand actionable insights into the model\'s behaviour. While other methods may\ngive an illusion of customisability by wrapping, otherwise static, explanations\nin an interactive interface, our explanations are truly interactive, in the\nsense of allowing the user to ""interrogate"" a black-box model. LIMEtree can\ntherefore produce consistent explanations on which an interactive exploratory\nprocess can be built.\n', ""  Transparency is a fundamental requirement for decision making systems when\nthese should be deployed in the real world. It is usually achieved by providing\nexplanations of the system's behavior. A prominent and intuitive type of\nexplanations are counterfactual explanations. Counterfactual explanations\nexplain a behavior to the user by proposing actions -- as changes to the input\n-- that would cause a different (specified) behavior of the system. However,\nsuch explanation methods can be unstable with respect to small changes to the\ninput -- i.e. even a small change in the input can lead to huge or arbitrary\nchanges in the output and of the explanation. This could be problematic for\ncounterfactual explanations, as two similar individuals might get very\ndifferent explanations. Even worse, if the recommended actions differ\nconsiderably in their complexity, one would consider such unstable\n(counterfactual) explanations as individually unfair.\n  In this work, we formally and empirically study the robustness of\ncounterfactual explanations in general, as well as under different models and\ndifferent kinds of perturbations. Furthermore, we propose that plausible\ncounterfactual explanations can be used instead of closest counterfactual\nexplanations to improve the robustness and consequently the individual fairness\nof counterfactual explanations.\n""]"
22,691,22_privacy_private_differential privacy_differential,"['privacy', 'private', 'differential privacy', 'differential', 'differentially', 'differentially private', 'data', 'dp', 'utility', 'preserving']","['differential privacy', 'differentially private', 'private algorithms', 'privacy preserving', 'private learning', 'privacy loss', 'data privacy', 'synthetic data', 'privacy guarantees', 'sample complexity']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['Privacy', '', '', '', '', '', '', '', '', '']","[""  Deep learning models are often trained on datasets that contain sensitive\ninformation such as individuals' shopping transactions, personal contacts, and\nmedical records. An increasingly important line of work therefore has sought to\ntrain neural networks subject to privacy constraints that are specified by\ndifferential privacy or its divergence-based relaxations. These privacy\ndefinitions, however, have weaknesses in handling certain important primitives\n(composition and subsampling), thereby giving loose or complicated privacy\nanalyses of training neural networks. In this paper, we consider a recently\nproposed privacy definition termed \\textit{$f$-differential privacy} [18] for a\nrefined privacy analysis of training neural networks. Leveraging the appealing\nproperties of $f$-differential privacy in handling composition and subsampling,\nthis paper derives analytically tractable expressions for the privacy\nguarantees of both stochastic gradient descent and Adam used in training deep\nneural networks, without the need of developing sophisticated techniques as [3]\ndid. Our results demonstrate that the $f$-differential privacy framework allows\nfor a new privacy analysis that improves on the prior analysis~[3], which in\nturn suggests tuning certain parameters of neural networks for a better\nprediction accuracy without violating the privacy budget. These theoretically\nderived improvements are confirmed by our experiments in a range of tasks in\nimage classification, text classification, and recommender systems. Python code\nto calculate the privacy cost for these experiments is publicly available in\nthe \\texttt{TensorFlow Privacy} library.\n"", '  Attacks that aim to identify the training data of public neural networks\nrepresent a severe threat to the privacy of individuals participating in the\ntraining data set. A possible protection is offered by anonymization of the\ntraining data or training function with differential privacy. However, data\nscientists can choose between local and central differential privacy and need\nto select meaningful privacy parameters $\\epsilon$ which is challenging for\nnon-privacy experts. We empirically compare local and central differential\nprivacy mechanisms under white- and black-box membership inference to evaluate\ntheir relative privacy-accuracy trade-offs. We experiment with several datasets\nand show that this trade-off is similar for both types of mechanisms. This\nsuggests that local differential privacy is a sound alternative to central\ndifferential privacy for differentially private deep learning, since small\n$\\epsilon$ in central differential privacy and large $\\epsilon$ in local\ndifferential privacy result in similar membership inference attack risk.\n', '  Differential privacy is a strong notion for privacy that can be used to prove\nformal guarantees, in terms of a privacy budget, $\\epsilon$, about how much\ninformation is leaked by a mechanism. However, implementations of\nprivacy-preserving machine learning often select large values of $\\epsilon$ in\norder to get acceptable utility of the model, with little understanding of the\nimpact of such choices on meaningful privacy. Moreover, in scenarios where\niterative learning procedures are used, differential privacy variants that\noffer tighter analyses are used which appear to reduce the needed privacy\nbudget but present poorly understood trade-offs between privacy and utility. In\nthis paper, we quantify the impact of these choices on privacy in experiments\nwith logistic regression and neural network models. Our main finding is that\nthere is a huge gap between the upper bounds on privacy loss that can be\nguaranteed, even with advanced mechanisms, and the effective privacy loss that\ncan be measured using current inference attacks. Current mechanisms for\ndifferentially private machine learning rarely offer acceptable utility-privacy\ntrade-offs with guarantees for complex learning tasks: settings that provide\nlimited accuracy loss provide meaningless privacy guarantees, and settings that\nprovide strong privacy guarantees result in useless models. Code for the\nexperiments can be found here: https://github.com/bargavj/EvaluatingDPML\n']"
23,662,23_energy_power_load_electricity,"['energy', 'power', 'load', 'electricity', 'forecasting', 'grid', 'wind', 'consumption', 'solar', 'smart']","['load forecasting', 'energy consumption', 'power grid', 'neural network', 'forecasting', 'renewable energy', 'power flow', 'ieee', 'machine learning', 'power systems']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['Science/Tech', '', '', '', '', '', '', '', '', '']","[""  The rising demand for electricity and its essential nature in today's world\ncalls for intelligent home energy management (HEM) systems that can reduce\nenergy usage. This involves scheduling of loads from peak hours of the day when\nenergy consumption is at its highest to leaner off-peak periods of the day when\nenergy consumption is relatively lower thereby reducing the system's peak load\ndemand, which would consequently result in lesser energy bills, and improved\nload demand profile. This work introduces a novel way to develop a learning\nsystem that can learn from experience to shift loads from one time instance to\nanother and achieve the goal of minimizing the aggregate peak load. This paper\nproposes a Deep Reinforcement Learning (DRL) model for demand response where\nthe virtual agent learns the task like humans do. The agent gets feedback for\nevery action it takes in the environment; these feedbacks will drive the agent\nto learn about the environment and take much smarter steps later in its\nlearning stages. Our method outperformed the state of the art mixed integer\nlinear programming (MILP) for load peak reduction. The authors have also\ndesigned an agent to learn to minimize both consumers' electricity bills and\nutilities' system peak load demand simultaneously. The proposed model was\nanalyzed with loads from five different residential consumers; the proposed\nmethod increases the monthly savings of each consumer by reducing their\nelectricity bill drastically along with minimizing the peak load on the system\nwhen time shiftable loads are handled by the proposed method.\n"", '  The housing structures have changed with urbanization and the growth due to\nthe construction of high-rise buildings all around the world requires end-use\nappliance energy conservation and management in real-time. This shift also came\nalong with smart-meters which enabled the estimation of appliance-specific\npower consumption from the buildings aggregate power consumption reading.\nNon-intrusive load monitoring (NILM) or energy disaggregation is aimed at\nseparating the household energy measured at the aggregate level into\nconstituent appliances. Over the years, signal processing and machine learning\nalgorithms have been combined to achieve this. Incredible research and\npublications have been conducted on energy disaggregation, non-intrusive load\nmonitoring, home energy management and appliance classification. There exists\nan API, NILMTK, a reproducible benchmark algorithm for the same. Many other\napproaches to perform energy disaggregation has been adapted such as deep\nneural network architectures and big data approach for household energy\ndisaggregation. This paper provides a survey of the effective NILM system\nframeworks and reviews the performance of the benchmark algorithms in a\ncomprehensive manner. This paper also summarizes the wide application scope and\nthe effectiveness of the algorithmic performance on three publicly available\ndata sets.\n', '  Energy disaggregation, a.k.a. Non-Intrusive Load Monitoring, aims to separate\nthe energy consumption of individual appliances from the readings of a mains\npower meter measuring the total energy consumption of, e.g. a whole house.\nEnergy consumption of individual appliances can be useful in many applications,\ne.g., providing appliance-level feedback to the end users to help them\nunderstand their energy consumption and ultimately save energy. Recently, with\nthe availability of large-scale energy consumption datasets, various neural\nnetwork models such as convolutional neural networks and recurrent neural\nnetworks have been investigated to solve the energy disaggregation problem.\nNeural network models can learn complex patterns from large amounts of data and\nhave been shown to outperform the traditional machine learning methods such as\nvariants of hidden Markov models. However, current neural network methods for\nenergy disaggregation are either computational expensive or are not capable of\nhandling long-term dependencies. In this paper, we investigate the application\nof the recently developed WaveNet models for the task of energy disaggregation.\nBased on a real-world energy dataset collected from 20 households over two\nyears, we show that WaveNet models outperforms the state-of-the-art deep\nlearning methods proposed in the literature for energy disaggregation in terms\nof both error measures and computational cost. On the basis of energy\ndisaggregation, we then investigate the performance of two deep-learning based\nframeworks for the task of on/off detection which aims at estimating whether an\nappliance is in operation or not. Based on the same dataset, we show that for\nthe task of on/off detection the second framework, i.e., directly training a\nbinary classifier, achieves better performance in terms of F1 score.\n']"
24,630,24_causal_treatment_observational_effect,"['causal', 'treatment', 'observational', 'effect', 'variables', 'causal inference', 'effects', 'observational data', 'data', 'confounders']","['causal models', 'causal model', 'causal inference', 'causal effects', 'causal effect', 'causal discovery', 'causal relationships', 'confounders', 'causal structure', 'causal graph']","['causal inference', '', '', '', '', '', '', '', '', '']","['causality', '', '', '', '', '', '', '', '', '']","['  Estimating individual treatment effects from data of randomized experiments\nis a critical task in causal inference. The Stable Unit Treatment Value\nAssumption (SUTVA) is usually made in causal inference. However, interference\ncan introduce bias when the assigned treatment on one unit affects the\npotential outcomes of the neighboring units. This interference phenomenon is\nknown as spillover effect in economics or peer effect in social science.\nUsually, in randomized experiments or observational studies with interconnected\nunits, one can only observe treatment responses under interference. Hence, how\nto estimate the superimposed causal effect and recover the individual treatment\neffect in the presence of interference becomes a challenging task in causal\ninference. In this work, we study causal effect estimation under general\nnetwork interference using GNNs, which are powerful tools for capturing the\ndependency in the graph. After deriving causal effect estimators, we further\nstudy intervention policy improvement on the graph under capacity constraint.\nWe give policy regret bounds under network interference and treatment capacity\nconstraint. Furthermore, a heuristic graph structure-dependent error bound for\nGNN-based causal estimators is provided.\n', '  Unobserved confounding is one of the main challenges when estimating causal\neffects. We propose a causal reduction method that, given a causal model,\nreplaces an arbitrary number of possibly high-dimensional latent confounders\nwith a single latent confounder that takes values in the same space as the\ntreatment variable, without changing the observational and interventional\ndistributions the causal model entails. This allows us to estimate the causal\neffect in a principled way from combined data without relying on the common but\noften unrealistic assumption that all confounders have been observed. We apply\nour causal reduction in three different settings. In the first setting, we\nassume the treatment and outcome to be discrete. The causal reduction then\nimplies bounds between the observational and interventional distributions that\ncan be exploited for estimation purposes. In certain cases with highly\nunbalanced observational samples, the accuracy of the causal effect estimate\ncan be improved by incorporating observational data. Second, for continuous\nvariables and assuming a linear-Gaussian model, we derive equality constraints\nfor the parameters of the observational and interventional distributions.\nThird, for the general continuous setting (possibly nonlinear or non-Gaussian),\nwe parameterize the reduced causal model using normalizing flows, a flexible\nclass of easily invertible nonlinear transformations. We perform a series of\nexperiments on synthetic data and find that in several cases the number of\ninterventional samples can be reduced when adding observational training\nsamples without sacrificing accuracy.\n', ""  An important problem in causal inference is to break down the total effect of\na treatment on an outcome into different causal pathways and to quantify the\ncausal effect in each pathway. For instance, in causal fairness, the total\neffect of being a male employee (i.e., treatment) constitutes its direct effect\non annual income (i.e., outcome) and the indirect effect via the employee's\noccupation (i.e., mediator). Causal mediation analysis (CMA) is a formal\nstatistical framework commonly used to reveal such underlying causal\nmechanisms. One major challenge of CMA in observational studies is handling\nconfounders, variables that cause spurious causal relationships among\ntreatment, mediator, and outcome. Conventional methods assume sequential\nignorability that implies all confounders can be measured, which is often\nunverifiable in practice. This work aims to circumvent the stringent sequential\nignorability assumptions and consider hidden confounders. Drawing upon proxy\nstrategies and recent advances in deep learning, we propose to simultaneously\nuncover the latent variables that characterize hidden confounders and estimate\nthe causal effects. Empirical evaluations using both synthetic and\nsemi-synthetic datasets validate the effectiveness of the proposed method. We\nfurther show the potentials of our approach for causal fairness analysis.\n""]"
25,621,25_kernel_regression_lasso_kernels,"['kernel', 'regression', 'lasso', 'kernels', 'sparse', 'linear', 'norm', 'random', 'regularization', 'ridge']","['kernel approximation', 'kernel learning', 'kernel matrix', 'kernel ridge', 'kernel methods', 'ridge regression', 'reproducing kernel', 'kernels', 'regression', 'group lasso']","['[TABLECONTEXT]', '', '', '', '', '', '', '', '', '']","['kernel ridge regression', '', '', '', '', '', '', '', '', '']","['  We consider supervised learning problems within the positive-definite kernel\nframework, such as kernel ridge regression, kernel logistic regression or the\nsupport vector machine. With kernels leading to infinite-dimensional feature\nspaces, a common practical limiting difficulty is the necessity of computing\nthe kernel matrix, which most frequently leads to algorithms with running time\nat least quadratic in the number of observations n, i.e., O(n^2). Low-rank\napproximations of the kernel matrix are often considered as they allow the\nreduction of running time complexities to O(p^2 n), where p is the rank of the\napproximation. The practicality of such methods thus depends on the required\nrank p. In this paper, we show that in the context of kernel ridge regression,\nfor approximations based on a random subset of columns of the original kernel\nmatrix, the rank p may be chosen to be linear in the degrees of freedom\nassociated with the problem, a quantity which is classically used in the\nstatistical analysis of such methods, and is often seen as the implicit number\nof parameters of non-parametric estimators. This result enables simple\nalgorithms that have sub-quadratic running time complexity, but provably\nexhibit the same predictive performance than existing algorithms, for any given\nproblem instance, and not only for worst-case situations.\n', '  Efficient and accurate low-rank approximations of multiple data sources are\nessential in the era of big data. The scaling of kernel-based learning\nalgorithms to large datasets is limited by the O(n^2) computation and storage\ncomplexity of the full kernel matrix, which is required by most of the recent\nkernel learning algorithms.\n  We present the Mklaren algorithm to approximate multiple kernel matrices\nlearn a regression model, which is entirely based on geometrical concepts. The\nalgorithm does not require access to full kernel matrices yet it accounts for\nthe correlations between all kernels. It uses Incomplete Cholesky\ndecomposition, where pivot selection is based on least-angle regression in the\ncombined, low-dimensional feature space. The algorithm has linear complexity in\nthe number of data points and kernels. When explicit feature space induced by\nthe kernel can be constructed, a mapping from the dual to the primal Ridge\nregression weights is used for model interpretation.\n  The Mklaren algorithm was tested on eight standard regression datasets. It\noutperforms contemporary kernel matrix approximation approaches when learning\nwith multiple kernels. It identifies relevant kernels, achieving highest\nexplained variance than other multiple kernel learning methods for the same\nnumber of iterations. Test accuracy, equivalent to the one using full kernel\nmatrices, was achieved with at significantly lower approximation ranks. A\ndifference in run times of two orders of magnitude was observed when either the\nnumber of samples or kernels exceeds 3000.\n', '  This paper generalizes regularized regression problems in a hyper-reproducing\nkernel Hilbert space (hyper-RKHS), illustrates its utility for kernel learning\nand out-of-sample extensions, and proves asymptotic convergence results for the\nintroduced regression models in an approximation theory view. Algorithmically,\nwe consider two regularized regression models with bivariate forms in this\nspace, including kernel ridge regression (KRR) and support vector regression\n(SVR) endowed with hyper-RKHS, and further combine divide-and-conquer with\nNystr\\""{o}m approximation for scalability in large sample cases. This framework\nis general: the underlying kernel is learned from a broad class, and can be\npositive definite or not, which adapts to various requirements in kernel\nlearning. Theoretically, we study the convergence behavior of regularized\nregression algorithms in hyper-RKHS and derive the learning rates, which goes\nbeyond the classical analysis on RKHS due to the non-trivial independence of\npairwise samples and the characterisation of hyper-RKHS. Experimentally,\nresults on several benchmarks suggest that the employed framework is able to\nlearn a general kernel function form an arbitrary similarity matrix, and thus\nachieves a satisfactory performance on classification tasks.\n']"
26,615,26_fairness_fair_bias_discrimination,"['fairness', 'fair', 'bias', 'discrimination', 'protected', 'decision', 'groups', 'algorithmic', 'sensitive', 'machine learning']","['fairness aware', 'group fairness', 'algorithmic fairness', 'fairness metrics', 'individual fairness', 'accuracy fairness', 'fairness constraints', 'fairness criteria', 'fairness', 'notions fairness']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['Fairness in machine learning', '', '', '', '', '', '', '', '', '']","['  The rise of algorithmic decision-making has spawned much research on fair\nmachine learning (ML). Financial institutions use ML for building risk\nscorecards that support a range of credit-related decisions. Yet, the\nliterature on fair ML in credit scoring is scarce. The paper makes three\ncontributions. First, we revisit statistical fairness criteria and examine\ntheir adequacy for credit scoring. Second, we catalog algorithmic options for\nincorporating fairness goals in the ML model development pipeline. Last, we\nempirically compare different fairness processors in a profit-oriented credit\nscoring context using real-world data. The empirical results substantiate the\nevaluation of fairness measures, identify suitable options to implement fair\ncredit scoring, and clarify the profit-fairness trade-off in lending decisions.\nWe find that multiple fairness criteria can be approximately satisfied at once\nand recommend separation as a proper criterion for measuring the fairness of a\nscorecard. We also find fair in-processors to deliver a good balance between\nprofit and fairness and show that algorithmic discrimination can be reduced to\na reasonable level at a relatively low cost. The codes corresponding to the\npaper are available on GitHub.\n', '  Using the concept of principal stratification from the causal inference\nliterature, we introduce a new notion of fairness, called principal fairness,\nfor human and algorithmic decision-making. The key idea is that one should not\ndiscriminate among individuals who would be similarly affected by the decision.\nUnlike the existing statistical definitions of fairness, principal fairness\nexplicitly accounts for the fact that individuals can be impacted by the\ndecision. Furthermore, we explain how principal fairness differs from the\nexisting causality-based fairness criteria. In contrast to the counterfactual\nfairness criteria, for example, principal fairness considers the effects of\ndecision in question rather than those of protected attributes of interest. We\nbriefly discuss how to approach empirical evaluation and policy learning\nproblems under the proposed principal fairness criterion.\n', '  Machine learning (ML) is increasingly being used to make decisions in our\nsociety. ML models, however, can be unfair to certain demographic groups (e.g.,\nAfrican Americans or females) according to various fairness metrics. Existing\ntechniques for producing fair ML models either are limited to the type of\nfairness constraints they can handle (e.g., preprocessing) or require\nnontrivial modifications to downstream ML training algorithms (e.g.,\nin-processing).\n  We propose a declarative system OmniFair for supporting group fairness in ML.\nOmniFair features a declarative interface for users to specify desired group\nfairness constraints and supports all commonly used group fairness notions,\nincluding statistical parity, equalized odds, and predictive parity. OmniFair\nis also model-agnostic in the sense that it does not require modifications to a\nchosen ML algorithm. OmniFair also supports enforcing multiple user declared\nfairness constraints simultaneously while most previous techniques cannot. The\nalgorithms in OmniFair maximize model accuracy while meeting the specified\nfairness constraints, and their efficiency is optimized based on the\ntheoretically provable monotonicity property regarding the trade-off between\naccuracy and fairness that is unique to our system.\n  We conduct experiments on commonly used datasets that exhibit bias against\nminority groups in the fairness literature. We show that OmniFair is more\nversatile than existing algorithmic fairness approaches in terms of both\nsupported fairness constraints and downstream ML models. OmniFair reduces the\naccuracy loss by up to $94.8\\%$ compared with the second best method. OmniFair\nalso achieves similar running time to preprocessing methods, and is up to\n$270\\times$ faster than in-processing methods.\n']"
27,609,27_distributed_communication_sgd_gradient,"['distributed', 'communication', 'sgd', 'gradient', 'convergence', 'decentralized', 'stochastic', 'workers', 'asynchronous', 'local']","['distributed optimization', 'distributed training', 'distributed stochastic', 'distributed sgd', 'distributed learning', 'stochastic gradient', 'gradient descent', 'distributed', 'convergence rate', 'gradients']","['Distributed Learning', '', '', '', '', '', '', '', '', '']","['Agrarian gradient descent', '', '', '', '', '', '', '', '', '']","['  With the increase in the amount of data and the expansion of model scale,\ndistributed parallel training becomes an important and successful technique to\naddress the optimization challenges. Nevertheless, although distributed\nstochastic gradient descent (SGD) algorithms can achieve a linear iteration\nspeedup, they are limited significantly in practice by the communication cost,\nmaking it difficult to achieve a linear time speedup. In this paper, we propose\na computation and communication decoupled stochastic gradient descent\n(CoCoD-SGD) algorithm to run computation and communication in parallel to\nreduce the communication cost. We prove that CoCoD-SGD has a linear iteration\nspeedup with respect to the total computation capability of the hardware\nresources. In addition, it has a lower communication complexity and better time\nspeedup comparing with traditional distributed SGD algorithms. Experiments on\ndeep neural network training demonstrate the significant improvements of\nCoCoD-SGD: when training ResNet18 and VGG16 with 16 Geforce GTX 1080Ti GPUs,\nCoCoD-SGD is up to 2-3$\\times$ faster than traditional synchronous SGD.\n', '  Recent developments on large-scale distributed machine learning applications,\ne.g., deep neural networks, benefit enormously from the advances in distributed\nnon-convex optimization techniques, e.g., distributed Stochastic Gradient\nDescent (SGD). A series of recent works study the linear speedup property of\ndistributed SGD variants with reduced communication. The linear speedup\nproperty enable us to scale out the computing capability by adding more\ncomputing nodes into our system. The reduced communication complexity is\ndesirable since communication overhead is often the performance bottleneck in\ndistributed systems. Recently, momentum methods are more and more widely\nadopted in training machine learning models and can often converge faster and\ngeneralize better. For example, many practitioners use distributed SGD with\nmomentum to train deep neural networks with big data. However, it remains\nunclear whether any distributed momentum SGD possesses the same linear speedup\nproperty as distributed SGD and has reduced communication complexity. This\npaper fills the gap by considering a distributed communication efficient\nmomentum SGD method and proving its linear speedup property.\n', '  Due to its efficiency and ease to implement, stochastic gradient descent\n(SGD) has been widely used in machine learning. In particular, SGD is one of\nthe most popular optimization methods for distributed learning. Recently,\nquantized SGD (QSGD), which adopts quantization to reduce the communication\ncost in SGD-based distributed learning, has attracted much attention. Although\nseveral QSGD methods have been proposed, some of them are heuristic without\ntheoretical guarantee, and others have high quantization variance which makes\nthe convergence become slow. In this paper, we propose a new method, called\nQuantized Epoch-SGD (QESGD), for communication-efficient distributed learning.\nQESGD compresses (quantizes) the parameter with variance reduction, so that it\ncan get almost the same performance as that of SGD with less communication\ncost. QESGD is implemented on the Parameter Server framework, and empirical\nresults on distributed deep learning show that QESGD can outperform other\nstate-of-the-art quantization methods to achieve the best performance.\n']"
28,609,28_covid_covid 19_19_pandemic,"['covid', 'covid 19', '19', 'pandemic', 'disease', 'chest', 'ct', 'coronavirus', 'patients', 'infection']","['sars cov', 'covid 19', 'covid19', 'cov', '2019 covid', '19 patients', 'coronavirus', 'covid', 'coronavirus disease', 'epidemic']","['COVID 19', '', '', '', '', '', '', '', '', '']","['COVID-19', '', '', '', '', '', '', '', '', '']","['  In 2019, the entire world is facing a situation of health emergency due to a\nnewly emerged coronavirus (COVID-19). Almost 196 countries are affected by\ncovid-19, while USA, Italy, China, Spain, Iran, and France have the maximum\nactive cases of COVID-19. The issues, medical and healthcare departments are\nfacing in delay of detecting the COVID-19. Several artificial intelligence\nbased system are designed for the automatic detection of COVID-19 using chest\nx-rays. In this article we will discuss the different approaches used for the\ndetection of COVID-19 and the challenges we are facing. It is mandatory to\ndevelop an automatic detection system to prevent the transfer of the virus\nthrough contact. Several deep learning architecture are deployed for the\ndetection of COVID-19 such as ResNet, Inception, Googlenet etc. All these\napproaches are detecting the subjects suffering with pneumonia while its hard\nto decide whether the pneumonia is caused by COVID-19 or due to any other\nbacterial or fungal attack.\n', '  The 2019 novel coronavirus (COVID-19) has spread rapidly all over the world\nand it is affecting the whole society. The current gold standard test for\nscreening COVID-19 patients is the polymerase chain reaction test. However, the\nCOVID-19 test kits are not widely available and time-consuming. Thus, as an\nalternative, chest X-rays are being considered for quick screening. Since the\npresentation of COVID-19 in chest X-rays is varied in features and\nspecialization in reading COVID-19 chest X-rays are required thus limiting its\nuse for diagnosis. To address this challenge of reading chest X-rays by\nradiologists quickly, we present a multi-channel transfer learning model based\non ResNet architecture to facilitate the diagnosis of COVID-19 chest X-ray.\nThree ResNet-based models (Models a, b, and c) were retrained using Dataset_A\n(1579 normal and 4429 diseased), Dataset_B (4245 pneumonia and 1763\nnon-pneumonia), and Dataset_C (184 COVID-19 and 5824 Non-COVID19),\nrespectively, to classify (a) normal or diseased, (b) pneumonia or\nnon-pneumonia, and (c) COVID-19 or non-COVID19. Finally, these three models\nwere ensembled and fine-tuned using Dataset_D (1579 normal, 4245 pneumonia, and\n184 COVID-19) to classify normal, pneumonia, and COVID-19 cases. Our results\nshow that the ensemble model is more accurate than the single ResNet model,\nwhich is also re-trained using Dataset_D as it extracts more relevant semantic\nfeatures for each class. Our approach provides a precision of 94 % and a recall\nof 100%. Thus, our method could potentially help clinicians in screening\npatients for COVID-19, thus facilitating immediate triaging and treatment for\nbetter outcomes.\n', '  Objectives: To investigate machine-learning classifiers and interpretable\nmodels using chest CT for detection of COVID-19 and differentiation from other\npneumonias, ILD and normal CTs.\n  Methods: Our retrospective multi-institutional study obtained 2096 chest CTs\nfrom 16 institutions (including 1077 COVID-19 patients). Training/testing\ncohorts included 927/100 COVID-19, 388/33 ILD, 189/33 other pneumonias, and\n559/34 normal (no pathologies) CTs. A metric-based approach for classification\nof COVID-19 used interpretable features, relying on logistic regression and\nrandom forests. A deep learning-based classifier differentiated COVID-19 via 3D\nfeatures extracted directly from CT attenuation and probability distribution of\nairspace opacities.\n  Results: Most discriminative features of COVID-19 are percentage of airspace\nopacity and peripheral and basal predominant opacities, concordant with the\ntypical characterization of COVID-19 in the literature. Unsupervised\nhierarchical clustering compares feature distribution across COVID-19 and\ncontrol cohorts. The metrics-based classifier achieved AUC=0.83,\nsensitivity=0.74, and specificity=0.79 of versus respectively 0.93, 0.90, and\n0.83 for the DL-based classifier. Most of ambiguity comes from non-COVID-19\npneumonia with manifestations that overlap with COVID-19, as well as mild\nCOVID-19 cases. Non-COVID-19 classification performance is 91% for ILD, 64% for\nother pneumonias and 94% for no pathologies, which demonstrates the robustness\nof our method against different compositions of control groups.\n  Conclusions: Our new method accurately discriminates COVID-19 from other\ntypes of pneumonia, ILD, and no pathologies CTs, using quantitative imaging\nfeatures derived from chest CT, while balancing interpretability of results and\nclassification performance, and therefore may be useful to facilitate diagnosis\nof COVID-19.\n']"
29,586,29_anomaly_anomaly detection_detection_anomalies,"['anomaly', 'anomaly detection', 'detection', 'anomalies', 'outlier', 'data', 'process', 'outlier detection', 'anomalous', 'normal']","['unsupervised anomaly', 'anomaly detection', 'based anomaly', 'outliers', 'outlier detection', 'detect anomalies', 'unsupervised', 'detection algorithms', 'outlier', 'anomaly']","['Anomaly detection', '', '', '', '', '', '', '', '', '']","['Anomaly detection', '', '', '', '', '', '', '', '', '']","['  Anomaly detection is typically posited as an unsupervised learning task in\nthe literature due to the prohibitive cost and difficulty to obtain large-scale\nlabeled anomaly data, but this ignores the fact that a very small number\n(e.g.,, a few dozens) of labeled anomalies can often be made available with\nsmall/trivial cost in many real-world anomaly detection applications. To\nleverage such labeled anomaly data, we study an important anomaly detection\nproblem termed weakly-supervised anomaly detection, in which, in addition to a\nlarge amount of unlabeled data, a limited number of labeled anomalies are\navailable during modeling. Learning with the small labeled anomaly data enables\nanomaly-informed modeling, which helps identify anomalies of interest and\naddress the notorious high false positives in unsupervised anomaly detection.\nHowever, the problem is especially challenging, since (i) the limited amount of\nlabeled anomaly data often, if not always, cannot cover all types of anomalies\nand (ii) the unlabeled data is often dominated by normal instances but has\nanomaly contamination. We address the problem by formulating it as a pairwise\nrelation prediction task. Particularly, our approach defines a two-stream\nordinal regression neural network to learn the relation of randomly sampled\ninstance pairs, i.e., whether the instance pair contains two labeled anomalies,\none labeled anomaly, or just unlabeled data instances. The resulting model\neffectively leverages both the labeled and unlabeled data to substantially\naugment the training data and learn well-generalized representations of\nnormality and abnormality. Comprehensive empirical results on 40 real-world\ndatasets show that our approach (i) significantly outperforms four\nstate-of-the-art methods in detecting both of the known and previously unseen\nanomalies and (ii) is substantially more data-efficient.\n', '  Anomaly detection aims to find instances that are considered unusual and is a\nfundamental problem of data science. Recently, deep anomaly detection methods\nwere shown to achieve superior results particularly in complex data such as\nimages. Our work focuses on deep one-class classification for anomaly detection\nwhich learns a mapping only from the normal samples. However, the non-linear\ntransformation performed by deep learning can potentially find patterns\nassociated with social bias. The challenge with adding fairness to deep anomaly\ndetection is to ensure both making fair and correct anomaly predictions\nsimultaneously. In this paper, we propose a new architecture for the fair\nanomaly detection approach (Deep Fair SVDD) and train it using an adversarial\nnetwork to de-correlate the relationships between the sensitive attributes and\nthe learned representations. This differs from how fairness is typically added\nnamely as a regularizer or a constraint. Further, we propose two effective\nfairness measures and empirically demonstrate that existing deep anomaly\ndetection methods are unfair. We show that our proposed approach can remove the\nunfairness largely with minimal loss on the anomaly detection performance.\nLastly, we conduct an in-depth analysis to show the strength and limitations of\nour proposed model, including parameter analysis, feature visualization, and\nrun-time analysis.\n', '  Although deep learning has been applied to successfully address many data\nmining problems, relatively limited work has been done on deep learning for\nanomaly detection. Existing deep anomaly detection methods, which focus on\nlearning new feature representations to enable downstream anomaly detection\nmethods, perform indirect optimization of anomaly scores, leading to\ndata-inefficient learning and suboptimal anomaly scoring. Also, they are\ntypically designed as unsupervised learning due to the lack of large-scale\nlabeled anomaly data. As a result, they are difficult to leverage prior\nknowledge (e.g., a few labeled anomalies) when such information is available as\nin many real-world anomaly detection applications.\n  This paper introduces a novel anomaly detection framework and its\ninstantiation to address these problems. Instead of representation learning,\nour method fulfills an end-to-end learning of anomaly scores by a neural\ndeviation learning, in which we leverage a few (e.g., multiple to dozens)\nlabeled anomalies and a prior probability to enforce statistically significant\ndeviations of the anomaly scores of anomalies from that of normal data objects\nin the upper tail. Extensive results show that our method can be trained\nsubstantially more data-efficiently and achieves significantly better anomaly\nscoring than state-of-the-art competing methods.\n']"
30,585,30_quantum_classical_machine_machine learning,"['quantum', 'classical', 'machine', 'machine learning', 'states', 'learning', 'circuit', 'circuits', 'quantum machine', 'quantum circuits']","['learning quantum', 'quantum neural', 'quantum computing', 'quantum computers', 'quantum algorithms', 'quantum circuits', 'quantum annealing', 'quantum algorithm', 'quantum computer', 'variational quantum']","['quantum generative model', '', '', '', '', '', '', '', '', '']","['quantum machine learning', '', '', '', '', '', '', '', '', '']","['  Classification of quantum data is essential for quantum machine learning and\nnear-term quantum technologies. In this paper, we propose a new hybrid\nquantum-classical framework for supervised quantum learning, which we call\nVariational Shadow Quantum Learning (VSQL). Our method in particular utilizes\nthe classical shadows of quantum data, which fundamentally represent the side\ninformation of quantum data with respect to certain physical observables.\nSpecifically, we first use variational shadow quantum circuits to extract\nclassical features in a convolution way and then utilize a fully-connected\nneural network to complete the classification task. We show that this method\ncould sharply reduce the number of parameters and thus better facilitate\nquantum circuit training. Simultaneously, less noise will be introduced since\nfewer quantum gates are employed in such shadow circuits. Moreover, we show\nthat the Barren Plateau issue, a significant gradient vanishing problem in\nquantum machine learning, could be avoided in VSQL. Finally, we demonstrate the\nefficiency of VSQL in quantum classification via numerical experiments on the\nclassification of quantum states and the recognition of multi-labeled\nhandwritten digits. In particular, our VSQL approach outperforms existing\nvariational quantum classifiers in the test accuracy in the binary case of\nhandwritten digit recognition and notably requires much fewer parameters.\n', '  The study of quantum generative models is well-motivated, not only because of\nits importance in quantum machine learning and quantum chemistry but also\nbecause of the perspective of its implementation on near-term quantum machines.\nInspired by previous studies on the adversarial training of classical and\nquantum generative models, we propose the first design of quantum Wasserstein\nGenerative Adversarial Networks (WGANs), which has been shown to improve the\nrobustness and the scalability of the adversarial training of quantum\ngenerative models even on noisy quantum hardware. Specifically, we propose a\ndefinition of the Wasserstein semimetric between quantum data, which inherits a\nfew key theoretical merits of its classical counterpart. We also demonstrate\nhow to turn the quantum Wasserstein semimetric into a concrete design of\nquantum WGANs that can be efficiently implemented on quantum machines. Our\nnumerical study, via classical simulation of quantum systems, shows the more\nrobust and scalable numerical performance of our quantum WGANs over other\nquantum GAN proposals. As a surprising application, our quantum WGAN has been\nused to generate a 3-qubit quantum circuit of ~50 gates that well approximates\na 3-qubit 1-d Hamiltonian simulation circuit that requires over 10k gates using\nstandard techniques.\n', '  We introduce TensorFlow Quantum (TFQ), an open source library for the rapid\nprototyping of hybrid quantum-classical models for classical or quantum data.\nThis framework offers high-level abstractions for the design and training of\nboth discriminative and generative quantum models under TensorFlow and supports\nhigh-performance quantum circuit simulators. We provide an overview of the\nsoftware architecture and building blocks through several examples and review\nthe theory of hybrid quantum-classical neural networks. We illustrate TFQ\nfunctionalities via several basic applications including supervised learning\nfor quantum classification, quantum control, simulating noisy quantum circuits,\nand quantum approximate optimization. Moreover, we demonstrate how one can\napply TFQ to tackle advanced quantum learning tasks including meta-learning,\nlayerwise learning, Hamiltonian learning, sampling thermal states, variational\nquantum eigensolvers, classification of quantum phase transitions, generative\nadversarial networks, and reinforcement learning. We hope this framework\nprovides the necessary tools for the quantum computing and machine learning\nresearch communities to explore models of both natural and artificial quantum\nsystems, and ultimately discover new quantum algorithms which could potentially\nyield a quantum advantage.\n']"
31,555,31_segmentation_video_object_semantic,"['segmentation', 'video', 'object', 'semantic', 'action', 'semantic segmentation', 'detection', 'image', 'object detection', 'temporal']","['semantic segmentation', 'image segmentation', 'object detection', 'segmentation', 'instance segmentation', 'convolutional', 'action recognition', 'unsupervised', 'localization', 'detection']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['Video segmentation', '', '', '', '', '', '', '', '', '']","['  Self-supervised learning allows for better utilization of unlabelled data.\nThe feature representation obtained by self-supervision can be used in\ndownstream tasks such as classification, object detection, segmentation, and\nanomaly detection. While classification, object detection, and segmentation\nhave been investigated with self-supervised learning, anomaly detection needs\nmore attention. We consider the problem of anomaly detection in images and\nvideos, and present a new visual anomaly detection technique for videos.\nNumerous seminal and state-of-the-art self-supervised methods are evaluated for\nanomaly detection on a variety of image datasets. The best performing\nimage-based self-supervised representation learning method is then used for\nvideo anomaly detection to see the importance of spatial features in visual\nanomaly detection in videos. We also propose a simple self-supervision approach\nfor learning temporal coherence across video frames without the use of any\noptical flow information. At its core, our method identifies the frame indices\nof a jumbled video sequence allowing it to learn the spatiotemporal features of\nthe video. This intuitive approach shows superior performance of visual anomaly\ndetection compared to numerous methods for images and videos on UCF101 and\nILSVRC2015 video datasets.\n', '  Object detection or localization is an incremental step in progression from\ncoarse to fine digital image inference. It not only provides the classes of the\nimage objects, but also provides the location of the image objects which have\nbeen classified. The location is given in the form of bounding boxes or\ncentroids. Semantic segmentation gives fine inference by predicting labels for\nevery pixel in the input image. Each pixel is labelled according to the object\nclass within which it is enclosed. Furthering this evolution, instance\nsegmentation gives different labels for separate instances of objects belonging\nto the same class. Hence, instance segmentation may be defined as the technique\nof simultaneously solving the problem of object detection as well as that of\nsemantic segmentation. In this survey paper on instance segmentation -- its\nbackground, issues, techniques, evolution, popular datasets, related work up to\nthe state of the art and future scope have been discussed. The paper provides\nvaluable information for those who want to do research in the field of instance\nsegmentation.\n', '  A major challenge for video semantic segmentation is the lack of labeled\ndata. In most benchmark datasets, only one frame of a video clip is annotated,\nwhich makes most supervised methods fail to utilize information from the rest\nof the frames. To exploit the spatio-temporal information in videos, many\nprevious works use pre-computed optical flows, which encode the temporal\nconsistency to improve the video segmentation. However, the video segmentation\nand optical flow estimation are still considered as two separate tasks. In this\npaper, we propose a novel framework for joint video semantic segmentation and\noptical flow estimation. Semantic segmentation brings semantic information to\nhandle occlusion for more robust optical flow estimation, while the\nnon-occluded optical flow provides accurate pixel-level temporal\ncorrespondences to guarantee the temporal consistency of the segmentation.\nMoreover, our framework is able to utilize both labeled and unlabeled frames in\nthe video through joint training, while no additional calculation is required\nin inference. Extensive experiments show that the proposed model makes the\nvideo semantic segmentation and optical flow estimation benefit from each other\nand outperforms existing methods under the same settings in both tasks.\n']"
32,546,32_federated_federated learning_fl_clients,"['federated', 'federated learning', 'fl', 'clients', 'privacy', 'learning', 'communication', 'client', 'server', 'data']","['federated learning', 'federated', 'federated averaging', 'differential privacy', 'decentralized', 'privacy preserving', 'model training', 'machine learning', 'local models', 'distributed']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['Federated learning', '', '', '', '', '', '', '', '', '']","['  The high demand of artificial intelligence services at the edges that also\npreserve data privacy has pushed the research on novel machine learning\nparadigms that fit those requirements. Federated learning has the ambition to\nprotect data privacy through distributed learning methods that keep the data in\ntheir data silos. Likewise, differential privacy attains to improve the\nprotection of data privacy by measuring the privacy loss in the communication\namong the elements of federated learning. The prospective matching of federated\nlearning and differential privacy to the challenges of data privacy protection\nhas caused the release of several software tools that support their\nfunctionalities, but they lack of the needed unified vision for those\ntechniques, and a methodological workflow that support their use. Hence, we\npresent the Sherpa.ai Federated Learning framework that is built upon an\nholistic view of federated learning and differential privacy. It results from\nthe study of how to adapt the machine learning paradigm to federated learning,\nand the definition of methodological guidelines for developing artificial\nintelligence services based on federated learning and differential privacy. We\nshow how to follow the methodological guidelines with the Sherpa.ai Federated\nLearning framework by means of a classification and a regression use cases.\n', ""  Today's AI still faces two major challenges. One is that in most industries,\ndata exists in the form of isolated islands. The other is the strengthening of\ndata privacy and security. We propose a possible solution to these challenges:\nsecure federated learning. Beyond the federated learning framework first\nproposed by Google in 2016, we introduce a comprehensive secure federated\nlearning framework, which includes horizontal federated learning, vertical\nfederated learning and federated transfer learning. We provide definitions,\narchitectures and applications for the federated learning framework, and\nprovide a comprehensive survey of existing works on this subject. In addition,\nwe propose building data networks among organizations based on federated\nmechanisms as an effective solution to allow knowledge to be shared without\ncompromising user privacy.\n"", '  Federated learning enables multiple parties to collaboratively learn a model\nwithout exchanging their data. While most existing federated learning\nalgorithms need many rounds to converge, one-shot federated learning (i.e.,\nfederated learning with a single communication round) is a promising approach\nto make federated learning applicable in cross-silo setting in practice.\nHowever, existing one-shot algorithms only support specific models and do not\nprovide any privacy guarantees, which significantly limit the applications in\npractice. In this paper, we propose a practical one-shot federated learning\nalgorithm named FedKT. By utilizing the knowledge transfer technique, FedKT can\nbe applied to any classification models and can flexibly achieve differential\nprivacy guarantees. Our experiments on various tasks show that FedKT can\nsignificantly outperform the other state-of-the-art federated learning\nalgorithms with a single communication round.\n']"
33,520,33_water_seismic_weather_climate,"['water', 'seismic', 'weather', 'climate', 'data', 'models', 'model', 'precipitation', 'prediction', 'flood']","['neural networks', 'neural network', 'deep learning', 'forecasting', 'forecasts', 'prediction', 'regression', 'seismic data', 'modeling', 'models']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['  The success of deep learning techniques over the last decades has opened up a\nnew avenue of research for weather forecasting. Here, we take the novel\napproach of using a neural network to predict full probability density\nfunctions at each point in space and time rather than a single output value,\nthus producing a probabilistic weather forecast. This enables the calculation\nof both uncertainty and skill metrics for the neural network predictions, and\novercomes the common difficulty of inferring uncertainty from these\npredictions.\n  This approach is data-driven and the neural network is trained on the\nWeatherBench dataset (processed ERA5 data) to forecast geopotential and\ntemperature 3 and 5 days ahead. Data exploration leads to the identification of\nthe most important input variables, which are also found to agree with physical\nreasoning, thereby validating our approach. In order to increase computational\nefficiency further, each neural network is trained on a small subset of these\nvariables. The outputs are then combined through a stacked neural network, the\nfirst time such a technique has been applied to weather data. Our approach is\nfound to be more accurate than some numerical weather prediction models and as\naccurate as more complex alternative neural networks, with the added benefit of\nproviding key probabilistic information necessary for making informed weather\nforecasts.\n', '  Seismic velocity is one of the most important parameters used in seismic\nexploration. Accurate velocity models are key prerequisites for reverse-time\nmigration and other high-resolution seismic imaging techniques. Such velocity\ninformation has traditionally been derived by tomography or full-waveform\ninversion (FWI), which are time consuming and computationally expensive, and\nthey rely heavily on human interaction and quality control. We investigate a\nnovel method based on the supervised deep fully convolutional neural network\n(FCN) for velocity-model building (VMB) directly from raw seismograms. Unlike\nthe conventional inversion method based on physical models, the supervised\ndeep-learning methods are based on big-data training rather than\nprior-knowledge assumptions. During the training stage, the network establishes\na nonlinear projection from the multi-shot seismic data to the corresponding\nvelocity models. During the prediction stage, the trained network can be used\nto estimate the velocity models from the new input seismic data. One key\ncharacteristic of the deep-learning method is that it can automatically extract\nmulti-layer useful features without the need for human-curated activities and\ninitial velocity setup. The data-driven method usually requires more time\nduring the training stage, and actual predictions take less time, with only\nseconds needed. Therefore, the computational time of geophysical inversions,\nincluding real-time inversions, can be dramatically reduced once a good\ngeneralized network is built. By using numerical experiments on synthetic\nmodels, the promising performances of our proposed method are shown in\ncomparison with conventional FWI even when the input data are in more realistic\nscenarios. Discussions on the deep-learning methods, training dataset, lack of\nlow frequencies, and advantages and disadvantages of the new method are also\nprovided.\n', ""  Accurate and efficient models for rainfall runoff (RR) simulations are\ncrucial for flood risk management. Most rainfall models in use today are\nprocess-driven; i.e. they solve either simplified empirical formulas or some\nvariation of the St. Venant (shallow water) equations. With the development of\nmachine-learning techniques, we may now be able to emulate rainfall models\nusing, for example, neural networks. In this study, a data-driven RR model\nusing a sequence-to-sequence Long-short-Term-Memory (LSTM) network was\nconstructed. The model was tested for a watershed in Houston, TX, known for\nsevere flood events. The LSTM network's capability in learning long-term\ndependencies between the input and output of the network allowed modeling RR\nwith high resolution in time (15 minutes). Using 10-years precipitation from\n153 rainfall gages and river channel discharge data (more than 5.3 million data\npoints), and by designing several numerical tests the developed model\nperformance in predicting river discharge was tested. The model results were\nalso compared with the output of a process-driven model Gridded Surface\nSubsurface Hydrologic Analysis (GSSHA). Moreover, physical consistency of the\nLSTM model was explored. The model results showed that the LSTM model was able\nto efficiently predict discharge and achieve good model performance. When\ncompared to GSSHA, the data-driven model was more efficient and robust in terms\nof prediction and calibration. Interestingly, the performance of the LSTM model\nimproved (test Nash-Sutcliffe model efficiency from 0.666 to 0.942) when a\nselected subset of rainfall gages based on the model performance, were used as\ninput instead of all rainfall gages.\n""]"
34,520,34_time series_series_time_forecasting,"['time series', 'series', 'time', 'forecasting', 'series classification', 'series data', 'dtw', 'series forecasting', 'data', 'multivariate']","['time series', 'series forecasting', 'series data', 'forecasting', 'series analysis', 'series classification', 'multivariate time', 'neural networks', 'prediction', 'forecasts']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['Time series forecasting', '', '', '', '', '', '', '', '', '']","['  In this paper, we present a new approach to time series forecasting. Time\nseries data are prevalent in many scientific and engineering disciplines. Time\nseries forecasting is a crucial task in modeling time series data, and is an\nimportant area of machine learning. In this work we developed a novel method\nthat employs Transformer-based machine learning models to forecast time series\ndata. This approach works by leveraging self-attention mechanisms to learn\ncomplex patterns and dynamics from time series data. Moreover, it is a generic\nframework and can be applied to univariate and multivariate time series data,\nas well as time series embeddings. Using influenza-like illness (ILI)\nforecasting as a case study, we show that the forecasting results produced by\nour approach are favorably comparable to the state-of-the-art.\n', '  The explosion of time series data in recent years has brought a flourish of\nnew time series analysis methods, for forecasting, clustering, classification\nand other tasks. The evaluation of these new methods requires either collecting\nor simulating a diverse set of time series benchmarking data to enable reliable\ncomparisons against alternative approaches. We propose GeneRAting TIme Series\nwith diverse and controllable characteristics, named GRATIS, with the use of\nmixture autoregressive (MAR) models. We simulate sets of time series using MAR\nmodels and investigate the diversity and coverage of the generated time series\nin a time series feature space. By tuning the parameters of the MAR models,\nGRATIS is also able to efficiently generate new time series with controllable\nfeatures. In general, as a costless surrogate to the traditional data\ncollection approach, GRATIS can be used as an evaluation tool for tasks such as\ntime series forecasting and classification. We illustrate the usefulness of our\ntime series generation process through a time series forecasting application.\n', '  Capturing the dynamical properties of time series concisely as interpretable\nfeature vectors can enable efficient clustering and classification for\ntime-series applications across science and industry. Selecting an appropriate\nfeature-based representation of time series for a given application can be\nachieved through systematic comparison across a comprehensive time-series\nfeature library, such as those in the hctsa toolbox. However, this approach is\ncomputationally expensive and involves evaluating many similar features,\nlimiting the widespread adoption of feature-based representations of time\nseries for real-world applications. In this work, we introduce a method to\ninfer small sets of time-series features that (i) exhibit strong classification\nperformance across a given collection of time-series problems, and (ii) are\nminimally redundant. Applying our method to a set of 93 time-series\nclassification datasets (containing over 147000 time series) and using a\nfiltered version of the hctsa feature library (4791 features), we introduce a\ngenerically useful set of 22 CAnonical Time-series CHaracteristics, catch22.\nThis dimensionality reduction, from 4791 to 22, is associated with an\napproximately 1000-fold reduction in computation time and near linear scaling\nwith time-series length, despite an average reduction in classification\naccuracy of just 7%. catch22 captures a diverse and interpretable signature of\ntime series in terms of their properties, including linear and non-linear\nautocorrelation, successive differences, value distributions and outliers, and\nfluctuation scaling properties. We provide an efficient implementation of\ncatch22, accessible from many programming environments, that facilitates\nfeature-based time-series analysis for scientific, industrial, financial and\nmedical applications using a common language of interpretable time-series\nproperties.\n']"
35,505,35_meta_meta learning_shot_shot learning,"['meta', 'meta learning', 'shot', 'shot learning', 'learning', 'tasks', 'task', 'maml', 'shot classification', 'classes']","['meta learning', 'meta training', 'shot learning', 'meta learner', 'learning algorithms', 'shot classification', 'gradient based', 'learning methods', 'learning', 'based meta']","['meta learning', '', '', '', '', '', '', '', '', '']","['meta-learning', '', '', '', '', '', '', '', '', '']","['  Majority of the modern meta-learning methods for few-shot classification\ntasks operate in two phases: a meta-training phase where the meta-learner\nlearns a generic representation by solving multiple few-shot tasks sampled from\na large dataset and a testing phase, where the meta-learner leverages its\nlearnt internal representation for a specific few-shot task involving classes\nwhich were not seen during the meta-training phase. To the best of our\nknowledge, all such meta-learning methods use a single base dataset for\nmeta-training to sample tasks from and do not adapt the algorithm after\nmeta-training. This strategy may not scale to real-world use-cases where the\nmeta-learner does not potentially have access to the full meta-training dataset\nfrom the very beginning and we need to update the meta-learner in an\nincremental fashion when additional training data becomes available. Through\nour experimental setup, we develop a notion of incremental learning during the\nmeta-training phase of meta-learning and propose a method which can be used\nwith multiple existing metric-based meta-learning algorithms. Experimental\nresults on benchmark dataset show that our approach performs favorably at test\ntime as compared to training a model with the full meta-training set and incurs\nnegligible amount of catastrophic forgetting\n', '  Few-shot learning aims to learn classifiers for new classes with only a few\ntraining examples per class. Most existing few-shot learning approaches belong\nto either metric-based meta-learning or optimization-based meta-learning\ncategory, both of which have achieved successes in the simplified ""$k$-shot\n$N$-way"" image classification settings. Specifically, the optimization-based\napproaches train a meta-learner to predict the parameters of the task-specific\nclassifiers. The task-specific classifiers are required to be\nhomogeneous-structured to ease the parameter prediction, so the meta-learning\napproaches could only handle few-shot learning problems where the tasks share a\nuniform number of classes. The metric-based approaches learn one task-invariant\nmetric for all the tasks. Even though the metric-learning approaches allow\ndifferent numbers of classes, they require the tasks all coming from a similar\ndomain such that there exists a uniform metric that could work across tasks. In\nthis work, we propose a hybrid meta-learning model called Meta-Metric-Learner\nwhich combines the merits of both optimization- and metric-based approaches.\nOur meta-metric-learning approach consists of two components, a task-specific\nmetric-based learner as a base model, and a meta-learner that learns and\nspecifies the base model. Thus our model is able to handle flexible numbers of\nclasses as well as generate more generalized metrics for classification across\ntasks. We test our approach in the standard ""$k$-shot $N$-way"" few-shot\nlearning setting following previous works and a new realistic few-shot setting\nwith flexible class numbers in both single-source form and multi-source forms.\nExperiments show that our approach can obtain superior performance in all\nsettings.\n', '  This article reviews meta-learning also known as learning-to-learn which\nseeks rapid and accurate model adaptation to unseen tasks with applications in\nhighly automated AI, few-shot learning, natural language processing and\nrobotics. Unlike deep learning, meta-learning can be applied to few-shot\nhigh-dimensional datasets and considers further improving model generalization\nto unseen tasks. Deep learning is focused upon in-sample prediction and\nmeta-learning concerns model adaptation for out-of-sample prediction.\nMeta-learning can continually perform self-improvement to achieve highly\nautonomous AI. Meta-learning may serve as an additional generalization block\ncomplementary for original deep learning model. Meta-learning seeks adaptation\nof machine learning models to unseen tasks which are vastly different from\ntrained tasks. Meta-learning with coevolution between agent and environment\nprovides solutions for complex tasks unsolvable by training from scratch.\nMeta-learning methodology covers a wide range of great minds and thoughts. We\nbriefly introduce meta-learning methodologies in the following categories:\nblack-box meta-learning, metric-based meta-learning, layered meta-learning and\nBayesian meta-learning framework. Recent applications concentrate upon the\nintegration of meta-learning with other machine learning framework to provide\nfeasible integrated problem solutions. We briefly present recent meta-learning\nadvances and discuss potential future research directions.\n']"
36,498,36_vae_latent_variational_generative,"['vae', 'latent', 'variational', 'generative', 'vaes', 'disentanglement', 'autoencoders', 'generative models', 'autoencoder', 'disentangled']","['autoencoders vaes', 'variational autoencoder', 'variational autoencoders', 'latent representation', 'generative models', 'generative model', 'representation learning', 'learning disentangled', 'encoders', 'autoencoders']","['generative model disentanglement', '', '', '', '', '', '', '', '', '']","['Variational autoencoder', '', '', '', '', '', '', '', '', '']","[""  Being one of the most popular generative framework, variational\nautoencoders(VAE) are known to suffer from a phenomenon termed posterior\ncollapse, i.e. the latent variational distributions collapse to the prior,\nespecially when a strong decoder network is used. In this work, we analyze the\nlatent representation of collapsed VAEs, and proposed a novel model, neighbor\nembedding VAE(NE-VAE), which explicitly constraints the encoder to encode\ninputs close in the input space to be close in the latent space. We observed\nthat for VAE variants that report similar ELBO, KL divergence or even mutual\ninformation scores may still behave quite differently in the latent\norganization. In our experiments, NE-VAE can produce qualitatively different\nlatent representations with majority of the latent dimensions remained active,\nwhich may benefit downstream latent space optimization tasks. NE-VAE can\nprevent posterior collapse to a much greater extent than it's predecessors, and\ncan be easily plugged into any autoencoder framework, without introducing\naddition model components and complex training routines.\n"", '  As a general-purpose generative model architecture, VAE has been widely used\nin the field of image and natural language processing. VAE maps high\ndimensional sample data into continuous latent variables with unsupervised\nlearning. Sampling in the latent variable space of the feature, VAE can\nconstruct new image or text data. As a general-purpose generation model, the\nvanilla VAE can not fit well with various data sets and neural networks with\ndifferent structures. Because of the need to balance the accuracy of\nreconstruction and the convenience of latent variable sampling in the training\nprocess, VAE often has problems known as ""posterior collapse"". images\nreconstructed by VAE are also often blurred. In this paper, we analyze the main\ncause of these problem, which is the lack of mutual information between the\nsample variable and the latent feature variable during the training process. To\nmaintain mutual information in model training, we propose to use the auxiliary\nsoftmax multi-classification network structure to improve the training effect\nof VAE, named VAE-AS. We use MNIST and Omniglot data sets to test the VAE-AS\nmodel. Based on the test results, It can be show that VAE-AS has obvious\neffects on the mutual information adjusting and solving the posterior collapse\nproblem.\n', ""  After deep generative models were successfully applied to image generation\ntasks, learning disentangled latent variables of data has become a crucial part\nof deep generative model research. Many models have been proposed to learn an\ninterpretable and factorized representation of latent variable by modifying\ntheir objective function or model architecture. To disentangle the latent\nvariable, some models show lower quality of reconstructed images and others\nincrease the model complexity which is hard to train. In this paper, we propose\na simple disentangling method based on a traditional whitening process. The\nproposed method is applied to the latent variables of variational auto-encoder\n(VAE), although it can be applied to any generative models with latent\nvariables. In experiment, we apply the proposed method to simple VAE models and\nexperiment results confirm that our method finds more interpretable factors\nfrom the latent space while keeping the reconstruction error the same as the\nconventional VAE's error.\n""]"
37,485,37_malware_detection_attacks_malicious,"['malware', 'detection', 'attacks', 'malicious', 'security', 'intrusion', 'traffic', 'intrusion detection', 'malware detection', 'cyber']","['malware classification', 'based malware', 'malware detection', 'malware', 'android malware', 'adversarial', 'deep learning', 'intrusion detection', 'network intrusion', 'anomaly detection']","['cyber security', '', '', '', '', '', '', '', '', '']","['malware detection', '', '', '', '', '', '', '', '', '']","[""  In the case of malware analysis, categorization of malicious files is an\nessential part after malware detection. Numerous static and dynamic techniques\nhave been reported so far for categorizing malware. This research presents a\ndeep learning-based malware detection (DLMD) technique based on static methods\nfor classifying different malware families. The proposed DLMD technique uses\nboth the byte and ASM files for feature engineering, thus classifying malware\nfamilies. First, features are extracted from byte files using two different\nDeep Convolutional Neural Networks (CNN). After that, essential and\ndiscriminative opcode features are selected using a wrapper-based mechanism,\nwhere Support Vector Machine (SVM) is used as a classifier. The idea is to\nconstruct a hybrid feature space by combining the different feature spaces to\novercome the shortcoming of particular feature space and thus, reduce the\nchances of missing a malware. Finally, the hybrid feature space is used to\ntrain a Multilayer Perceptron, which classifies all nine different malware\nfamilies. Experimental results show that proposed DLMD technique achieves\nlog-loss of 0.09 for ten independent runs. Moreover, the proposed DLMD\ntechnique's performance is compared against different classifiers and shows its\neffectiveness in categorizing malware. The relevant code and database can be\nfound at\nhttps://github.com/cyberhunters/Malware-Detection-Using-Machine-Learning.\n"", ""  Behavioral malware detection aims to improve on the performance of static\nsignature-based techniques used by anti-virus systems, which are less effective\nagainst modern polymorphic and metamorphic malware. Behavioral malware\nclassification aims to go beyond the detection of malware by also identifying a\nmalware's family according to a naming scheme such as the ones used by\nanti-virus vendors. Behavioral malware classification techniques use run-time\nfeatures, such as file system or network activities, to capture the behavioral\ncharacteristic of running processes. The increasing volume of malware samples,\ndiversity of malware families, and the variety of naming schemes given to\nmalware samples by anti-virus vendors present challenges to behavioral malware\nclassifiers. We describe a behavioral classifier that uses a Convolutional\nRecurrent Neural Network and data from Microsoft Windows Prefetch files. We\ndemonstrate the model's improvement on the state-of-the-art using a large\ndataset of malware families and four major anti-virus vendor naming schemes.\nThe model is effective in classifying malware samples that belong to common and\nrare malware families and can incrementally accommodate the introduction of new\nmalware samples and families.\n"", '  My research lies in the intersection of security and machine learning. This\noverview summarizes one component of my research: combining computer vision\nwith malware exploit detection for enhanced security solutions. I will present\nthe perspectives of efficacy, reliability and resiliency to formulate threat\ndetection as computer vision problems and develop state-of-the-art image-based\nmalware classification. Representing malware binary as images provides a direct\nvisualization of data samples, reduces the efforts for feature extraction, and\nconsumes the whole binary for holistic structural analysis. Employing transfer\nlearning of deep neural networks effective for large scale image classification\nto malware classification demonstrates superior classification efficacy\ncompared with classical machine learning algorithms. To enhance reliability of\nthese vision-based malware detectors, interpretation frameworks can be\nconstructed on the malware visual representations and useful for extracting\nfaithful explanation, so that security practitioners have confidence in the\nmodel before deployment. In cyber-security applications, we should always\nassume that a malware writer constantly modifies code to bypass detection.\nAddressing the resiliency of the malware detectors is equivalently important as\nefficacy and reliability. Via understanding the attack surfaces of machine\nlearning models used for malware detection, we can greatly improve the\nrobustness of the algorithms to combat malware adversaries in the wild. Finally\nI will discuss future research directions worth pursuing in this research\ncommunity.\n']"
38,456,38_hardware_memory_gpu_dnn,"['hardware', 'memory', 'gpu', 'dnn', 'energy', 'fpga', 'accelerator', 'accelerators', 'devices', 'deep']","['hardware accelerators', 'neural networks', 'neural network', 'convolutional neural', 'deep neural', 'deep learning', 'fpgas', 'fpga', 'cnns', 'accelerators']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['A framework for designing CNN accelerator on embedded FPGA for image classification', '', '', '', '', '', '', '', '', '']","[""  The effectiveness of deep neural networks (DNN) in vision, speech, and\nlanguage processing has prompted a tremendous demand for energy-efficient\nhigh-performance DNN inference systems. Due to the increasing memory intensity\nof most DNN workloads, main memory can dominate the system's energy consumption\nand stall time. One effective way to reduce the energy consumption and increase\nthe performance of DNN inference systems is by using approximate memory, which\noperates with reduced supply voltage and reduced access latency parameters that\nviolate standard specifications. Using approximate memory reduces reliability,\nleading to higher bit error rates. Fortunately, neural networks have an\nintrinsic capacity to tolerate increased bit errors. This can enable\nenergy-efficient and high-performance neural network inference using\napproximate DRAM devices.\n  Based on this observation, we propose EDEN, a general framework that reduces\nDNN energy consumption and DNN evaluation latency by using approximate DRAM\ndevices, while strictly meeting a user-specified target DNN accuracy. EDEN\nrelies on two key ideas: 1) retraining the DNN for a target approximate DRAM\ndevice to increase the DNN's error tolerance, and 2) efficient mapping of the\nerror tolerance of each individual DNN data type to a corresponding approximate\nDRAM partition in a way that meets the user-specified DNN accuracy\nrequirements.\n  We evaluate EDEN on multi-core CPUs, GPUs, and DNN accelerators with error\nmodels obtained from real approximate DRAM devices. For a target accuracy\nwithin 1% of the original DNN, our results show that EDEN enables 1) an average\nDRAM energy reduction of 21%, 37%, 31%, and 32% in CPU, GPU, and two DNN\naccelerator architectures, respectively, across a variety of DNNs, and 2) an\naverage (maximum) speedup of 8% (17%) and 2.7% (5.5%) in CPU and GPU\narchitectures, respectively, when evaluating latency-bound DNNs.\n"", '  In recent years, Convolutional Neural Network (CNN) based methods have\nachieved great success in a large number of applications and have been among\nthe most powerful and widely used techniques in computer vision. However,\nCNN-based methods are computational-intensive and resource-consuming, and thus\nare hard to be integrated into embedded systems such as smart phones, smart\nglasses, and robots. FPGA is one of the most promising platforms for\naccelerating CNN, but the limited on-chip memory size limit the performance of\nFPGA accelerator for CNN. In this paper, we propose a framework for designing\nCNN accelerator on embedded FPGA for image classification. The proposed\nframework provides a tool for FPGA resource-aware design space exploration of\nCNNs and automatically generates the hardware description of the CNN to be\nprogrammed on a target FPGA. The framework consists of three main backends;\nsoftware, hardware generation, and simulation/precision adjustment. The\nsoftware backend serves as an API to the designer to design the CNN and train\nit according to the hardware resources that are available. Using the CNN model,\nhardware backend generates the necessary hardware components and integrates\nthem to generate the hardware description of the CNN. Finaly,\nSimulation/precision adjustment backend adjusts the inter-layer precision units\nto minimize the classification error. We used 16-bit fixed-point data in a CNN\naccelerator (FPGA) and compared it to the exactly similar software version\nrunning on an ARM processor (32-bit floating point data). We encounter about 3%\naccuracy loss in classification of the accelerated (FPGA) version. In return,\nwe got up to 15.75x speedup by classifying with the accelerated version on the\nFPGA.\n', '  Convolutional neural networks (CNN) have achieved major breakthroughs in\nrecent years. Their performance in computer vision have matched and in some\nareas even surpassed human capabilities. Deep neural networks can capture\ncomplex non-linear features; however this ability comes at the cost of high\ncomputational and memory requirements. State-of-art networks require billions\nof arithmetic operations and millions of parameters. To enable embedded devices\nsuch as smartphones, Google glasses and monitoring cameras with the astonishing\npower of deep learning, dedicated hardware accelerators can be used to decrease\nboth execution time and power consumption. In applications where fast\nconnection to the cloud is not guaranteed or where privacy is important,\ncomputation needs to be done locally. Many hardware accelerators for deep\nneural networks have been proposed recently. A first important step of\naccelerator design is hardware-oriented approximation of deep networks, which\nenables energy-efficient inference. We present Ristretto, a fast and automated\nframework for CNN approximation. Ristretto simulates the hardware arithmetic of\na custom hardware accelerator. The framework reduces the bit-width of network\nparameters and outputs of resource-intense layers, which reduces the chip area\nfor multiplication units significantly. Alternatively, Ristretto can remove the\nneed for multipliers altogether, resulting in an adder-only arithmetic. The\ntool fine-tunes trimmed networks to achieve high classification accuracy. Since\ntraining of deep neural networks can be time-consuming, Ristretto uses highly\noptimized routines which run on the GPU. This enables fast compression of any\ngiven network. Given a maximum tolerance of 1%, Ristretto can successfully\ncondense CaffeNet and SqueezeNet to 8-bit. The code for Ristretto is available.\n']"
39,452,39_domain_domain adaptation_adaptation_target,"['domain', 'domain adaptation', 'adaptation', 'target', 'source', 'target domain', 'domains', 'source domain', 'unsupervised domain', 'source target']","['domain adaptation', 'learning domain', 'unsupervised domain', 'domain adversarial', 'domain generalization', 'domain alignment', 'discriminative', 'target domain', 'target domains', 'source domain']","['domain adaptation', '', '', '', '', '', '', '', '', '']","['Domain adaptation', '', '', '', '', '', '', '', '', '']","['  Early Unsupervised Domain Adaptation (UDA) methods have mostly assumed the\nsetting of a single source domain, where all the labeled source data come from\nthe same distribution. However, in practice the labeled data can come from\nmultiple source domains with different distributions. In such scenarios, the\nsingle source domain adaptation methods can fail due to the existence of domain\nshifts across different source domains and multi-source domain adaptation\nmethods need to be designed. In this paper, we propose a novel multi-source\ndomain adaptation method, Mutual Learning Network for Multiple Source Domain\nAdaptation (ML-MSDA). Under the framework of mutual learning, the proposed\nmethod pairs the target domain with each single source domain to train a\nconditional adversarial domain adaptation network as a branch network, while\ntaking the pair of the combined multi-source domain and target domain to train\na conditional adversarial adaptive network as the guidance network. The\nmultiple branch networks are aligned with the guidance network to achieve\nmutual learning by enforcing JS-divergence regularization over their prediction\nprobability distributions on the corresponding target data. We conduct\nextensive experiments on multiple multi-source domain adaptation benchmark\ndatasets. The results show the proposed ML-MSDA method outperforms the\ncomparison methods and achieves the state-of-the-art performance.\n', '  In this paper, we propose a simple model referred as Contradistinguisher\n(CTDR) for unsupervised domain adaptation whose objective is to jointly learn\nto contradistinguish on unlabeled target domain in a fully unsupervised manner\nalong with prior knowledge acquired by supervised learning on an entirely\ndifferent domain. Most recent works in domain adaptation rely on an indirect\nway of first aligning the source and target domain distributions and then learn\na classifier on a labeled source domain to classify target domain. This\napproach of an indirect way of addressing the real task of unlabeled target\ndomain classification has three main drawbacks. (i) The sub-task of obtaining a\nperfect alignment of the domain in itself might be impossible due to large\ndomain shift (e.g., language domains). (ii) The use of multiple classifiers to\nalign the distributions unnecessarily increases the complexity of the neural\nnetworks leading to over-fitting in many cases. (iii) Due to distribution\nalignment, the domain-specific information is lost as the domains get morphed.\nIn this work, we propose a simple and direct approach that does not require\ndomain alignment. We jointly learn CTDR on both source and target distribution\nfor unsupervised domain adaptation task using contradistinguish loss for the\nunlabeled target domain in conjunction with a supervised loss for labeled\nsource domain. Our experiments show that avoiding domain alignment by directly\naddressing the task of unlabeled target domain classification using CTDR\nachieves state-of-the-art results on eight visual and four language benchmark\ndomain adaptation datasets.\n', '  The primary objective of domain adaptation methods is to transfer knowledge\nfrom a source domain to a target domain that has similar but different data\ndistributions. Thus, in order to correctly classify the unlabeled target domain\nsamples, the standard approach is to learn a common representation for both\nsource and target domain, thereby indirectly addressing the problem of learning\na classifier in the target domain. However, such an approach does not address\nthe task of classification in the target domain directly. In contrast, we\npropose an approach that directly addresses the problem of learning a\nclassifier in the unlabeled target domain. In particular, we train a classifier\nto correctly classify the training samples while simultaneously classifying the\nsamples in the target domain in an unsupervised manner. The corresponding model\nis referred to as Discriminative Encoding for Domain Adaptation (DEDA). We show\nthat this simple approach for performing unsupervised domain adaptation is\nindeed quite powerful. Our method achieves state of the art results in\nunsupervised adaptation tasks on various image classification benchmarks. We\nalso obtained state of the art performance on domain adaptation in Amazon\nreviews sentiment classification dataset. We perform additional experiments\nwhen the source data has less labeled examples and also on zero-shot domain\nadaptation task where no target domain samples are used for training.\n']"
40,452,40_variational_inference_posterior_mcmc,"['variational', 'inference', 'posterior', 'mcmc', 'variational inference', 'bayesian', 'carlo', 'monte carlo', 'monte', 'gradient']","['variational inference', 'stochastic gradient', 'approximate inference', 'bayesian inference', 'kl divergence', 'posterior distribution', 'bayesian', 'divergence', 'variational', 'approximations']","['variational inference', '', '', '', '', '', '', '', '', '']","['variational inference', '', '', '', '', '', '', '', '', '']","['  Bayesian inference typically requires the computation of an approximation to\nthe posterior distribution. An important requirement for an approximate\nBayesian inference algorithm is to output high-accuracy posterior mean and\nuncertainty estimates. Classical Monte Carlo methods, particularly Markov Chain\nMonte Carlo, remain the gold standard for approximate Bayesian inference\nbecause they have a robust finite-sample theory and reliable convergence\ndiagnostics. However, alternative methods, which are more scalable or apply to\nproblems where Markov Chain Monte Carlo cannot be used, lack the same\nfinite-data approximation theory and tools for evaluating their accuracy. In\nthis work, we develop a flexible new approach to bounding the error of mean and\nuncertainty estimates of scalable inference algorithms. Our strategy is to\ncontrol the estimation errors in terms of Wasserstein distance, then bound the\nWasserstein distance via a generalized notion of Fisher distance. Unlike\ncomputing the Wasserstein distance, which requires access to the normalized\nposterior distribution, the Fisher distance is tractable to compute because it\nrequires access only to the gradient of the log posterior density. We\ndemonstrate the usefulness of our Fisher distance approach by deriving bounds\non the Wasserstein error of the Laplace approximation and Hilbert coresets. We\nanticipate that our approach will be applicable to many other approximate\ninference methods such as the integrated Laplace approximation, variational\ninference, and approximate Bayesian computation\n', '  Variational inference is an umbrella term for algorithms which cast Bayesian\ninference as optimization. Classically, variational inference uses the\nKullback-Leibler divergence to define the optimization. Though this divergence\nhas been widely used, the resultant posterior approximation can suffer from\nundesirable statistical properties. To address this, we reexamine variational\ninference from its roots as an optimization problem. We use operators, or\nfunctions of functions, to design variational objectives. As one example, we\ndesign a variational objective with a Langevin-Stein operator. We develop a\nblack box algorithm, operator variational inference (OPVI), for optimizing any\noperator objective. Importantly, operators enable us to make explicit the\nstatistical and computational tradeoffs for variational inference. We can\ncharacterize different properties of variational objectives, such as objectives\nthat admit data subsampling---allowing inference to scale to massive data---as\nwell as objectives that admit variational programs---a rich class of posterior\napproximations that does not require a tractable density. We illustrate the\nbenefits of OPVI on a mixture model and a generative model of images.\n', '  We show how to use Stein variational gradient descent (SVGD) to carry out\ninference in Gaussian process (GP) models with non-Gaussian likelihoods and\nlarge data volumes. Markov chain Monte Carlo (MCMC) is extremely\ncomputationally intensive for these situations, but the parametric assumptions\nrequired for efficient variational inference (VI) result in incorrect inference\nwhen they encounter the multi-modal posterior distributions that are common for\nsuch models. SVGD provides a non-parametric alternative to variational\ninference which is substantially faster than MCMC. We prove that for GP models\nwith Lipschitz gradients the SVGD algorithm monotonically decreases the\nKullback-Leibler divergence from the sampling distribution to the true\nposterior. Our method is demonstrated on benchmark problems in both regression\nand classification, a multimodal posterior, and an air quality example with\n550,134 spatiotemporal observations, showing substantial performance\nimprovements over MCMC and VI.\n']"
41,443,41_recurrent_rnns_rnn_recurrent neural,"['recurrent', 'rnns', 'rnn', 'recurrent neural', 'lstm', 'memory', 'long', 'networks', 'neural', 'term']","['recurrent networks', 'neural network', 'recurrent neural', 'neural networks', 'memory lstm', 'networks rnns', 'backpropagation', 'language modeling', 'gated recurrent', 'lstm']","['Recurrent neural networks', '', '', '', '', '', '', '', '', '']","['Recurrent neural networks', '', '', '', '', '', '', '', '', '']","['  Recurrent neural networks (RNNs) have been widely used for processing\nsequential data. However, RNNs are commonly difficult to train due to the\nwell-known gradient vanishing and exploding problems and hard to learn\nlong-term patterns. Long short-term memory (LSTM) and gated recurrent unit\n(GRU) were developed to address these problems, but the use of hyperbolic\ntangent and the sigmoid action functions results in gradient decay over layers.\nConsequently, construction of an efficiently trainable deep network is\nchallenging. In addition, all the neurons in an RNN layer are entangled\ntogether and their behaviour is hard to interpret. To address these problems, a\nnew type of RNN, referred to as independently recurrent neural network\n(IndRNN), is proposed in this paper, where neurons in the same layer are\nindependent of each other and they are connected across layers. We have shown\nthat an IndRNN can be easily regulated to prevent the gradient exploding and\nvanishing problems while allowing the network to learn long-term dependencies.\nMoreover, an IndRNN can work with non-saturated activation functions such as\nrelu (rectified linear unit) and be still trained robustly. Multiple IndRNNs\ncan be stacked to construct a network that is deeper than the existing RNNs.\nExperimental results have shown that the proposed IndRNN is able to process\nvery long sequences (over 5000 time steps), can be used to construct very deep\nnetworks (21 layers used in the experiment) and still be trained robustly.\nBetter performances have been achieved on various tasks by using IndRNNs\ncompared with the traditional RNN and LSTM. The code is available at\nhttps://github.com/Sunnydreamrain/IndRNN_Theano_Lasagne.\n', '  Recurrent neural networks (RNNs) are widely used as a memory model for\nsequence-related problems. Many variants of RNN have been proposed to solve the\ngradient problems of training RNNs and process long sequences. Although some\nclassical models have been proposed, capturing long-term dependence while\nresponding to short-term changes remains a challenge. To this problem, we\npropose a new model named Dual Recurrent Neural Networks (DuRNN). The DuRNN\nconsists of two parts to learn the short-term dependence and progressively\nlearn the long-term dependence. The first part is a recurrent neural network\nwith constrained full recurrent connections to deal with short-term dependence\nin sequence and generate short-term memory. Another part is a recurrent neural\nnetwork with independent recurrent connections which helps to learn long-term\ndependence and generate long-term memory. A selection mechanism is added\nbetween two parts to help the needed long-term information transfer to the\nindependent neurons. Multiple modules can be stacked to form a multi-layer\nmodel for better performance. Our contributions are: 1) a new recurrent model\ndeveloped based on the divide-and-conquer strategy to learn long and short-term\ndependence separately, and 2) a selection mechanism to enhance the separating\nand learning of different temporal scales of dependence. Both theoretical\nanalysis and extensive experiments are conducted to validate the performance of\nour model, and we also conduct simple visualization experiments and ablation\nanalyses for the model interpretability. Experimental results indicate that the\nproposed DuRNN model can handle not only very long sequences (over 5000 time\nsteps), but also short sequences very well. Compared with many state-of-the-art\nRNN models, our model has demonstrated efficient and better performance.\n', '  Recurrent Neural Networks (RNNs), which are a powerful scheme for modeling\ntemporal and sequential data need to capture long-term dependencies on datasets\nand represent them in hidden layers with a powerful model to capture more\ninformation from inputs. For modeling long-term dependencies in a dataset, the\ngating mechanism concept can help RNNs remember and forget previous\ninformation. Representing the hidden layers of an RNN with more expressive\noperations (i.e., tensor products) helps it learn a more complex relationship\nbetween the current input and the previous hidden layer information. These\nideas can generally improve RNN performances. In this paper, we proposed a\nnovel RNN architecture that combine the concepts of gating mechanism and the\ntensor product into a single model. By combining these two concepts into a\nsingle RNN, our proposed models learn long-term dependencies by modeling with\ngating units and obtain more expressive and direct interaction between input\nand hidden layers using a tensor product on 3-dimensional array (tensor) weight\nparameters. We use Long Short Term Memory (LSTM) RNN and Gated Recurrent Unit\n(GRU) RNN and combine them with a tensor product inside their formulations. Our\nproposed RNNs, which are called a Long-Short Term Memory Recurrent Neural\nTensor Network (LSTMRNTN) and Gated Recurrent Unit Recurrent Neural Tensor\nNetwork (GRURNTN), are made by combining the LSTM and GRU RNN models with the\ntensor product. We conducted experiments with our proposed models on word-level\nand character-level language modeling tasks and revealed that our proposed\nmodels significantly improved their performance compared to our baseline\nmodels.\n']"
42,441,42_physics_data_galaxy_gravitational,"['physics', 'data', 'galaxy', 'gravitational', 'events', 'particle', 'machine learning', 'machine', 'astronomical', 'detector']","['hadron collider', 'neural networks', 'deep learning', 'lhc', 'neural network', 'convolutional neural', 'collider', 'gravitational wave', 'convolutional', 'particle physics']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['Machine learning and gravitational wave astronomy', '', '', '', '', '', '', '', '', '']","['  This thesis demonstrate the efficacy of designing and developing machine\nlearning (ML) algorithms to selected use cases that encompass many of the\noutstanding challenges in the field of experimental high energy physics.\nAlthough simple implementations of neural networks and boosted decision trees\nhave been used in high energy physics for a long time, the field of ML has\nquickly evolved by devising more complex, fast and stable implementations of\nlearning algorithms. The complexity and power of state-of-the-art deep learning\nfar exceeds those of the learning algorithms implemented in the CERN-developed\n\\texttt{ROOT} library. All aspects of experimental high energy physics have\nbeen and will continue being revolutionized by the software- and hardware-based\ntechnological advances spearheaded by both academic and industrial research in\nother technical disciplines, and the emergent trend of increased\ninterdisciplinarity will soon reframe many scientific domains. This thesis\nexemplifies this spirit of versatility and multidisciplinarity by bridging the\ngap between ML and particle physics, and exploring original lines of work to\nmodernize the reconstruction, particle identification, simulation, and analysis\nworkflows. This contribution documents a collection of novel approaches to\naugment traditional domain-specific methods with modern, automated techniques\nbased on industry-standard, open-source libraries. Specifically, it contributes\nto setting the state-of-the-art for impact parameter-based flavor tagging and\ndi-Higgs searches in the $\\gamma \\gamma b\\bar{b} $ channel with the ATLAS\ndetector at the LHC, it introduces and lays the foundations for the use of\ngenerative adversarial networks for the simulation of particle showers in\ncalorimeters. These results substantiate the notion of ML powering particle\nphysics in the upcoming years and establish baselines for future applications.\n', '  Accurate and fast simulation of particle physics processes is crucial for the\nhigh-energy physics community. Simulating particle interactions with detectors\nis both time consuming and computationally expensive. With the proton-proton\ncollision energy of 13 TeV, the Large Hadron Collider is uniquely positioned to\ndetect and measure the rare phenomena that can shape our knowledge of new\ninteractions. The High-Luminosity Large Hadron Collider (HL-LHC) upgrade will\nput a significant strain on the computing infrastructure due to increased event\nrate and levels of pile-up. Simulation of high-energy physics collisions needs\nto be significantly faster without sacrificing the physics accuracy. Machine\nlearning approaches can offer faster solutions, while maintaining a high level\nof fidelity. We discuss a graph generative model that provides effective\nreconstruction of LHC events, paving the way for full detector level fast\nsimulation for HL-LHC.\n', '  The recent Nobel-prize-winning detections of gravitational waves from merging\nblack holes and the subsequent detection of the collision of two neutron stars\nin coincidence with electromagnetic observations have inaugurated a new era of\nmultimessenger astrophysics. To enhance the scope of this emergent field of\nscience, we pioneered the use of deep learning with convolutional neural\nnetworks, that take time-series inputs, for rapid detection and\ncharacterization of gravitational wave signals. This approach, Deep Filtering,\nwas initially demonstrated using simulated LIGO noise. In this article, we\npresent the extension of Deep Filtering using real data from LIGO, for both\ndetection and parameter estimation of gravitational waves from binary black\nhole mergers using continuous data streams from multiple LIGO detectors. We\ndemonstrate for the first time that machine learning can detect and estimate\nthe true parameters of real events observed by LIGO. Our results show that Deep\nFiltering achieves similar sensitivities and lower errors compared to\nmatched-filtering while being far more computationally efficient and more\nresilient to glitches, allowing real-time processing of weak time-series\nsignals in non-stationary non-Gaussian noise with minimal resources, and also\nenables the detection of new classes of gravitational wave sources that may go\nunnoticed with existing detection algorithms. This unified framework for data\nanalysis is ideally suited to enable coincident detection campaigns of\ngravitational waves and their multimessenger counterparts in real-time.\n']"
43,431,43_dictionary_sparse_dictionary learning_signal,"['dictionary', 'sparse', 'dictionary learning', 'signal', 'recovery', 'sensing', 'measurements', 'signals', 'sparsity', 'algorithm']","['sparse coding', 'sparse representation', 'sparse signals', 'sparse signal', 'sparse recovery', 'compressed sensing', 'compressive sensing', 'dictionary learning', 'sparse', 'compressive']","['dictionary learning', '', '', '', '', '', '', '', '', '']","['dictionary learning', '', '', '', '', '', '', '', '', '']","['  Dictionary learning is a branch of signal processing and machine learning\nthat aims at finding a frame (called dictionary) in which some training data\nadmits a sparse representation. The sparser the representation, the better the\ndictionary. The resulting dictionary is in general a dense matrix, and its\nmanipulation can be computationally costly both at the learning stage and later\nin the usage of this dictionary, for tasks such as sparse coding. Dictionary\nlearning is thus limited to relatively small-scale problems. In this paper,\ninspired by usual fast transforms, we consider a general dictionary structure\nthat allows cheaper manipulation, and propose an algorithm to learn such\ndictionaries --and their fast implementation-- over training data. The approach\nis demonstrated experimentally with the factorization of the Hadamard matrix\nand with synthetic dictionary learning experiments.\n', '  Sparse coding in learned dictionaries has been established as a successful\napproach for signal denoising, source separation and solving inverse problems\nin general. A dictionary learning method adapts an initial dictionary to a\nparticular signal class by iteratively computing an approximate factorization\nof a training data matrix into a dictionary and a sparse coding matrix. The\nlearned dictionary is characterized by two properties: the coherence of the\ndictionary to observations of the signal class, and the self-coherence of the\ndictionary atoms. A high coherence to the signal class enables the sparse\ncoding of signal observations with a small approximation error, while a low\nself-coherence of the atoms guarantees atom recovery and a more rapid residual\nerror decay rate for the sparse coding algorithm. The two goals of high signal\ncoherence and low self-coherence are typically in conflict, therefore one seeks\na trade-off between them, depending on the application. We present a dictionary\nlearning method with an effective control over the self-coherence of the\ntrained dictionary, enabling a trade-off between maximizing the sparsity of\ncodings and approximating an equiangular tight frame.\n', '  We consider the dictionary learning problem, where the aim is to model the\ngiven data as a linear combination of a few columns of a matrix known as a\ndictionary, where the sparse weights forming the linear combination are known\nas coefficients. Since the dictionary and coefficients, parameterizing the\nlinear model are unknown, the corresponding optimization is inherently\nnon-convex. This was a major challenge until recently, when provable algorithms\nfor dictionary learning were proposed. Yet, these provide guarantees only on\nthe recovery of the dictionary, without explicit recovery guarantees on the\ncoefficients. Moreover, any estimation error in the dictionary adversely\nimpacts the ability to successfully localize and estimate the coefficients.\nThis potentially limits the utility of existing provable dictionary learning\nmethods in applications where coefficient recovery is of interest. To this end,\nwe develop NOODL: a simple Neurally plausible alternating Optimization-based\nOnline Dictionary Learning algorithm, which recovers both the dictionary and\ncoefficients exactly at a geometric rate, when initialized appropriately. Our\nalgorithm, NOODL, is also scalable and amenable for large scale distributed\nimplementations in neural architectures, by which we mean that it only involves\nsimple linear and non-linear operations. Finally, we corroborate these\ntheoretical results via experimental evaluation of the proposed algorithm with\nthe current state-of-the-art techniques.\n  Keywords: dictionary learning, provable dictionary learning, online\ndictionary learning, non-convex, sparse coding, support recovery, iterative\nhard thresholding, matrix factorization, neural architectures, neural networks,\nnoodl, sparse representations, sparse signal processing.\n']"
44,424,44_code_program_software_source code,"['code', 'program', 'software', 'source code', 'programs', 'source', 'language', 'bug', 'programming', 'developers']","['programming language', 'source code', 'programming languages', 'software engineering', 'neural program', 'program synthesis', 'embeddings', 'natural language', 'neural network', 'snippets']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['  Building deep learning models on source code has found many successful\nsoftware engineering applications, such as code search, code comment\ngeneration, bug detection, code migration, and so on. Current learning\ntechniques, however, have a major drawback that these models are mostly trained\non datasets labeled for particular downstream tasks, and code representations\nmay not be suitable for other tasks. While some techniques produce\nrepresentations from unlabeled code, they are far from satisfactory when\napplied to downstream tasks. Although certain techniques generate\nrepresentations from unlabeled code when applied to downstream tasks they are\nfar from satisfactory. This paper proposes InferCode to overcome the limitation\nby adapting the self-supervised learning mechanism to build source code model.\nThe key novelty lies in training code representations by predicting\nautomatically identified subtrees from the context of the ASTs. Subtrees in\nASTs are treated with InferCode as the labels for training code representations\nwithout any human labeling effort or the overhead of expensive graph\nconstruction, and the trained representations are no longer tied to any\nspecific downstream tasks or code units. We trained an InferCode model instance\nusing the Tree-based CNN as the encoder of a large set of Java code and applied\nit to downstream unsupervised tasks such as code clustering, code clone\ndetection, cross-language code search or reused under a transfer learning\nscheme to continue training the model weights for supervised tasks such as code\nclassification and method name prediction. Compared to previous code learning\ntechniques applied to the same downstream tasks, such as Code2Vec, Code2Seq,\nASTNN, higher performance results are achieved using our pre-trained InferCode\nmodel with a significant margin for most tasks including those involving\ndifferent programming languages.\n', '  Program comprehension is a fundamental task in software development and\nmaintenance processes. Software developers often need to understand a large\namount of existing code before they can develop new features or fix bugs in\nexisting programs. Being able to process programming language code\nautomatically and provide summaries of code functionality accurately can\nsignificantly help developers to reduce time spent in code navigation and\nunderstanding, and thus increase productivity. Different from natural language\narticles, source code in programming languages often follows rigid syntactical\nstructures and there can exist dependencies among code elements that are\nlocated far away from each other through complex control flows and data flows.\nExisting studies on tree-based convolutional neural networks (TBCNN) and gated\ngraph neural networks (GGNN) are not able to capture essential semantic\ndependencies among code elements accurately. In this paper, we propose novel\ntree-based capsule networks (TreeCaps) and relevant techniques for processing\nprogram code in an automated way that encodes code syntactical structures and\ncaptures code dependencies more accurately. Based on evaluation on programs\nwritten in different programming languages, we show that our TreeCaps-based\napproach can outperform other approaches in classifying the functionalities of\nmany programs.\n', '  Two key contributions presented in this paper are: i) A method for building a\ndataset containing source code features extracted from source files taken from\nOpen Source Software (OSS) and associated bug reports, ii) A predictive model\nfor estimating defectiveness of a given source code. These artifacts can be\nuseful for building tools and techniques pertaining to several automated\nsoftware engineering areas such as bug localization, code review, and\nrecommendation and program repair.\n  In order to achieve our goal, we first extract coding style information (e.g.\nrelated to programming language constructs used in the source code) for source\ncode files present on GitHub. Then the information available in bug reports (if\nany) associated with these source code files are extracted. Thus fetched un(/\nsemi)-structured information is then transformed into a structured knowledge\nbase. We considered more than 30400 source code files from 20 different GitHub\nrepositories with about 14950 associated bug reports across 4 bug tracking\nportals. The source code files considered are written in four programming\nlanguages (viz., C, C++, Java, and Python) and belong to different types of\napplications.\n  A machine learning (ML) model for estimating the defectiveness of a given\ninput source code is then trained using the knowledge base. In order to pick\nthe best ML model, we evaluated 8 different ML algorithms such as Random\nForest, K Nearest Neighbour and SVM with around 50 parameter configurations to\ncompare their performance on our tasks. One of our findings shows that best\nK-fold (with k=5) cross-validation results are obtained with the NuSVM\ntechnique that gives a mean F1 score of 0.914.\n']"
45,405,45_entity_knowledge_relation_entities,"['entity', 'knowledge', 'relation', 'entities', 'knowledge graph', 'knowledge graphs', 'relations', 'kg', 'graph', 'relation extraction']","['knowledge graphs', 'graph embedding', 'knowledge graph', 'embedding models', 'embeddings', 'embedding', 'kg embedding', 'knowledge bases', 'entity linking', 'relation extraction']","['Knowledge graph completion', '', '', '', '', '', '', '', '', '']","['Knowledge graphs', '', '', '', '', '', '', '', '', '']","['  Relation Extraction (RE) is a vital step to complete Knowledge Graph (KG) by\nextracting entity relations from texts.However, it usually suffers from the\nlong-tail issue. The training data mainly concentrates on a few types of\nrelations, leading to the lackof sufficient annotations for the remaining types\nof relations. In this paper, we propose a general approach to learn relation\nprototypesfrom unlabeled texts, to facilitate the long-tail relation extraction\nby transferring knowledge from the relation types with sufficient trainingdata.\nWe learn relation prototypes as an implicit factor between entities, which\nreflects the meanings of relations as well as theirproximities for transfer\nlearning. Specifically, we construct a co-occurrence graph from texts, and\ncapture both first-order andsecond-order entity proximities for embedding\nlearning. Based on this, we further optimize the distance from entity pairs\ntocorresponding prototypes, which can be easily adapted to almost arbitrary RE\nframeworks. Thus, the learning of infrequent or evenunseen relation types will\nbenefit from semantically proximate relations through pairs of entities and\nlarge-scale textual information.We have conducted extensive experiments on two\npublicly available datasets: New York Times and Google Distant\nSupervision.Compared with eight state-of-the-art baselines, our proposed model\nachieves significant improvements (4.1% F1 on average). Furtherresults on\nlong-tail relations demonstrate the effectiveness of the learned relation\nprototypes. We further conduct an ablation study toinvestigate the impacts of\nvarying components, and apply it to four basic relation extraction models to\nverify the generalization ability.Finally, we analyze several example cases to\ngive intuitive impressions as qualitative analysis. Our codes will be released\nlater.\n', '  Knowledge graph completion aims to predict missing relations between entities\nin a knowledge graph. In this work, we propose a relational message passing\nmethod for knowledge graph completion. Different from existing embedding-based\nmethods, relational message passing only considers edge features (i.e.,\nrelation types) without entity IDs in the knowledge graph, and passes\nrelational messages among edges iteratively to aggregate neighborhood\ninformation. Specifically, two kinds of neighborhood topology are modeled for a\ngiven entity pair under the relational message passing framework: (1)\nRelational context, which captures the relation types of edges adjacent to the\ngiven entity pair; (2) Relational paths, which characterize the relative\nposition between the given two entities in the knowledge graph. The two message\npassing modules are combined together for relation prediction. Experimental\nresults on knowledge graph benchmarks as well as our newly proposed dataset\nshow that, our method PathCon outperforms state-of-the-art knowledge graph\ncompletion methods by a large margin. PathCon is also shown applicable to\ninductive settings where entities are not seen in training stage, and it is\nable to provide interpretable explanations for the predicted results. The code\nand all datasets are available at https://github.com/hwwang55/PathCon.\n', '  We introduce a new method DOLORES for learning knowledge graph embeddings\nthat effectively captures contextual cues and dependencies among entities and\nrelations. First, we note that short paths on knowledge graphs comprising of\nchains of entities and relations can encode valuable information regarding\ntheir contextual usage. We operationalize this notion by representing knowledge\ngraphs not as a collection of triples but as a collection of entity-relation\nchains, and learn embeddings for entities and relations using deep neural\nmodels that capture such contextual usage. In particular, our model is based on\nBi-Directional LSTMs and learn deep representations of entities and relations\nfrom constructed entity-relation chains. We show that these representations can\nvery easily be incorporated into existing models to significantly advance the\nstate of the art on several knowledge graph prediction tasks like link\nprediction, triple classification, and missing relation type prediction (in\nsome cases by at least 9.5%).\n']"
46,382,46_satellite_remote sensing_images_remote,"['satellite', 'remote sensing', 'images', 'remote', 'crop', 'image', 'imagery', 'sensing', 'classification', 'land']","['satellite imagery', 'remote sensing', 'satellite images', 'multispectral', 'hyperspectral image', 'convolutional neural', 'hyperspectral', 'neural networks', 'deep learning', 'land cover']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['  Land cover mapping is essential to monitoring the environment and\nunderstanding the effects of human activities on it. The automatic approaches\nto land cover mapping (i.e., image segmentation) mostly used traditional\nmachine learning that requires heuristic feature design. On natural images,\ndeep learning has outperformed traditional machine learning approaches for\nimage segmentation. On remote sensing images, recent studies demonstrate\nsuccessful applications of specific deep learning models to small-scale land\ncover mapping tasks (e.g., to classify wetland complexes). However, it is not\nreadily clear which of the existing models are the best candidates for which\nremote sensing task. In this study, we answer that question for mapping the\nfundamental land cover classes using satellite radar data. We took Sentinel-1\nC-band SAR images available at no cost to users as representative data. CORINE\nland cover map was used as a reference, and the models were trained to\ndistinguish between the 5 major CORINE classes. We selected seven among the\nstate-of-the-art semantic segmentation models so that they cover a diverse set\nof approaches: U-Net, DeepLabV3+, PSPNet, BiSeNet, SegNet, FC-DenseNet, and\nFRRN-B. The models were pre-trained on the ImageNet dataset and further\nfine-tuned in this study. All the models demonstrated solid performance with\noverall accuracy between 87.9% and 93.1%, and with good to a very good\nagreement (kappa statistic between 0.75 and 0.86). The two best models were\nFC-DenseNet and SegNet, with the latter having a much smaller inference time.\nOverall, our results indicate that the semantic segmentation models are\nsuitable for efficient wide-area mapping using satellite SAR imagery and also\nprovide baseline accuracy against which the newly proposed models should be\nevaluated.\n', '  The increasing spatial and temporal resolution of globally available\nsatellite images, such as provided by Sentinel-2, creates new possibilities for\nresearchers to use freely available multi-spectral optical images, with\ndecametric spatial resolution and more frequent revisits for remote sensing\napplications such as land cover and crop classification (LC&CC), agricultural\nmonitoring and management, environment monitoring. Existing solutions dedicated\nto cropland mapping can be categorized based on per-pixel based and\nobject-based. However, it is still challenging when more classes of\nagricultural crops are considered at a massive scale. In this paper, a novel\nand optimal deep learning model for pixel-based LC&CC is developed and\nimplemented based on Recurrent Neural Networks (RNN) in combination with\nConvolutional Neural Networks (CNN) using multi-temporal sentinel-2 imagery of\ncentral north part of Italy, which has diverse agricultural system dominated by\neconomic crop types. The proposed methodology is capable of automated feature\nextraction by learning time correlation of multiple images, which reduces\nmanual feature engineering and modeling crop phenological stages. Fifteen\nclasses, including major agricultural crops, were considered in this study. We\nalso tested other widely used traditional machine learning algorithms for\ncomparison such as support vector machine SVM, random forest (RF), Kernal SVM,\nand gradient boosting machine, also called XGBoost. The overall accuracy\nachieved by our proposed Pixel R-CNN was 96.5%, which showed considerable\nimprovements in comparison with existing mainstream methods. This study showed\nthat Pixel R-CNN based model offers a highly accurate way to assess and employ\ntime-series data for multi-temporal classification tasks.\n', '  The land cover classification has played an important role in remote sensing\nbecause it can intelligently identify things in one huge remote sensing image\nto reduce the work of humans. However, a lot of classification methods are\ndesigned based on the pixel feature or limited spatial feature of the remote\nsensing image, which limits the classification accuracy and universality of\ntheir methods. This paper proposed a novel method to take into the information\nof remote sensing image, i.e., geographic latitude-longitude information. In\naddition, a dual-branch convolutional neural network (CNN) classification\nmethod is designed in combination with the global information to mine the pixel\nfeatures of the image. Then, the features of the two neural networks are fused\nwith another fully neural network to realize the classification of remote\nsensing images. Finally, two remote sensing images are used to verify the\neffectiveness of our method, including hyperspectral imaging (HSI) and\npolarimetric synthetic aperture radar (PolSAR) imagery. The result of the\nproposed method is superior to the traditional single-channel convolutional\nneural network.\n']"
47,380,47_pruning_compression_network_networks,"['pruning', 'compression', 'network', 'networks', 'pruned', 'sparsity', 'accuracy', 'neural', 'weight', 'neural networks']","['network pruning', 'channel pruning', 'pruning quantization', 'based pruning', 'neural networks', 'filter pruning', 'model compression', 'neural network', 'pruning methods', 'deep neural']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['Neural Network Pruning', '', '', '', '', '', '', '', '', '']","['  Pruning neural network parameters is often viewed as a means to compress\nmodels, but pruning has also been motivated by the desire to prevent\noverfitting. This motivation is particularly relevant given the perhaps\nsurprising observation that a wide variety of pruning approaches increase test\naccuracy despite sometimes massive reductions in parameter counts. To better\nunderstand this phenomenon, we analyze the behavior of pruning over the course\nof training, finding that pruning\'s benefit to generalization increases with\npruning\'s instability (defined as the drop in test accuracy immediately\nfollowing pruning). We demonstrate that this ""generalization-stability\ntradeoff"" is present across a wide variety of pruning settings and propose a\nmechanism for its cause: pruning regularizes similarly to noise injection.\nSupporting this, we find less pruning stability leads to more model flatness\nand the benefits of pruning do not depend on permanent parameter removal. These\nresults explain the compatibility of pruning-based generalization improvements\nand the high generalization recently observed in overparameterized networks.\n', '  Modern deep networks have millions to billions of parameters, which leads to\nhigh memory and energy requirements during training as well as during inference\non resource-constrained edge devices. Consequently, pruning techniques have\nbeen proposed that remove less significant weights in deep networks, thereby\nreducing their memory and computational requirements. Pruning is usually\nperformed after training the original network, and is followed by further\nretraining to compensate for the accuracy loss incurred during pruning. The\nprune-and-retrain procedure is repeated iteratively until an optimum tradeoff\nbetween accuracy and efficiency is reached. However, such iterative retraining\nadds to the overall training complexity of the network. In this work, we\npropose a dynamic pruning-while-training procedure, wherein we prune filters of\nthe convolutional layers of a deep network during training itself, thereby\nprecluding the need for separate retraining. We evaluate our dynamic\npruning-while-training approach with three different pre-existing pruning\nstrategies, viz. mean activation-based pruning, random pruning, and L1\nnormalization-based pruning. Our results for VGG-16 trained on CIFAR10 shows\nthat L1 normalization provides the best performance among all the techniques\nexplored in this work with less than 1% drop in accuracy after pruning 80% of\nthe filters compared to the original network. We further evaluated the L1\nnormalization based pruning mechanism on CIFAR100. Results indicate that\npruning while training yields a compressed network with almost no accuracy loss\nafter pruning 50% of the filters compared to the original network and ~5% loss\nfor high pruning rates (>80%). The proposed pruning methodology yields 41%\nreduction in the number of computations and memory accesses during training for\nCIFAR10, CIFAR100 and ImageNet compared to training with retraining for 10\nepochs .\n', '  The rapidly growing parameter volume of deep neural networks (DNNs) hinders\nthe artificial intelligence applications on resource constrained devices, such\nas mobile and wearable devices. Neural network pruning, as one of the\nmainstream model compression techniques, is under extensive study to reduce the\nnumber of parameters and computations. In contrast to irregular pruning that\nincurs high index storage and decoding overhead, structured pruning techniques\nhave been proposed as the promising solutions. However, prior studies on\nstructured pruning tackle the problem mainly from the perspective of\nfacilitating hardware implementation, without analyzing the characteristics of\nsparse neural networks. The neglect on the study of sparse neural networks\ncauses inefficient trade-off between regularity and pruning ratio.\nConsequently, the potential of structurally pruning neural networks is not\nsufficiently mined.\n  In this work, we examine the structural characteristics of the irregularly\npruned weight matrices, such as the diverse redundancy of different rows, the\nsensitivity of different rows to pruning, and the positional characteristics of\nretained weights. By leveraging the gained insights as a guidance, we first\npropose the novel block-max weight masking (BMWM) method, which can effectively\nretain the salient weights while imposing high regularity to the weight matrix.\nAs a further optimization, we propose a density-adaptive regular-block (DARB)\npruning that outperforms prior structured pruning work with high pruning ratio\nand decoding efficiency. Our experimental results show that DARB can achieve\n13$\\times$ to 25$\\times$ pruning ratio, which are 2.8$\\times$ to 4.3$\\times$\nimprovements than the state-of-the-art counterparts on multiple neural network\nmodels and tasks. Moreover, DARB can achieve 14.3$\\times$ decoding efficiency\nthan block pruning with higher pruning ratio.\n']"
48,366,48_continual_forgetting_continual learning_catastrophic forgetting,"['continual', 'forgetting', 'continual learning', 'catastrophic forgetting', 'catastrophic', 'tasks', 'learning', 'task', 'incremental', 'new']","['continual learning', 'incremental learning', 'catastrophic forgetting', 'learned tasks', 'episodic memory', 'neural networks', 'forgetting previously', 'continual', 'generative replay', 'forgetting']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['  Most artificial intelligence models have limiting ability to solve new tasks\nfaster, without forgetting previously acquired knowledge. The recently emerging\nparadigm of continual learning aims to solve this issue, in which the model\nlearns various tasks in a sequential fashion. In this work, a novel approach\nfor continual learning is proposed, which searches for the best neural\narchitecture for each coming task via sophisticatedly designed reinforcement\nlearning strategies. We name it as Reinforced Continual Learning. Our method\nnot only has good performance on preventing catastrophic forgetting but also\nfits new tasks well. The experiments on sequential classification tasks for\nvariants of MNIST and CIFAR-100 datasets demonstrate that the proposed approach\noutperforms existing continual learning alternatives for deep networks.\n', '  Catastrophic forgetting in neural networks is a significant problem for\ncontinual learning. A majority of the current methods replay previous data\nduring training, which violates the constraints of an ideal continual learning\nsystem. Additionally, current approaches that deal with forgetting ignore the\nproblem of catastrophic remembering, i.e. the worsening ability to discriminate\nbetween data from different tasks. In our work, we introduce Relevance Mapping\nNetworks (RMNs) which are inspired by the Optimal Overlap Hypothesis. The\nmappings reflects the relevance of the weights for the task at hand by\nassigning large weights to essential parameters. We show that RMNs learn an\noptimized representational overlap that overcomes the twin problem of\ncatastrophic forgetting and remembering. Our approach achieves state-of-the-art\nperformance across all common continual learning datasets, even significantly\noutperforming data replay methods while not violating the constraints for an\nideal continual learning system. Moreover, RMNs retain the ability to detect\ndata from new tasks in an unsupervised manner, thus proving their resilience\nagainst catastrophic remembering.\n', '  Continual learning aims to learn continuously from a stream of tasks and data\nin an online-learning fashion, being capable of exploiting what was learned\npreviously to improve current and future tasks while still being able to\nperform well on the previous tasks. One common limitation of many existing\ncontinual learning methods is that they often train a model directly on all\navailable training data without validation due to the nature of continual\nlearning, thus suffering poor generalization at test time. In this work, we\npresent a novel framework of continual learning named ""Bilevel Continual\nLearning"" (BCL) by unifying a {\\it bilevel optimization} objective and a {\\it\ndual memory management} strategy comprising both episodic memory and\ngeneralization memory to achieve effective knowledge transfer to future tasks\nand alleviate catastrophic forgetting on old tasks simultaneously. Our\nextensive experiments on continual learning benchmarks demonstrate the efficacy\nof the proposed BCL compared to many state-of-the-art methods. Our\nimplementation is available at\nhttps://github.com/phquang/bilevel-continual-learning.\n']"
49,363,49_eeg_brain_bci_eeg signals,"['eeg', 'brain', 'bci', 'eeg signals', 'signals', 'seizure', 'brain computer', 'eeg data', 'classification', 'electroencephalography']","['eeg based', 'eeg classification', 'electroencephalography', 'electroencephalography eeg', 'brain computer', 'electroencephalogram', 'brain signals', 'electroencephalogram eeg', 'eeg data', 'eeg signal']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['A novel neural network architecture for EEG-based motor imagery classification', '', '', '', '', '', '', '', '', '']","['  In previous studies, decoding electroencephalography (EEG) signals has not\nconsidered the topological relationship of EEG electrodes. However, the latest\nneuroscience has suggested brain network connectivity. Thus, the exhibited\ninteraction between EEG channels might not be appropriately measured via\nEuclidean distance. To fill the gap, an attention-based graph residual network,\na novel structure of Graph Convolutional Neural Network (GCN), was presented to\ndetect human motor intents from raw EEG signals, where the topological\nstructure of EEG electrodes was built as a graph. Meanwhile, deep residual\nlearning with a full-attention architecture was introduced to address the\ndegradation problem concerning deeper networks in raw EEG motor imagery (MI)\ndata. Individual variability, the critical and longstanding challenge\nunderlying EEG signals, has been successfully handled with the state-of-the-art\nperformance, 98.08% accuracy at the subject level, 94.28% for 20 subjects.\nNumerical results were promising that the implementation of the\ngraph-structured topology was superior to decode raw EEG data. The innovative\ndeep learning approach was expected to entail a universal method towards both\nneuroscience research and real-world EEG-based practical applications, e.g.,\nseizure prediction.\n', '  Brain-Computer Interface (BCI) system provides a pathway between humans and\nthe outside world by analyzing brain signals which contain potential neural\ninformation. Electroencephalography (EEG) is one of most commonly used brain\nsignals and EEG recognition is an important part of BCI system. Recently,\nconvolutional neural networks (ConvNet) in deep learning are becoming the new\ncutting edge tools to tackle the problem of EEG recognition. However, training\nan effective deep learning model requires a big number of data, which limits\nthe application of EEG datasets with a small number of samples. In order to\nsolve the issue of data insufficiency in deep learning for EEG decoding, we\npropose a novel data augmentation method that add perturbations to amplitudes\nof EEG signals after transform them to frequency domain. In experiments, we\nexplore the performance of signal recognition with the state-of-the-art models\nbefore and after data augmentation on BCI Competition IV dataset 2a and our\nlocal dataset. The results show that our data augmentation technique can\nimprove the accuracy of EEG recognition effectively.\n', '  Classification of EEG-based motor imagery (MI) is a crucial non-invasive\napplication in brain-computer interface (BCI) research. This paper proposes a\nnovel convolutional neural network (CNN) architecture for accurate and robust\nEEG-based MI classification that outperforms the state-of-the-art methods. The\nproposed CNN model, namely EEG-Inception, is built on the backbone of the\nInception-Time network, which showed to be highly efficient and accurate for\ntime-series classification. Also, the proposed network is an end-to-end\nclassification, as it takes the raw EEG signals as the input and does not\nrequire complex EEG signal-preprocessing. Furthermore, this paper proposes a\nnovel data augmentation method for EEG signals to enhance the accuracy, at\nleast by 3%, and reduce overfitting with limited BCI datasets. The proposed\nmodel outperforms all the state-of-the-art methods by achieving the average\naccuracy of 88.4% and 88.6% on the 2008 BCI Competition IV 2a (four-classes)\nand 2b datasets (binary-classes), respectively. Furthermore, it takes less than\n0.025 seconds to test a sample suitable for real-time processing. Moreover, the\nclassification standard deviation for nine different subjects achieves the\nlowest value of 5.5 for the 2b dataset and 7.1 for the 2a dataset, which\nvalidates that the proposed method is highly robust. From the experiment\nresults, it can be inferred that the EEG-Inception network exhibits a strong\npotential as a subject-independent classifier for EEG-based MI tasks.\n']"
50,359,50_spiking_snns_spike_neurons,"['spiking', 'snns', 'spike', 'neurons', 'spiking neural', 'neural', 'synaptic', 'neuromorphic', 'networks', 'snn']","['spiking neural', 'spiking neurons', 'spike based', 'synaptic plasticity', 'neuromorphic hardware', 'networks snns', 'neural network', 'neural networks', 'spike timing', 'artificial neural']","['Spiking neural networks', '', '', '', '', '', '', '', '', '']","['Spiking neural networks', '', '', '', '', '', '', '', '', '']","[""  Spiking Neural Networks (SNNs) are brain-inspired, event-driven machine\nlearning algorithms that have been widely recognized in producing\nultra-high-energy-efficient hardware. Among existing SNNs, unsupervised SNNs\nbased on synaptic plasticity, especially Spike-Timing-Dependent Plasticity\n(STDP), are considered to have great potential in imitating the learning\nprocess of the biological brain. Nevertheless, the existing STDP-based SNNs\nhave limitations in constrained learning capability and/or slow learning speed.\nMost STDP-based SNNs adopted a slow-learning Fully-Connected (FC) architecture\nand used a sub-optimal vote-based scheme for spike decoding. In this paper, we\novercome these limitations with: 1) a design of high-parallelism network\narchitecture, inspired by the Inception module in Artificial Neural Networks\n(ANNs); 2) use of a Vote-for-All (VFA) decoding layer as a replacement to the\nstandard vote-based spike decoding scheme, to reduce the information loss in\nspike decoding and, 3) a proposed adaptive repolarization (resetting) mechanism\nthat accelerates SNNs' learning by enhancing spiking activities. Our\nexperimental results on two established benchmark datasets (MNIST/EMNIST) show\nthat our network architecture resulted in superior performance compared to the\nwidely used FC architecture and a more advanced Locally-Connected (LC)\narchitecture, and that our SNN achieved competitive results with\nstate-of-the-art unsupervised SNNs (95.64%/80.11% accuracy on the MNIST/EMNISE\ndataset) while having superior learning efficiency and robustness against\nhardware damage. Our SNN achieved great classification accuracy with only\nhundreds of training iterations, and random destruction of large numbers of\nsynapses or neurons only led to negligible performance degradation.\n"", '  Spiking Neural Networks (SNNs) use spatio-temporal spike patterns to\nrepresent and transmit information, which is not only biologically realistic\nbut also suitable for ultra-low-power event-driven neuromorphic implementation.\nMotivated by the success of deep learning, the study of Deep Spiking Neural\nNetworks (DeepSNNs) provides promising directions for artificial intelligence\napplications. However, training of DeepSNNs is not straightforward because the\nwell-studied error back-propagation (BP) algorithm is not directly applicable.\nIn this paper, we first establish an understanding as to why error\nback-propagation does not work well in DeepSNNs. To address this problem, we\npropose a simple yet efficient Rectified Linear Postsynaptic Potential function\n(ReL-PSP) for spiking neurons and propose a Spike-Timing-Dependent\nBack-Propagation (STDBP) learning algorithm for DeepSNNs. In STDBP algorithm,\nthe timing of individual spikes is used to convey information (temporal\ncoding), and learning (back-propagation) is performed based on spike timing in\nan event-driven manner. Our experimental results show that the proposed\nlearning algorithm achieves state-of-the-art classification accuracy in single\nspike time based learning algorithms of DeepSNNs. Furthermore, by utilizing the\ntrained model parameters obtained from the proposed STDBP learning algorithm,\nwe demonstrate the ultra-low-power inference operations on a recently proposed\nneuromorphic inference accelerator. Experimental results show that the\nneuromorphic hardware consumes 0.751~mW of the total power consumption and\nachieves a low latency of 47.71~ms to classify an image from the MNIST dataset.\nOverall, this work investigates the contribution of spike timing dynamics to\ninformation encoding, synaptic plasticity and decision making, providing a new\nperspective to design of future DeepSNNs and neuromorphic hardware systems.\n', '  Spiking neural networks (SNNs) have garnered a great amount of interest for\nsupervised and unsupervised learning applications. This paper deals with the\nproblem of training multi-layer feedforward SNNs. The non-linear\nintegrate-and-fire dynamics employed by spiking neurons make it difficult to\ntrain SNNs to generate desired spike trains in response to a given input. To\ntackle this, first the problem of training a multi-layer SNN is formulated as\nan optimization problem such that its objective function is based on the\ndeviation in membrane potential rather than the spike arrival instants. Then,\nan optimization method named Normalized Approximate Descent (NormAD),\nhand-crafted for such non-convex optimization problems, is employed to derive\nthe iterative synaptic weight update rule. Next, it is reformulated to\nefficiently train multi-layer SNNs, and is shown to be effectively performing\nspatio-temporal error backpropagation. The learning rule is validated by\ntraining $2$-layer SNNs to solve a spike based formulation of the XOR problem\nas well as training $3$-layer SNNs for generic spike based training problems.\nThus, the new algorithm is a key step towards building deep spiking neural\nnetworks capable of efficient event-triggered learning.\n']"
51,353,51_regret_online_convex_algorithm,"['regret', 'online', 'convex', 'algorithm', 'bounds', 'online learning', 'bound', 'algorithms', 'experts', 'setting']","['online optimization', 'optimal regret', 'dynamic regret', 'regret bounds', 'online algorithms', 'convex optimization', 'online algorithm', 'online convex', 'online gradient', 'regret bound']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['Online Learning', '', '', '', '', '', '', '', '', '']","['  We consider online algorithms under both the competitive ratio criteria and\nthe regret minimization one. Our main goal is to build a unified methodology\nthat would be able to guarantee both criteria simultaneously.\n  For a general class of online algorithms, namely any Metrical Task System\n(MTS), we show that one can simultaneously guarantee the best known competitive\nratio and a natural regret bound. For the paging problem we further show an\nefficient online algorithm (polynomial in the number of pages) with this\nguarantee.\n  To this end, we extend an existing regret minimization algorithm\n(specifically, Kapralov and Panigrahy) to handle movement cost (the cost of\nswitching between states of the online system). We then show how to use the\nextended regret minimization algorithm to combine multiple online algorithms.\nOur end result is an online algorithm that can combine a ""base"" online\nalgorithm, having a guaranteed competitive ratio, with a range of online\nalgorithms that guarantee a small regret over any interval of time. The\ncombined algorithm guarantees both that the competitive ratio matches that of\nthe base algorithm and a low regret over any time interval.\n  As a by product, we obtain an expert algorithm with close to optimal regret\nbound on every time interval, even in the presence of switching costs. This\nresult is of independent interest.\n', '  In citep{Hazan-2008-extract}, the authors showed that the regret of online\nlinear optimization can be bounded by the total variation of the cost vectors.\nIn this paper, we extend this result to general online convex optimization. We\nfirst analyze the limitations of the algorithm in \\citep{Hazan-2008-extract}\nwhen applied it to online convex optimization. We then present two algorithms\nfor online convex optimization whose regrets are bounded by the variation of\ncost functions. We finally consider the bandit setting, and present a\nrandomized algorithm for online bandit convex optimization with a\nvariation-based regret bound. We show that the regret bound for online bandit\nconvex optimization is optimal when the variation of cost functions is\nindependent of the number of trials.\n', '  This paper considers the stability of online learning algorithms and its\nimplications for learnability (bounded regret). We introduce a novel quantity\ncalled {\\em forward regret} that intuitively measures how good an online\nlearning algorithm is if it is allowed a one-step look-ahead into the future.\nWe show that given stability, bounded forward regret is equivalent to bounded\nregret. We also show that the existence of an algorithm with bounded regret\nimplies the existence of a stable algorithm with bounded regret and bounded\nforward regret. The equivalence results apply to general, possibly non-convex\nproblems. To the best of our knowledge, our analysis provides the first general\nconnection between stability and regret in the online setting that is not\nrestricted to a particular class of algorithms. Our stability-regret connection\nprovides a simple recipe for analyzing regret incurred by any online learning\nalgorithm. Using our framework, we analyze several existing online learning\nalgorithms as well as the ""approximate"" versions of algorithms like RDA that\nsolve an optimization problem at each iteration. Our proofs are simpler than\nexisting analysis for the respective algorithms, show a clear trade-off between\nstability and forward regret, and provide tighter regret bounds in some cases.\nFurthermore, using our recipe, we analyze ""approximate"" versions of several\nalgorithms such as follow-the-regularized-leader (FTRL) that requires solving\nan optimization problem at each step.\n']"
52,343,52_activity_activity recognition_har_gait,"['activity', 'activity recognition', 'har', 'gait', 'recognition', 'sensor', 'activities', 'human', 'human activity', 'sensors']","['activity recognition', 'recognition har', 'wearable sensors', 'accelerometer', 'human activity', 'physical activity', 'sensor data', 'sensor based', 'fall detection', 'wearable devices']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['Human activity recognition', '', '', '', '', '', '', '', '', '']","['  Activity recognition is the ability to identify and recognize the action or\ngoals of the agent. The agent can be any object or entity that performs action\nthat has end goals. The agents can be a single agent performing the action or\ngroup of agents performing the actions or having some interaction. Human\nactivity recognition has gained popularity due to its demands in many practical\napplications such as entertainment, healthcare, simulations and surveillance\nsystems. Vision based activity recognition is gaining advantage as it does not\nrequire any human intervention or physical contact with humans. Moreover, there\nare set of cameras that are networked with the intention to track and recognize\nthe activities of the agent. Traditional applications that were required to\ntrack or recognize human activities made use of wearable devices. However, such\napplications require physical contact of the person. To overcome such\nchallenges, vision based activity recognition system can be used, which uses a\ncamera to record the video and a processor that performs the task of\nrecognition. The work is implemented in two stages. In the first stage, an\napproach for the Implementation of Activity recognition is proposed using\nbackground subtraction of images, followed by 3D- Convolutional Neural\nNetworks. The impact of using Background subtraction prior to 3D-Convolutional\nNeural Networks has been reported. In the second stage, the work is further\nextended and implemented on Raspberry Pi, that can be used to record a stream\nof video, followed by recognizing the activity that was involved in the video.\nThus, a proof-of-concept for activity recognition using small, IoT based\ndevice, is provided, which can enhance the system and extend its applications\nin various forms like, increase in portability, networking, and other\ncapabilities of the device.\n', '  Wearable sensor based human activity recognition is a challenging problem due\nto difficulty in modeling spatial and temporal dependencies of sensor signals.\nRecognition models in closed-set assumption are forced to yield members of\nknown activity classes as prediction. However, activity recognition models can\nencounter an unseen activity due to body-worn sensor malfunction or disability\nof the subject performing the activities. This problem can be addressed through\nmodeling solution according to the assumption of open-set recognition. Hence,\nthe proposed self attention based approach combines data hierarchically from\ndifferent sensor placements across time to classify closed-set activities and\nit obtains notable performance improvement over state-of-the-art models on five\npublicly available datasets. The decoder in this autoencoder architecture\nincorporates self-attention based feature representations from encoder to\ndetect unseen activity classes in open-set recognition setting. Furthermore,\nattention maps generated by the hierarchical model demonstrate explainable\nselection of features in activity recognition. We conduct extensive leave one\nsubject out validation experiments that indicate significantly improved\nrobustness to noise and subject specific variability in body-worn sensor\nsignals. The source code is available at:\ngithub.com/saif-mahmud/hierarchical-attention-HAR\n', '  Despite the widespread installation of accelerometers in almost all mobile\nphones and wearable devices, activity recognition using accelerometers is still\nimmature due to the poor recognition accuracy of existing recognition methods\nand the scarcity of labeled training data. We consider the problem of human\nactivity recognition using triaxial accelerometers and deep learning paradigms.\nThis paper shows that deep activity recognition models (a) provide better\nrecognition accuracy of human activities, (b) avoid the expensive design of\nhandcrafted features in existing systems, and (c) utilize the massive unlabeled\nacceleration samples for unsupervised feature extraction. Moreover, a hybrid\napproach of deep learning and hidden Markov models (DL-HMM) is presented for\nsequential activity recognition. This hybrid approach integrates the\nhierarchical representations of deep activity recognition models with the\nstochastic modeling of temporal sequences in the hidden Markov models. We show\nsubstantial recognition improvement on real world datasets over\nstate-of-the-art methods of human activity recognition using triaxial\naccelerometers.\n']"
53,331,53_logic_reasoning_symbolic_relational,"['logic', 'reasoning', 'symbolic', 'relational', 'logical', 'learning', 'proof', 'ilp', 'knowledge', 'theorem']","['inductive logic', 'logic programming', 'logic programs', 'relational learning', 'probabilistic logic', 'markov logic', 'inductive', 'relational reasoning', 'symbolic reasoning', 'neural network']","['Learning', '', '', '', '', '', '', '', '', '']","['Logic programming', '', '', '', '', '', '', '', '', '']","['  Artificial Intelligence agents are required to learn from their surroundings\nand to reason about the knowledge that has been learned in order to make\ndecisions. While state-of-the-art learning from data typically uses\nsub-symbolic distributed representations, reasoning is normally useful at a\nhigher level of abstraction with the use of a first-order logic language for\nknowledge representation. As a result, attempts at combining symbolic AI and\nneural computation into neural-symbolic systems have been on the increase. In\nthis paper, we present Logic Tensor Networks (LTN), a neurosymbolic formalism\nand computational model that supports learning and reasoning through the\nintroduction of a many-valued, end-to-end differentiable first-order logic\ncalled Real Logic as a representation language for deep learning. We show that\nLTN provides a uniform language for the specification and the computation of\nseveral AI tasks such as data clustering, multi-label classification,\nrelational learning, query answering, semi-supervised learning, regression and\nembedding learning. We implement and illustrate each of the above tasks with a\nnumber of simple explanatory examples using TensorFlow 2. Keywords:\nNeurosymbolic AI, Deep Learning and Reasoning, Many-valued Logic.\n', '  Rules complement and extend ontologies on the Semantic Web. We refer to these\nrules as onto-relational since they combine DL-based ontology languages and\nKnowledge Representation formalisms supporting the relational data model within\nthe tradition of Logic Programming and Deductive Databases. Rule authoring is a\nvery demanding Knowledge Engineering task which can be automated though\npartially by applying Machine Learning algorithms. In this chapter we show how\nInductive Logic Programming (ILP), born at the intersection of Machine Learning\nand Logic Programming and considered as a major approach to Relational\nLearning, can be adapted to Onto-Relational Learning. For the sake of\nillustration, we provide details of a specific Onto-Relational Learning\nsolution to the problem of learning rule-based definitions of DL concepts and\nroles with ILP.\n', '  Recent years have seen a surge of interest in Probabilistic Logic Programming\n(PLP) and Statistical Relational Learning (SRL) models that combine logic with\nprobabilities. Structure learning of these systems is an intersection area of\nInductive Logic Programming (ILP) and statistical learning (SL). However, ILP\ncannot deal with probabilities, SL cannot model relational hypothesis. The\nbiggest challenge of integrating these two machine learning frameworks is how\nto estimate the probability of a logic clause only from the observation of\ngrounded logic atoms. Many current methods models a joint probability by\nrepresenting clause as graphical model and literals as vertices in it. This\nmodel is still too complicate and only can be approximate by pseudo-likelihood.\nWe propose Inductive Logic Boosting framework to transform the relational\ndataset into a feature-based dataset, induces logic rules by boosting Problog\nRule Trees and relaxes the independence constraint of pseudo-likelihood.\nExperimental evaluation on benchmark datasets demonstrates that the AUC-PR and\nAUC-ROC value of ILP learned rules are higher than current state-of-the-art SRL\nmethods.\n']"
54,330,54_gp_gaussian_gaussian processes_gaussian process,"['gp', 'gaussian', 'gaussian processes', 'gaussian process', 'gps', 'processes', 'process', 'regression', 'inference', 'kernel']","['gaussian processes', 'gaussian process', 'variational inference', 'deep gaussian', 'gp regression', 'process regression', 'gp models', 'process models', 'non gaussian', 'gaussian']","['Gaussian Processes', '', '', '', '', '', '', '', '', '']","['Gaussian process models', '', '', '', '', '', '', '', '', '']","['  We introduce a new interpretation of sparse variational approximations for\nGaussian processes using inducing points, which can lead to more scalable\nalgorithms than previous methods. It is based on decomposing a Gaussian process\nas a sum of two independent processes: one spanned by a finite basis of\ninducing points and the other capturing the remaining variation. We show that\nthis formulation recovers existing approximations and at the same time allows\nto obtain tighter lower bounds on the marginal likelihood and new stochastic\nvariational inference algorithms. We demonstrate the efficiency of these\nalgorithms in several Gaussian process models ranging from standard regression\nto multi-class classification using (deep) convolutional Gaussian processes and\nreport state-of-the-art results on CIFAR-10 among purely GP-based models.\n', '  We propose a method (TT-GP) for approximate inference in Gaussian Process\n(GP) models. We build on previous scalable GP research including stochastic\nvariational inference based on inducing inputs, kernel interpolation, and\nstructure exploiting algebra. The key idea of our method is to use Tensor Train\ndecomposition for variational parameters, which allows us to train GPs with\nbillions of inducing inputs and achieve state-of-the-art results on several\nbenchmarks. Further, our approach allows for training kernels based on deep\nneural networks without any modifications to the underlying GP model. A neural\nnetwork learns a multidimensional embedding for the data, which is used by the\nGP to make the final prediction. We train GP and neural network parameters\nend-to-end without pretraining, through maximization of GP marginal likelihood.\nWe show the efficiency of the proposed approach on several regression and\nclassification benchmark datasets including MNIST, CIFAR-10, and Airline.\n', '  In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a\ndeep belief network based on Gaussian process mappings. The data is modeled as\nthe output of a multivariate GP. The inputs to that Gaussian process are then\ngoverned by another GP. A single layer model is equivalent to a standard GP or\nthe GP latent variable model (GP-LVM). We perform inference in the model by\napproximate variational marginalization. This results in a strict lower bound\non the marginal likelihood of the model which we use for model selection\n(number of layers and nodes per layer). Deep belief networks are typically\napplied to relatively large data sets using stochastic gradient descent for\noptimization. Our fully Bayesian treatment allows for the application of deep\nmodels even when data is scarce. Model selection by our variational bound shows\nthat a five layer hierarchy is justified even when modelling a digit data set\ncontaining only 150 examples.\n']"
55,324,55_dialogue_dialog_conversational_responses,"['dialogue', 'dialog', 'conversational', 'responses', 'dialogue systems', 'conversation', 'user', 'response', 'oriented', 'human']","['dialogue systems', 'dialog systems', 'oriented dialogue', 'dialogue', 'dialogues', 'dialogs', 'oriented dialog', 'natural language', 'conversational', 'chatbots']","['dialogue systems', '', '', '', '', '', '', '', '', '']","['Dialogue policy learning', '', '', '', '', '', '', '', '', '']","['  Spoken Language Understanding (SLU) is a key component of goal oriented\ndialogue systems that would parse user utterances into semantic frame\nrepresentations. Traditionally SLU does not utilize the dialogue history beyond\nthe previous system turn and contextual ambiguities are resolved by the\ndownstream components. In this paper, we explore novel approaches for modeling\ndialogue context in a recurrent neural network (RNN) based language\nunderstanding system. We propose the Sequential Dialogue Encoder Network, that\nallows encoding context from the dialogue history in chronological order. We\ncompare the performance of our proposed architecture with two context models,\none that uses just the previous turn context and another that encodes dialogue\ncontext in a memory network, but loses the order of utterances in the dialogue\nhistory. Experiments with a multi-domain dialogue dataset demonstrate that the\nproposed architecture results in reduced semantic frame error rates.\n', '  Traditional goal-oriented dialogue systems rely on various components such as\nnatural language understanding, dialogue state tracking, policy learning and\nresponse generation. Training each component requires annotations which are\nhard to obtain for every new domain, limiting scalability of such systems.\nSimilarly, rule-based dialogue systems require extensive writing and\nmaintenance of rules and do not scale either. End-to-End dialogue systems, on\nthe other hand, do not require module-specific annotations but need a large\namount of data for training. To overcome these problems, in this demo, we\npresent Alexa Conversations, a new approach for building goal-oriented dialogue\nsystems that is scalable, extensible as well as data efficient. The components\nof this system are trained in a data-driven manner, but instead of collecting\nannotated conversations for training, we generate them using a novel dialogue\nsimulator based on a few seed dialogues and specifications of APIs and entities\nprovided by the developer. Our approach provides out-of-the-box support for\nnatural conversational phenomena like entity sharing across turns or users\nchanging their mind during conversation without requiring developers to provide\nany such dialogue flows. We exemplify our approach using a simple pizza\nordering task and showcase its value in reducing the developer burden for\ncreating a robust experience. Finally, we evaluate our system using a typical\nmovie ticket booking task and show that the dialogue simulator is an essential\ncomponent of the system that leads to over $50\\%$ improvement in turn-level\naction signature prediction accuracy.\n', '  Building an end-to-end conversational agent for multi-domain task-oriented\ndialogues has been an open challenge for two main reasons. First, tracking\ndialogue states of multiple domains is non-trivial as the dialogue agent must\nobtain complete states from all relevant domains, some of which might have\nshared slots among domains as well as unique slots specifically for one domain\nonly. Second, the dialogue agent must also process various types of information\nacross domains, including dialogue context, dialogue states, and database, to\ngenerate natural responses to users. Unlike the existing approaches that are\noften designed to train each module separately, we propose ""UniConv"" -- a novel\nunified neural architecture for end-to-end conversational systems in\nmulti-domain task-oriented dialogues, which is designed to jointly train (i) a\nBi-level State Tracker which tracks dialogue states by learning signals at both\nslot and domain level independently, and (ii) a Joint Dialogue Act and Response\nGenerator which incorporates information from various input components and\nmodels dialogue acts and target responses simultaneously. We conduct\ncomprehensive experiments in dialogue state tracking, context-to-text, and\nend-to-end settings on the MultiWOZ2.1 benchmark, achieving superior\nperformance over competitive baselines.\n']"
56,318,56_brain_fmri_functional_disease,"['brain', 'fmri', 'functional', 'disease', 'connectivity', 'asd', 'ad', 'neuroimaging', 'alzheimer', 'imaging']","['brain network', 'brain networks', 'neuroimaging data', 'rs fmri', 'fmri', 'fmri data', 'neuroimaging', 'functional connectivity', 'imaging fmri', 'brain connectivity']","['ASD', '', '', '', '', '', '', '', '', '']","['A neural network for predicting cognitive abilities', '', '', '', '', '', '', '', '', '']","['  Objective: Multi-modal functional magnetic resonance imaging (fMRI) can be\nused to make predictions about individual behavioral and cognitive traits based\non brain connectivity networks. Methods: To take advantage of complementary\ninformation from multi-modal fMRI, we propose an interpretable multi-modal\ngraph convolutional network (MGCN) model, incorporating the fMRI time series\nand the functional connectivity (FC) between each pair of brain regions.\nSpecifically, our model learns a graph embedding from individual brain networks\nderived from multi-modal data. A manifold-based regularization term is then\nenforced to consider the relationships of subjects both within and between\nmodalities. Furthermore, we propose the gradient-weighted regression activation\nmapping (Grad-RAM) and the edge mask learning to interpret the model, which is\nused to identify significant cognition-related biomarkers. Results: We validate\nour MGCN model on the Philadelphia Neurodevelopmental Cohort to predict\nindividual wide range achievement test (WRAT) score. Our model obtains superior\npredictive performance over GCN with a single modality and other competing\napproaches. The identified biomarkers are cross-validated from different\napproaches. Conclusion and Significance: This paper develops a new\ninterpretable graph deep learning framework for cognitive ability prediction,\nwith the potential to overcome the limitations of several current data-fusion\nmodels. The results demonstrate the power of MGCN in analyzing multi-modal fMRI\nand discovering significant biomarkers for human brain studies.\n', ""  Characterizing the subtle changes of functional brain networks associated\nwith the pathological cascade of Alzheimer's disease (AD) is important for\nearly diagnosis and prediction of disease progression prior to clinical\nsymptoms. We developed a new deep learning method, termed multiple graph\nGaussian embedding model (MG2G), which can learn highly informative network\nfeatures by mapping high-dimensional resting-state brain networks into a\nlow-dimensional latent space. These latent distribution-based embeddings enable\na quantitative characterization of subtle and heterogeneous brain connectivity\npatterns at different regions and can be used as input to traditional\nclassifiers for various downstream graph analytic tasks, such as AD early stage\nprediction, and statistical evaluation of between-group significant alterations\nacross brain regions. We used MG2G to detect the intrinsic latent\ndimensionality of MEG brain networks, predict the progression of patients with\nmild cognitive impairment (MCI) to AD, and identify brain regions with network\nalterations related to MCI.\n"", '  Resting-state functional MRI (rs-fMRI) in functional neuroimaging techniques\nhave improved in brain disorders, dysfunction studies via mapping the topology\nof the brain connections, i.e. connectopic mapping. Since, there are the slight\ndifferences between healthy and unhealthy brain regions and functions,\ninvestigation into the complex topology of functional and structural brain\nnetworks in human is a complicated task with the growth of evaluation criteria.\nIrregular graph deep learning applications have widely spread to understanding\nhuman cognitive functions that are linked to gene expression and related\ndistributed spatial patterns, because the neuronal networks of the brain can\nhold dynamically a variety of brain solutions with different activity patterns\nand functional connectivity, these applications might also be involved with\nboth node-centric and graph-centric tasks. In this paper, we performed a novel\napproach of individual generative model and high order graph analysis for the\nregion of interest recognition areas of the brain which do not have a normal\nconnection during applying certain tasks. Here, we proposed a high order\nframework of Graph Auto-Encoder (GAE) with a hypersphere distributer for\nfunctional data analysis in brain imaging studies that is underlying\nnon-Euclidean structure in the learning of strong non-rigid graphs among large\nscale data. In addition, we distinguished the possible modes of correlations in\nabnormal brain connections. Our finding will show the degree of correlation\nbetween the affected regions and their simultaneous occurrence over time that\ncan be used to diagnose brain diseases or revealing the ability of the nervous\nsystem to modify in brain topology at all angles, brain plasticity, according\nto input stimuli.\n']"
57,314,57_nas_search_architecture_architecture search,"['nas', 'search', 'architecture', 'architecture search', 'neural architecture', 'architectures', 'search nas', 'search space', 'neural', 'space']","['neural architecture', 'neural architectures', 'nas algorithms', 'neural network', 'neural networks', 'search nas', 'network architecture', 'network architectures', 'architecture search', 'imagenet']","['Neural architecture search', '', '', '', '', '', '', '', '', '']","['Neural Architecture Search', '', '', '', '', '', '', '', '', '']","['  Neural Architecture Search (NAS), aiming at automatically designing network\narchitectures by machines, is hoped and expected to bring about a new\nrevolution in machine learning. Despite these high expectation, the\neffectiveness and efficiency of existing NAS solutions are unclear, with some\nrecent works going so far as to suggest that many existing NAS solutions are no\nbetter than random architecture selection. The inefficiency of NAS solutions\nmay be attributed to inaccurate architecture evaluation. Specifically, to speed\nup NAS, recent works have proposed under-training different candidate\narchitectures in a large search space concurrently by using shared network\nparameters; however, this has resulted in incorrect architecture ratings and\nfurthered the ineffectiveness of NAS.\n  In this work, we propose to modularize the large search space of NAS into\nblocks to ensure that the potential candidate architectures are fully trained;\nthis reduces the representation shift caused by the shared parameters and leads\nto the correct rating of the candidates. Thanks to the block-wise search, we\ncan also evaluate all of the candidate architectures within a block. Moreover,\nwe find that the knowledge of a network model lies not only in the network\nparameters but also in the network architecture. Therefore, we propose to\ndistill the neural architecture (DNA) knowledge from a teacher model as the\nsupervision to guide our block-wise architecture search, which significantly\nimproves the effectiveness of NAS. Remarkably, the capacity of our searched\narchitecture has exceeded the teacher model, demonstrating the practicability\nand scalability of our method. Finally, our method achieves a state-of-the-art\n78.4\\% top-1 accuracy on ImageNet in a mobile setting, which is about a 2.1\\%\ngain over EfficientNet-B0. All of our searched models along with the evaluation\ncode are available online.\n', '  Neural Architecture Search (NAS) technologies have emerged in many domains to\njointly learn the architectures and weights of the neural network. However,\nmost existing NAS works claim they are task-specific and focus only on\noptimizing a single architecture to replace a human-designed neural network, in\nfact, their search processes are almost independent of domain knowledge of the\ntasks. In this paper, we propose Pose Neural Fabrics Search (PoseNFS). We\nexplore a new solution for NAS and human pose estimation task: part-specific\nneural architecture search, which can be seen as a variant of multi-task\nlearning. Firstly, we design a new neural architecture search space, Cell-based\nNeural Fabric (CNF), to learn micro as well as macro neural architecture using\na differentiable search strategy. Then, we view locating human keypoints as\nmultiple disentangled prediction sub-tasks, and then use prior knowledge of\nbody structure as guidance to search for multiple part-specific neural\narchitectures for different human parts. After search, all these part-specific\nCNFs have distinct micro and macro architecture parameters. The results show\nthat such knowledge-guided NAS-based architectures have obvious performance\nimprovements to a hand-designed part-based baseline model. The experiments on\nMPII and MS-COCO datasets demonstrate that PoseNFS\\footnote{Code is available\nat \\url{https://github.com/yangsenius/PoseNFS}} can achieve comparable\nperformance to some efficient and state-of-the-art methods.\n', '  We propose Stochastic Neural Architecture Search (SNAS), an economical\nend-to-end solution to Neural Architecture Search (NAS) that trains neural\noperation parameters and architecture distribution parameters in same round of\nback-propagation, while maintaining the completeness and differentiability of\nthe NAS pipeline. In this work, NAS is reformulated as an optimization problem\non parameters of a joint distribution for the search space in a cell. To\nleverage the gradient information in generic differentiable loss for\narchitecture search, a novel search gradient is proposed. We prove that this\nsearch gradient optimizes the same objective as reinforcement-learning-based\nNAS, but assigns credits to structural decisions more efficiently. This credit\nassignment is further augmented with locally decomposable reward to enforce a\nresource-efficient constraint. In experiments on CIFAR-10, SNAS takes less\nepochs to find a cell architecture with state-of-the-art accuracy than\nnon-differentiable evolution-based and reinforcement-learning-based NAS, which\nis also transferable to ImageNet. It is also shown that child networks of SNAS\ncan maintain the validation accuracy in searching, with which attention-based\nNAS requires parameter retraining to compete, exhibiting potentials to stride\ntowards efficient NAS on big datasets. We have released our implementation at\nhttps://github.com/SNAS-Series/SNAS-Series.\n']"
58,300,58_news_fake_social_fake news,"['news', 'fake', 'social', 'fake news', 'media', 'social media', 'twitter', 'detection', 'political', 'users']","['news detection', 'social networks', 'disinformation', 'fake news', 'social network', 'social media', 'article', 'fact checking', 'bots', 'accuracy']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['Fake news detection', '', '', '', '', '', '', '', '', '']","['  The viral spread of fake news has caused great social harm, making fake news\ndetection an urgent task. Current fake news detection methods rely heavily on\ntext information by learning the extracted news content or writing style of\ninternal knowledge. However, deliberate rumors can mask writing style,\nbypassing language models and invalidating simple text-based models. In fact,\nnews articles and other related components (such as news creators and news\ntopics) can be modeled as a heterogeneous information network (HIN for short).\nIn this paper, we propose a novel fake news detection framework, namely\nHierarchical Graph Attention Network(HGAT), which uses a novel hierarchical\nattention mechanism to perform node representation learning in HIN, and then\ndetects fake news by classifying news article nodes. Experiments on two\nreal-world fake news datasets show that HGAT can outperform text-based models\nand other network-based models. In addition, the experiment proved the\nexpandability and generalizability of our for graph representation learning and\nother node classification related applications in heterogeneous graphs.\n', ""  Today social media has become the primary source for news. Via social media\nplatforms, fake news travel at unprecedented speeds, reach global audiences and\nput users and communities at great risk. Therefore, it is extremely important\nto detect fake news as early as possible. Recently, deep learning based\napproaches have shown improved performance in fake news detection. However, the\ntraining of such models requires a large amount of labeled data, but manual\nannotation is time-consuming and expensive. Moreover, due to the dynamic nature\nof news, annotated samples may become outdated quickly and cannot represent the\nnews articles on newly emerged events. Therefore, how to obtain fresh and\nhigh-quality labeled samples is the major challenge in employing deep learning\nmodels for fake news detection. In order to tackle this challenge, we propose a\nreinforced weakly-supervised fake news detection framework, i.e., WeFEND, which\ncan leverage users' reports as weak supervision to enlarge the amount of\ntraining data for fake news detection. The proposed framework consists of three\nmain components: the annotator, the reinforced selector and the fake news\ndetector. The annotator can automatically assign weak labels for unlabeled news\nbased on users' reports. The reinforced selector using reinforcement learning\ntechniques chooses high-quality samples from the weakly labeled data and\nfilters out those low-quality ones that may degrade the detector's prediction\nperformance. The fake news detector aims to identify fake news based on the\nnews content. We tested the proposed framework on a large collection of news\narticles published via WeChat official accounts and associated user reports.\nExtensive experiments on this dataset show that the proposed WeFEND model\nachieves the best performance compared with the state-of-the-art methods.\n"", ""  Social media are nowadays one of the main news sources for millions of people\naround the globe due to their low cost, easy access and rapid dissemination.\nThis however comes at the cost of dubious trustworthiness and significant risk\nof exposure to 'fake news', intentionally written to mislead the readers.\nAutomatically detecting fake news poses challenges that defy existing\ncontent-based analysis approaches. One of the main reasons is that often the\ninterpretation of the news requires the knowledge of political or social\ncontext or 'common sense', which current NLP algorithms are still missing.\nRecent studies have shown that fake and real news spread differently on social\nmedia, forming propagation patterns that could be harnessed for the automatic\nfake news detection. Propagation-based approaches have multiple advantages\ncompared to their content-based counterparts, among which is language\nindependence and better resilience to adversarial attacks. In this paper we\nshow a novel automatic fake news detection model based on geometric deep\nlearning. The underlying core algorithms are a generalization of classical CNNs\nto graphs, allowing the fusion of heterogeneous data such as content, user\nprofile and activity, social graph, and news propagation. Our model was trained\nand tested on news stories, verified by professional fact-checking\norganizations, that were spread on Twitter. Our experiments indicate that\nsocial network structure and propagation are important features allowing highly\naccurate (92.7% ROC AUC) fake news detection. Second, we observe that fake news\ncan be reliably detected at an early stage, after just a few hours of\npropagation. Third, we test the aging of our model on training and testing data\nseparated in time. Our results point to the promise of propagation-based\napproaches for fake news detection as an alternative or complementary strategy\nto content-based approaches.\n""]"
59,298,59_community_communities_community detection_clustering,"['community', 'communities', 'community detection', 'clustering', 'block model', 'stochastic block', 'graph', 'block', 'graphs', 'spectral']","['community detection', 'stochastic blockmodel', 'graph clustering', 'stochastic block', 'block models', 'spectral clustering', 'community structure', 'community structures', 'network analysis', 'block model']","['Community detection', '', '', '', '', '', '', '', '', '']","['Community detection', '', '', '', '', '', '', '', '', '']","['  The stochastic block model (SBM) is an important generative model for random\ngraphs in network science and machine learning, useful for benchmarking\ncommunity detection (or clustering) algorithms. The symmetric SBM generates a\ngraph with $2n$ nodes which cluster into two equally sized communities. Nodes\nconnect with probability $p$ within a community and $q$ across different\ncommunities. We consider the case of $p=a\\ln (n)/n$ and $q=b\\ln (n)/n$. In this\ncase, it was recently shown that recovering the community membership (or label)\nof every node with high probability (w.h.p.) using only the graph is possible\nif and only if the Chernoff-Hellinger (CH) divergence\n$D(a,b)=(\\sqrt{a}-\\sqrt{b})^2 \\geq 1$. In this work, we study if, and by how\nmuch, community detection below the clustering threshold (i.e. $D(a,b)<1$) is\npossible by querying the labels of a limited number of chosen nodes (i.e.,\nactive learning). Our main result is to show that, under certain conditions,\nsampling the labels of a vanishingly small fraction of nodes (a number\nsub-linear in $n$) is sufficient for exact community detection even when\n$D(a,b)<1$. Furthermore, we provide an efficient learning algorithm which\nrecovers the community memberships of all nodes w.h.p. as long as the number of\nsampled points meets the sufficient condition. We also show that recovery is\nnot possible if the number of observed labels is less than $n^{1-D(a,b)}$. The\nvalidity of our results is demonstrated through numerical experiments.\n', '  Community detection is the task of detecting hidden communities from observed\ninteractions. Guaranteed community detection has so far been mostly limited to\nmodels with non-overlapping communities such as the stochastic block model. In\nthis paper, we remove this restriction, and provide guaranteed community\ndetection for a family of probabilistic network models with overlapping\ncommunities, termed as the mixed membership Dirichlet model, first introduced\nby Airoldi et al. This model allows for nodes to have fractional memberships in\nmultiple communities and assumes that the community memberships are drawn from\na Dirichlet distribution. Moreover, it contains the stochastic block model as a\nspecial case. We propose a unified approach to learning these models via a\ntensor spectral decomposition method. Our estimator is based on low-order\nmoment tensor of the observed network, consisting of 3-star counts. Our\nlearning method is fast and is based on simple linear algebraic operations,\ne.g. singular value decomposition and tensor power iterations. We provide\nguaranteed recovery of community memberships and model parameters and present a\ncareful finite sample analysis of our learning method. As an important special\ncase, our results match the best known scaling requirements for the\n(homogeneous) stochastic block model.\n', '  Real-world networks usually have community structure, that is, nodes are\ngrouped into densely connected communities. Community detection is one of the\nmost popular and best-studied research topics in network science and has\nattracted attention in many different fields, including computer science,\nstatistics, social sciences, among others. Numerous approaches for community\ndetection have been proposed in literature, from ad-hoc algorithms to\nsystematic model-based approaches. The large number of available methods leads\nto a fundamental question: whether a certain method can provide consistent\nestimates of community labels. The stochastic blockmodel (SBM) and its variants\nprovide a convenient framework for the study of such problems. This article is\na survey on the recent theoretical advances of community detection. The authors\nreview a number of community detection methods and their theoretical\nproperties, including graph cut methods, profile likelihoods, the\npseudo-likelihood method, the variational method, belief propagation, spectral\nclustering, and semidefinite relaxations of the SBM. The authors also briefly\ndiscuss other research topics in community detection such as robust community\ndetection, community detection with nodal covariates and model selection, as\nwell as suggest a few possible directions for future research.\n']"
60,281,60_emotion_emotion recognition_recognition_emotions,"['emotion', 'emotion recognition', 'recognition', 'emotions', 'facial', 'emotional', 'speech', 'expression', 'valence', 'affective']","['emotion recognition', 'emotion classification', 'emotion detection', 'speech emotion', 'expression recognition', 'affective computing', 'facial expression', 'emotion', 'facial expressions', 'emotions']","['Emotion recognition', '', '', '', '', '', '', '', '', '']","['Emotion recognition', '', '', '', '', '', '', '', '', '']","[""  In this paper, a domain adaptation based technique for recognizing the\nemotions in images containing facial, non-facial, and non-human components has\nbeen proposed. We have also proposed a novel technique to explain the proposed\nsystem's predictions in terms of Intersection Score. Image emotion recognition\nis useful for graphics, gaming, animation, entertainment, and cinematography.\nHowever, well-labeled large scale datasets and pre-trained models are not\navailable for image emotion recognition. To overcome this challenge, we have\nproposed a deep learning approach based on an attentional convolutional network\nthat adapts pre-trained facial expression recognition models. It detects the\nvisual features of an image and performs emotion classification based on them.\nThe experiments have been performed on the Flickr image dataset, and the images\nhave been classified in 'angry,' 'happy,' 'sad,' and 'neutral' emotion classes.\nThe proposed system has demonstrated better performance than the benchmark\nresults with an accuracy of 63.87% for image emotion recognition. We have also\nanalyzed the embedding plots for various emotion classes to explain the\nproposed system's predictions.\n"", ""  Emotion analysis has been attracting researchers' attention. Most previous\nworks in the artificial intelligence field focus on recognizing emotion rather\nthan mining the reason why emotions are not or wrongly recognized. Correlation\namong emotions contributes to the failure of emotion recognition. In this\npaper, we try to fill the gap between emotion recognition and emotion\ncorrelation mining through natural language text from web news. Correlation\namong emotions, expressed as the confusion and evolution of emotion, is\nprimarily caused by human emotion cognitive bias. To mine emotion correlation\nfrom emotion recognition through text, three kinds of features and two deep\nneural network models are presented. The emotion confusion law is extracted\nthrough orthogonal basis. The emotion evolution law is evaluated from three\nperspectives, one-step shift, limited-step shifts, and shortest path transfer.\nThe method is validated using three datasets-the titles, the bodies, and the\ncomments of news articles, covering both objective and subjective texts in\nvarying lengths (long and short). The experimental results show that, in\nsubjective comments, emotions are easily mistaken as anger. Comments tend to\narouse emotion circulations of love-anger and sadness-anger. In objective news,\nit is easy to recognize text emotion as love and cause fear-joy circulation.\nThat means, journalists may try to attract attention using fear and joy words\nbut arouse the emotion love instead; After news release, netizens generate\nemotional comments to express their intense emotions, i.e., anger, sadness, and\nlove. These findings could provide insights for applications regarding\naffective interaction such as network public sentiment, social media\ncommunication, and human-computer interaction.\n"", '  Emotion recognition is a core research area at the intersection of artificial\nintelligence and human communication analysis. It is a significant technical\nchallenge since humans display their emotions through complex idiosyncratic\ncombinations of the language, visual and acoustic modalities. In contrast to\ntraditional multimodal fusion techniques, we approach emotion recognition from\nboth direct person-independent and relative person-dependent perspectives. The\ndirect person-independent perspective follows the conventional emotion\nrecognition approach which directly infers absolute emotion labels from\nobserved multimodal features. The relative person-dependent perspective\napproaches emotion recognition in a relative manner by comparing partial video\nsegments to determine if there was an increase or decrease in emotional\nintensity. Our proposed model integrates these direct and relative prediction\nperspectives by dividing the emotion recognition task into three easier\nsubtasks. The first subtask involves a multimodal local ranking of relative\nemotion intensities between two short segments of a video. The second subtask\nuses local rankings to infer global relative emotion ranks with a Bayesian\nranking algorithm. The third subtask incorporates both direct predictions from\nobserved multimodal behaviors and relative emotion ranks from local-global\nrankings for final emotion prediction. Our approach displays excellent\nperformance on an audio-visual emotion recognition benchmark and improves over\nother algorithms for multimodal fusion.\n']"
61,274,61_active_active learning_learning_al,"['active', 'active learning', 'learning', 'al', 'label', 'unlabeled', 'labeling', 'labeled', 'data', 'query']","['active learning', 'learning algorithms', 'learning algorithm', 'learning methods', 'training data', 'based active', 'uncertainty sampling', 'supervised', 'learning', 'datasets']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['active learning', '', '', '', '', '', '', '', '', '']","['  Active learning is able to reduce the amount of labelling effort by using a\nmachine learning model to query the user for specific inputs.\n  While there are many papers on new active learning techniques, these\ntechniques rarely satisfy the constraints of a real-world project. In this\npaper, we analyse the main drawbacks of current active learning techniques and\nwe present approaches to alleviate them. We do a systematic study on the\neffects of the most common issues of real-world datasets on the deep active\nlearning process: model convergence, annotation error, and dataset imbalance.\nWe derive two techniques that can speed up the active learning loop such as\npartial uncertainty sampling and larger query size. Finally, we present our\nopen-source Bayesian active learning library, BaaL.\n', '  Even though active learning forms an important pillar of machine learning,\ndeep learning tools are not prevalent within it. Deep learning poses several\ndifficulties when used in an active learning setting. First, active learning\n(AL) methods generally rely on being able to learn and update models from small\namounts of data. Recent advances in deep learning, on the other hand, are\nnotorious for their dependence on large amounts of data. Second, many AL\nacquisition functions rely on model uncertainty, yet deep learning methods\nrarely represent such model uncertainty. In this paper we combine recent\nadvances in Bayesian deep learning into the active learning framework in a\npractical way. We develop an active learning framework for high dimensional\ndata, a task which has been extremely challenging so far, with very sparse\nexisting literature. Taking advantage of specialised models such as Bayesian\nconvolutional neural networks, we demonstrate our active learning techniques\nwith image data, obtaining a significant improvement on existing active\nlearning approaches. We demonstrate this on both the MNIST dataset, as well as\nfor skin cancer diagnosis from lesion images (ISIC2016 task).\n', '  Supervised machine learning methods usually require a large set of labeled\nexamples for model training. However, in many real applications, there are\nplentiful unlabeled data but limited labeled data; and the acquisition of\nlabels is costly. Active learning (AL) reduces the labeling cost by iteratively\nselecting the most valuable data to query their labels from the annotator. This\narticle introduces a Python toobox ALiPy for active learning. ALiPy provides a\nmodule based implementation of active learning framework, which allows users to\nconveniently evaluate, compare and analyze the performance of active learning\nmethods. In the toolbox, multiple options are available for each component of\nthe learning framework, including data process, active selection, label query,\nresults visualization, etc. In addition to the implementations of more than 20\nstate-of-the-art active learning algorithms, ALiPy also supports users to\neasily configure and implement their own approaches under different active\nlearning settings, such as AL for multi-label data, AL with noisy annotators,\nAL with different costs and so on. The toolbox is well-documented and\nopen-source on Github, and can be easily installed through PyPI.\n']"
62,269,62_uncertainty_bayesian_bayesian neural_neural,"['uncertainty', 'bayesian', 'bayesian neural', 'neural', 'networks', 'neural networks', 'distribution', 'bnns', 'inference', 'posterior']","['bayesian deep', 'predictive uncertainty', 'bayesian neural', 'model uncertainty', 'predictive distribution', 'neural networks', 'deep learning', 'bayesian inference', 'uncertainty estimation', 'neural network']","['Bayesian neural networks', '', '', '', '', '', '', '', '', '']","['Bayesian neural networks', '', '', '', '', '', '', '', '', '']","['  Bayesian neural networks (BNNs) have recently regained a significant amount\nof attention in the deep learning community due to the development of scalable\napproximate Bayesian inference techniques. There are several advantages of\nusing Bayesian approach: Parameter and prediction uncertainty become easily\navailable, facilitating rigid statistical analysis. Furthermore, prior\nknowledge can be incorporated. However so far there have been no scalable\ntechniques capable of combining both model (structural) and parameter\nuncertainty. In this paper we introduce the concept of model uncertainty in\nBNNs and hence make inference in the joint space of models and parameters.\nMoreover, we suggest an adaptation of a scalable variational inference approach\nwith reparametrization of marginal inclusion probabilities to incorporate the\nmodel space constraints. Finally, we show that incorporating model uncertainty\nvia Bayesian model averaging and Bayesian model selection allows to drastically\nsparsify the structure of BNNs.\n', '  While deep neural networks have become the go-to approach in computer vision,\nthe vast majority of these models fail to properly capture the uncertainty\ninherent in their predictions. Estimating this predictive uncertainty can be\ncrucial, for example in automotive applications. In Bayesian deep learning,\npredictive uncertainty is commonly decomposed into the distinct types of\naleatoric and epistemic uncertainty. The former can be estimated by letting a\nneural network output the parameters of a certain probability distribution.\nEpistemic uncertainty estimation is a more challenging problem, and while\ndifferent scalable methods recently have emerged, no extensive comparison has\nbeen performed in a real-world setting. We therefore accept this task and\npropose a comprehensive evaluation framework for scalable epistemic uncertainty\nestimation methods in deep learning. Our proposed framework is specifically\ndesigned to test the robustness required in real-world computer vision\napplications. We also apply this framework to provide the first properly\nextensive and conclusive comparison of the two current state-of-the-art\nscalable methods: ensembling and MC-dropout. Our comparison demonstrates that\nensembling consistently provides more reliable and practically useful\nuncertainty estimates. Code is available at\nhttps://github.com/fregu856/evaluating_bdl.\n', '  Deep neural networks (NNs) are powerful black box predictors that have\nrecently achieved impressive performance on a wide spectrum of tasks.\nQuantifying predictive uncertainty in NNs is a challenging and yet unsolved\nproblem. Bayesian NNs, which learn a distribution over weights, are currently\nthe state-of-the-art for estimating predictive uncertainty; however these\nrequire significant modifications to the training procedure and are\ncomputationally expensive compared to standard (non-Bayesian) NNs. We propose\nan alternative to Bayesian NNs that is simple to implement, readily\nparallelizable, requires very little hyperparameter tuning, and yields high\nquality predictive uncertainty estimates. Through a series of experiments on\nclassification and regression benchmarks, we demonstrate that our method\nproduces well-calibrated uncertainty estimates which are as good or better than\napproximate Bayesian NNs. To assess robustness to dataset shift, we evaluate\nthe predictive uncertainty on test examples from known and unknown\ndistributions, and show that our method is able to express higher uncertainty\non out-of-distribution examples. We demonstrate the scalability of our method\nby evaluating predictive uncertainty estimates on ImageNet.\n']"
63,266,63_tensor_rank_tensors_decomposition,"['tensor', 'rank', 'tensors', 'decomposition', 'rank tensor', 'tensor completion', 'low rank', 'completion', 'tensor decomposition', 'low']","['tensor decompositions', 'tensor decomposition', 'tensor completion', 'tensor factorization', 'rank tensors', 'tensor rank', 'rank tensor', 'tensor data', 'tensor train', 'tensors']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['Tensor completion', '', '', '', '', '', '', '', '', '']","['  Tensor, also known as multi-dimensional array, arises from many applications\nin signal processing, manufacturing processes, healthcare, among others. As one\nof the most popular methods in tensor literature, Robust tensor principal\ncomponent analysis (RTPCA) is a very effective tool to extract the low rank and\nsparse components in tensors. In this paper, a new method to analyze RTPCA is\nproposed based on the recently developed tensor-tensor product and tensor\nsingular value decomposition (t-SVD). Specifically, it aims to solve a convex\noptimization problem whose objective function is a weighted combination of the\ntensor nuclear norm and the l1-norm. In most of literature of RTPCA, the exact\nrecovery is built on the tensor incoherence conditions and the assumption of a\nuniform model on the sparse support. Unlike this conventional way, in this\npaper, without any assumption of randomness, the exact recovery can be achieved\nin a completely deterministic fashion by characterizing the tensor\nrank-sparsity incoherence, which is an uncertainty principle between the\nlow-rank tensor spaces and the pattern of sparse tensor.\n', '  Recently, the \\textit{Tensor Nuclear Norm~(TNN)} regularization based on\nt-SVD has been widely used in various low tubal-rank tensor recovery tasks.\nHowever, these models usually require smooth change of data along the third\ndimension to ensure their low rank structures. In this paper, we propose a new\ndefinition of data dependent tensor rank named \\textit{tensor Q-rank} by a\nlearnable orthogonal matrix $\\mathbf{Q}$, and further introduce a unified data\ndependent low rank tensor recovery model. According to the low rank hypothesis,\nwe introduce two explainable selection method of $\\mathbf{Q}$, under which the\ndata tensor may have a more significant low tensor Q-rank structure than that\nof low tubal-rank structure. Specifically, maximizing the variance of singular\nvalue distribution leads to Variance Maximization Tensor Q-Nuclear\nnorm~(VMTQN), while minimizing the value of nuclear norm through manifold\noptimization leads to Manifold Optimization Tensor Q-Nuclear norm~(MOTQN).\nMoreover, we apply these two models to the low rank tensor completion problem,\nand then give an effective algorithm and briefly analyze why our method works\nbetter than TNN based methods in the case of complex data with low sampling\nrate. Finally, experimental results on real-world datasets demonstrate the\nsuperiority of our proposed model in the tensor completion problem with respect\nto other tensor rank regularization models.\n', '  Tensor completion is a challenging problem with various applications. Many\nrelated models based on the low-rank prior of the tensor have been proposed.\nHowever, the low-rank prior may not be enough to recover the original tensor\nfrom the observed incomplete tensor. In this paper, we prose a tensor\ncompletion method by exploiting both the low-rank and sparse prior of tensor.\nSpecifically, the tensor completion task can be formulated as a low-rank\nminimization problem with a sparse regularizer. The low-rank property is\ndepicted by the tensor truncated nuclear norm based on tensor singular value\ndecomposition (T-SVD) which is a better approximation of tensor tubal rank than\ntensor nuclear norm. While the sparse regularizer is imposed by a\n$\\ell_{1}$-norm in a discrete cosine transformation (DCT) domain, which can\nbetter employ the local sparse property of completed data. To solve the\noptimization problem, we employ an alternating direction method of multipliers\n(ADMM) in which we only need to solve several subproblems which have\nclosed-form solutions. Substantial experiments on real world images and videos\nshow that the proposed method has better performances than the existing\nstate-of-the-art methods.\n']"
64,265,64_topic_topics_topic models_lda,"['topic', 'topics', 'topic models', 'lda', 'topic modeling', 'document', 'documents', 'topic model', 'dirichlet', 'words']","['topic modeling', 'topic models', 'topic model', 'topic discovery', 'topic coherence', 'dirichlet allocation', 'latent topics', 'latent dirichlet', 'latent topic', 'probabilistic topic']","['topic model', '', '', '', '', '', '', '', '', '']","['topic model evaluation', '', '', '', '', '', '', '', '', '']","['  A popular approach to topic modeling involves extracting co-occurring n-grams\nof a corpus into semantic themes. The set of n-grams in a theme represents an\nunderlying topic, but most topic modeling approaches are not able to label\nthese sets of words with a single n-gram. Such labels are useful for topic\nidentification in summarization systems. This paper introduces a novel approach\nto labeling a group of n-grams comprising an individual topic. The approach\ntaken is to complement the existing topic distributions over words with a known\ndistribution based on a predefined set of topics. This is done by integrating\nexisting labeled knowledge sources representing known potential topics into the\nprobabilistic topic model. These knowledge sources are translated into a\ndistribution and used to set the hyperparameters of the Dirichlet generated\ndistribution over words. In the inference these modified distributions guide\nthe convergence of the latent topics to conform with the complementary\ndistributions. This approach ensures that the topic inference process is\nconsistent with existing knowledge. The label assignment from the complementary\nknowledge sources are then transferred to the latent topics of the corpus. The\nresults show both accurate label assignment to topics as well as improved topic\ngeneration than those obtained using various labeling approaches based off\nLatent Dirichlet allocation (LDA).\n', ""  Topic models are widely used unsupervised models capable of learning topics -\nweighted lists of words and documents - from large collections of text\ndocuments. When topic models are used for discovery of topics in text\ncollections, a question that arises naturally is how well the model-induced\ntopics correspond to topics of interest to the analyst. In this paper we\nrevisit and extend a so far neglected approach to topic model evaluation based\non measuring topic coverage - computationally matching model topics with a set\nof reference topics that models are expected to uncover. The approach is well\nsuited for analyzing models' performance in topic discovery and for large-scale\nanalysis of both topic models and measures of model quality. We propose new\nmeasures of coverage and evaluate, in a series of experiments, different types\nof topic models on two distinct text domains for which interest for topic\ndiscovery exists. The experiments include evaluation of model quality, analysis\nof coverage of distinct topic categories, and the analysis of the relationship\nbetween coverage and other methods of topic model evaluation. The paper\ncontributes a new supervised measure of coverage, and the first unsupervised\nmeasure of coverage. The supervised measure achieves topic matching accuracy\nclose to human agreement. The unsupervised measure correlates highly with the\nsupervised one (Spearman's $\\rho \\geq 0.95$). Other contributions include\ninsights into both topic models and different methods of model evaluation, and\nthe datasets and code for facilitating future research on topic coverage.\n"", ""  This paper proposes a new methodology to study sequential corpora by\nimplementing a two-stage algorithm that learns time-based topics with respect\nto a scale of document positions and introduces the concept of Topic Scaling\nwhich ranks learned topics within the same document scale. The first stage\nranks documents using Wordfish, a Poisson-based document scaling method, to\nestimate document positions that serve, in the second stage, as a dependent\nvariable to learn relevant topics via a supervised Latent Dirichlet Allocation.\nThis novelty brings two innovations in text mining as it explains document\npositions, whose scale is a latent variable, and ranks the inferred topics on\nthe document scale to match their occurrences within the corpus and track their\nevolution. Tested on the U.S. State Of The Union two-party addresses, this\ninductive approach reveals that each party dominates one end of the learned\nscale with interchangeable transitions that follow the parties' term of office.\nBesides a demonstrated high accuracy in predicting in-sample documents'\npositions from topic scores, this method reveals further hidden topics that\ndifferentiate similar documents by increasing the number of learned topics to\nunfold potential nested hierarchical topic structures. Compared to other\npopular topic models, Topic Scaling learns topics with respect to document\nsimilarities without specifying a time frequency to learn topic evolution, thus\ncapturing broader topic patterns than dynamic topic models and yielding more\ninterpretable outputs than a plain latent Dirichlet allocation.\n""]"
65,254,65_image_captioning_captions_visual,"['image', 'captioning', 'captions', 'visual', 'modal', 'image captioning', 'language', 'caption', 'text', 'cross modal']","['captioning models', 'image captioning', 'video captioning', 'captioning', 'vision language', 'multimodal', 'image caption', 'embeddings', 'embedding', 'caption']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['Image Captioning', '', '', '', '', '', '', '', '', '']","['  Visual captioning aims to generate textual descriptions given images or\nvideos. Traditionally, image captioning models are trained on human annotated\ndatasets such as Flickr30k and MS-COCO, which are limited in size and\ndiversity. This limitation hinders the generalization capabilities of these\nmodels while also rendering them liable to making mistakes. Language models\ncan, however, be trained on vast amounts of freely available unlabelled data\nand have recently emerged as successful language encoders and coherent text\ngenerators. Meanwhile, several unimodal and multimodal fusion techniques have\nbeen proven to work well for natural language generation and automatic speech\nrecognition. Building on these recent developments, and with the aim of\nimproving the quality of generated captions, the contribution of our work in\nthis paper is two-fold: First, we propose a generic multimodal model fusion\nframework for caption generation as well as emendation where we utilize\ndifferent fusion strategies to integrate a pretrained Auxiliary Language Model\n(AuxLM) within the traditional encoder-decoder visual captioning frameworks.\nNext, we employ the same fusion strategies to integrate a pretrained Masked\nLanguage Model (MLM), namely BERT, with a visual captioning model, viz. Show,\nAttend, and Tell, for emending both syntactic and semantic errors in captions.\nOur caption emendation experiments on three benchmark image captioning\ndatasets, viz. Flickr8k, Flickr30k, and MSCOCO, show improvements over the\nbaseline, indicating the usefulness of our proposed multimodal fusion\nstrategies. Further, we perform a preliminary qualitative analysis on the\nemended captions and identify error categories based on the type of\ncorrections.\n', '  Image captioning models have been able to generate grammatically correct and\nhuman understandable sentences. However most of the captions convey limited\ninformation as the model used is trained on datasets that do not caption all\npossible objects existing in everyday life. Due to this lack of prior\ninformation most of the captions are biased to only a few objects present in\nthe scene, hence limiting their usage in daily life. In this paper, we attempt\nto show the biased nature of the currently existing image captioning models and\npresent a new image captioning dataset, Egoshots, consisting of 978 real life\nimages with no captions. We further exploit the state of the art pre-trained\nimage captioning and object recognition networks to annotate our images and\nshow the limitations of existing works. Furthermore, in order to evaluate the\nquality of the generated captions, we propose a new image captioning metric,\nobject based Semantic Fidelity (SF). Existing image captioning metrics can\nevaluate a caption only in the presence of their corresponding annotations;\nhowever, SF allows evaluating captions generated for images without\nannotations, making it highly useful for real life generated captions.\n', '  While many BERT-based cross-modal pre-trained models produce excellent\nresults on downstream understanding tasks like image-text retrieval and VQA,\nthey cannot be applied to generation tasks directly. In this paper, we propose\nXGPT, a new method of Cross-modal Generative Pre-Training for Image Captioning\nthat is designed to pre-train text-to-image caption generators through three\nnovel generation tasks, including Image-conditioned Masked Language Modeling\n(IMLM), Image-conditioned Denoising Autoencoding (IDA), and Text-conditioned\nImage Feature Generation (TIFG). As a result, the pre-trained XGPT can be\nfine-tuned without any task-specific architecture modifications to create\nstate-of-the-art models for image captioning. Experiments show that XGPT\nobtains new state-of-the-art results on the benchmark datasets, including COCO\nCaptions and Flickr30k Captions. We also use XGPT to generate new image\ncaptions as data augmentation for the image retrieval task and achieve\nsignificant improvement on all recall metrics.\n']"
66,252,66_quantization_precision_bit_quantized,"['quantization', 'precision', 'bit', 'quantized', 'weights', 'floating', 'floating point', 'accuracy', 'low precision', 'bits']","['network quantization', 'precision quantization', 'training quantization', 'quantization methods', 'quantization scheme', 'quantization method', 'precision networks', 'quantization', 'quantizing', 'qnns']","['Quantization', '', '', '', '', '', '', '', '', '']","['Quantization of deep neural networks', '', '', '', '', '', '', '', '', '']","['  Recent work in network quantization produced state-of-the-art results using\nmixed precision quantization. An imperative requirement for many efficient edge\ndevice hardware implementations is that their quantizers are uniform and with\npower-of-two thresholds. In this work, we introduce the Hardware Friendly Mixed\nPrecision Quantization Block (HMQ) in order to meet this requirement. The HMQ\nis a mixed precision quantization block that repurposes the Gumbel-Softmax\nestimator into a smooth estimator of a pair of quantization parameters, namely,\nbit-width and threshold. HMQs use this to search over a finite space of\nquantization schemes. Empirically, we apply HMQs to quantize classification\nmodels trained on CIFAR10 and ImageNet. For ImageNet, we quantize four\ndifferent architectures and show that, in spite of the added restrictions to\nour quantization scheme, we achieve competitive and, in some cases,\nstate-of-the-art results.\n', '  Quantization enables efficient acceleration of deep neural networks by\nreducing model memory footprint and exploiting low-cost integer math hardware\nunits. Quantization maps floating-point weights and activations in a trained\nmodel to low-bitwidth integer values using scale factors. Excessive\nquantization, reducing precision too aggressively, results in accuracy\ndegradation. When scale factors are shared at a coarse granularity across many\ndimensions of each tensor, effective precision of individual elements within\nthe tensor are limited. To reduce quantization-related accuracy loss, we\npropose using a separate scale factor for each small vector of ($\\approx$16-64)\nelements within a single dimension of a tensor. To achieve an efficient\nhardware implementation, the per-vector scale factors can be implemented with\nlow-bitwidth integers when calibrated using a two-level quantization scheme. We\nfind that per-vector scaling consistently achieves better inference accuracy at\nlow precision compared to conventional scaling techniques for popular neural\nnetworks without requiring retraining. We also modify a deep learning\naccelerator hardware design to study the area and energy overheads of\nper-vector scaling support. Our evaluation demonstrates that per-vector scaled\nquantization with 4-bit weights and activations achieves 37% area saving and\n24% energy saving while maintaining over 75% accuracy for ResNet50 on ImageNet.\n4-bit weights and 8-bit activations achieve near-full-precision accuracy for\nboth BERT-base and BERT-large on SQuAD while reducing area by 26% compared to\nan 8-bit baseline.\n', '  We present an overview of techniques for quantizing convolutional neural\nnetworks for inference with integer weights and activations. Per-channel\nquantization of weights and per-layer quantization of activations to 8-bits of\nprecision post-training produces classification accuracies within 2% of\nfloating point networks for a wide variety of CNN architectures. Model sizes\ncan be reduced by a factor of 4 by quantizing weights to 8-bits, even when\n8-bit arithmetic is not supported. This can be achieved with simple, post\ntraining quantization of weights.We benchmark latencies of quantized networks\non CPUs and DSPs and observe a speedup of 2x-3x for quantized implementations\ncompared to floating point on CPUs. Speedups of up to 10x are observed on\nspecialized processors with fixed point SIMD capabilities, like the Qualcomm\nQDSPs with HVX.\n  Quantization-aware training can provide further improvements, reducing the\ngap to floating point to 1% at 8-bit precision. Quantization-aware training\nalso allows for reducing the precision of weights to four bits with accuracy\nlosses ranging from 2% to 10%, with higher accuracy drop for smaller\nnetworks.We introduce tools in TensorFlow and TensorFlowLite for quantizing\nconvolutional networks and review best practices for quantization-aware\ntraining to obtain high accuracy with quantized weights and activations. We\nrecommend that per-channel quantization of weights and per-layer quantization\nof activations be the preferred quantization scheme for hardware acceleration\nand kernel optimization. We also propose that future processors and hardware\naccelerators for optimized inference support precisions of 4, 8 and 16 bits.\n']"
67,251,67_word_embeddings_words_word embeddings,"['word', 'embeddings', 'words', 'word embeddings', 'embedding', 'semantic', 'word embedding', 'representations', 'similarity', 'vectors']","['word embedding', 'word embeddings', 'word representations', 'representations words', 'word representation', 'embedding models', 'embedding', 'embeddings', 'trained word', 'word vectors']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['Word embeddings', '', '', '', '', '', '', '', '', '']","['  Distributed language representation has become the most widely used technique\nfor language representation in various natural language processing tasks. Most\nof the natural language processing models that are based on deep learning\ntechniques use already pre-trained distributed word representations, commonly\ncalled word embeddings. Determining the most qualitative word embeddings is of\ncrucial importance for such models. However, selecting the appropriate word\nembeddings is a perplexing task since the projected embedding space is not\nintuitive to humans. In this paper, we explore different approaches for\ncreating distributed word representations. We perform an intrinsic evaluation\nof several state-of-the-art word embedding methods. Their performance on\ncapturing word similarities is analysed with existing benchmark datasets for\nword pairs similarities. The research in this paper conducts a correlation\nanalysis between ground truth word similarities and similarities obtained by\ndifferent word embedding methods.\n', '  Natural language processing models have attracted much interest in the deep\nlearning community. This branch of study is composed of some applications such\nas machine translation, sentiment analysis, named entity recognition, question\nand answer, and others. Word embeddings are continuous word representations,\nthey are an essential module for those applications and are generally used as\ninput word representation to the deep learning models. Word2Vec and GloVe are\ntwo popular methods to learn word embeddings. They achieve good word\nrepresentations, however, they learn representations with limited information\nbecause they ignore the morphological information of the words and consider\nonly one representation vector for each word. This approach implies that\nWord2Vec and GloVe are unaware of the word inner structure. To mitigate this\nproblem, the FastText model represents each word as a bag of characters\nn-grams. Hence, each n-gram has a continuous vector representation, and the\nfinal word representation is the sum of its characters n-grams vectors.\nNevertheless, the use of all n-grams character of a word is a poor approach\nsince some n-grams have no semantic relation with their words and increase the\namount of potentially useless information. This approach also increases the\ntraining phase time. In this work, we propose a new method for training word\nembeddings, and its goal is to replace the FastText bag of character n-grams\nfor a bag of word morphemes through the morphological analysis of the word.\nThus, words with similar context and morphemes are represented by vectors close\nto each other. To evaluate our new approach, we performed intrinsic evaluations\nconsidering 15 different tasks, and the results show a competitive performance\ncompared to FastText.\n', '  Continuous word representation (aka word embedding) is a basic building block\nin many neural network-based models used in natural language processing tasks.\nAlthough it is widely accepted that words with similar semantics should be\nclose to each other in the embedding space, we find that word embeddings\nlearned in several tasks are biased towards word frequency: the embeddings of\nhigh-frequency and low-frequency words lie in different subregions of the\nembedding space, and the embedding of a rare word and a popular word can be far\nfrom each other even if they are semantically similar. This makes learned word\nembeddings ineffective, especially for rare words, and consequently limits the\nperformance of these neural network models. In this paper, we develop a neat,\nsimple yet effective way to learn \\emph{FRequency-AGnostic word Embedding}\n(FRAGE) using adversarial training. We conducted comprehensive studies on ten\ndatasets across four natural language processing tasks, including word\nsimilarity, language modeling, machine translation and text classification.\nResults show that with FRAGE, we achieve higher performance than the baselines\nin all tasks.\n']"
68,239,68_question_questions_answer_answering,"['question', 'questions', 'answer', 'answering', 'qa', 'question answering', 'comprehension', 'reading', 'answers', 'reading comprehension']","['question generation', 'question answering', 'answering qa', 'qa datasets', 'answering dataset', 'language models', 'qa systems', 'natural language', 'conversational', 'commonsense knowledge']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['A list of open domain question answering tasks', '', '', '', '', '', '', '', '', '']","['  The ability to ask questions is important in both human and machine\nintelligence. Learning to ask questions helps knowledge acquisition, improves\nquestion-answering and machine reading comprehension tasks, and helps a chatbot\nto keep the conversation flowing with a human. Existing question generation\nmodels are ineffective at generating a large amount of high-quality\nquestion-answer pairs from unstructured text, since given an answer and an\ninput passage, question generation is inherently a one-to-many mapping. In this\npaper, we propose Answer-Clue-Style-aware Question Generation (ACS-QG), which\naims at automatically generating high-quality and diverse question-answer pairs\nfrom unlabeled text corpus at scale by imitating the way a human asks\nquestions. Our system consists of: i) an information extractor, which samples\nfrom the text multiple types of assistive information to guide question\ngeneration; ii) neural question generators, which generate diverse and\ncontrollable questions, leveraging the extracted assistive information; and\niii) a neural quality controller, which removes low-quality generated data\nbased on text entailment. We compare our question generation models with\nexisting approaches and resort to voluntary human evaluation to assess the\nquality of the generated question-answer pairs. The evaluation results suggest\nthat our system dramatically outperforms state-of-the-art neural question\ngeneration models in terms of the generation quality, while being scalable in\nthe meantime. With models trained on a relatively smaller amount of data, we\ncan generate 2.8 million quality-assured question-answer pairs from a million\nsentences found in Wikipedia.\n', '  In addition to the traditional task of getting machines to answer questions,\na major research question in question answering is to create interesting,\nchallenging questions that can help systems learn how to answer questions and\nalso reveal which systems are the best at answering questions. We argue that\ncreating a question answering dataset -- and the ubiquitous leaderboard that\ngoes with it -- closely resembles running a trivia tournament: you write\nquestions, have agents (either humans or machines) answer the questions, and\ndeclare a winner. However, the research community has ignored the decades of\nhard-learned lessons from decades of the trivia community creating vibrant,\nfair, and effective question answering competitions. After detailing problems\nwith existing QA datasets, we outline the key lessons -- removing ambiguity,\ndiscriminating skill, and adjudicating disputes -- that can transfer to QA\nresearch and how they might be implemented for the QA community.\n', '  This paper tackles the problem of open domain factual Arabic question\nanswering (QA) using Wikipedia as our knowledge source. This constrains the\nanswer of any question to be a span of text in Wikipedia. Open domain QA for\nArabic entails three challenges: annotated QA datasets in Arabic, large scale\nefficient information retrieval and machine reading comprehension. To deal with\nthe lack of Arabic QA datasets we present the Arabic Reading Comprehension\nDataset (ARCD) composed of 1,395 questions posed by crowdworkers on Wikipedia\narticles, and a machine translation of the Stanford Question Answering Dataset\n(Arabic-SQuAD). Our system for open domain question answering in Arabic (SOQAL)\nis based on two components: (1) a document retriever using a hierarchical\nTF-IDF approach and (2) a neural reading comprehension model using the\npre-trained bi-directional transformer BERT. Our experiments on ARCD indicate\nthe effectiveness of our approach with our BERT-based reader achieving a 61.3\nF1 score, and our open domain system SOQAL achieving a 27.6 F1 score.\n']"
69,233,69_convolutional_cnns_networks_cnn,"['convolutional', 'cnns', 'networks', 'cnn', 'layers', 'convolutional neural', 'layer', 'network', 'image', 'neural']","['networks cnns', 'cnn architectures', 'convolutional layers', 'convolutional neural', 'deep convolutional', 'convolutional networks', 'convolutional filters', 'convolution', 'cnns', 'convolutional']","['Diffusion neural networks', '', '', '', '', '', '', '', '', '']","['Deep Learning', '', '', '', '', '', '', '', '', '']","['  In this paper, we propose a fixed convolutional layer with an order of\nsmoothness not only for avoiding checkerboard artifacts in convolutional neural\nnetworks (CNNs) but also for enhancing the performance of CNNs, where the\nsmoothness of its filter kernel can be controlled by a parameter. It is\nwell-known that a number of CNNs generate checkerboard artifacts in both of two\nprocess: forward-propagation of upsampling layers and backward-propagation of\nstrided convolutional layers. The proposed layer can perfectly prevent\ncheckerboard artifacts caused by strided convolutional layers or upsampling\nlayers including transposed convolutional layers. In an image-classification\nexperiment with four CNNs: a simple CNN, VGG8, ResNet-18, and ResNet-101,\napplying the fixed layers to these CNNs is shown to improve the classification\nperformance of all CNNs. In addition, the fixed layer are applied to generative\nadversarial networks (GANs), for the first time. From image-generation results,\na smoother fixed convolutional layer is demonstrated to enable us to improve\nthe quality of images generated with GANs.\n', '  Deep convolutional neural networks have recently proven extremely competitive\nin challenging image recognition tasks. This paper proposes the epitomic\nconvolution as a new building block for deep neural networks. An epitomic\nconvolution layer replaces a pair of consecutive convolution and max-pooling\nlayers found in standard deep convolutional neural networks. The main version\nof the proposed model uses mini-epitomes in place of filters and computes\nresponses invariant to small translations by epitomic search instead of\nmax-pooling over image positions. The topographic version of the proposed model\nuses large epitomes to learn filter maps organized in translational\ntopographies. We show that error back-propagation can successfully learn\nmultiple epitomic layers in a supervised fashion. The effectiveness of the\nproposed method is assessed in image classification tasks on standard\nbenchmarks. Our experiments on Imagenet indicate improved recognition\nperformance compared to standard convolutional neural networks of similar\narchitecture. Our models pre-trained on Imagenet perform excellently on\nCaltech-101. We also obtain competitive image classification results on the\nsmall-image MNIST and CIFAR-10 datasets.\n', '  Building large models with parameter sharing accounts for most of the success\nof deep convolutional neural networks (CNNs). In this paper, we propose doubly\nconvolutional neural networks (DCNNs), which significantly improve the\nperformance of CNNs by further exploring this idea. In stead of allocating a\nset of convolutional filters that are independently learned, a DCNN maintains\ngroups of filters where filters within each group are translated versions of\neach other. Practically, a DCNN can be easily implemented by a two-step\nconvolution procedure, which is supported by most modern deep learning\nlibraries. We perform extensive experiments on three image classification\nbenchmarks: CIFAR-10, CIFAR-100 and ImageNet, and show that DCNNs consistently\noutperform other competing architectures. We have also verified that replacing\na convolutional layer with a doubly convolutional layer at any depth of a CNN\ncan improve its performance. Moreover, various design choices of DCNNs are\ndemonstrated, which shows that DCNN can serve the dual purpose of building more\naccurate models and/or reducing the memory footprint without sacrificing the\naccuracy.\n']"
70,230,70_translation_nmt_machine translation_neural machine,"['translation', 'nmt', 'machine translation', 'neural machine', 'english', 'bleu', 'languages', 'translation nmt', 'machine', 'language']","['translation models', 'translation model', 'translation systems', 'machine translation', 'translation nmt', 'translation task', 'monolingual data', 'translation tasks', 'translation', 'translation quality']","['machine translation', '', '', '', '', '', '', '', '', '']","['Neural Machine Translation', '', '', '', '', '', '', '', '', '']","['  Machine translation has many applications such as news translation, email\ntranslation, official letter translation etc. Commercial translators, e.g.\nGoogle Translation lags in regional vocabulary and are unable to learn the\nbilingual text in the source and target languages within the input. In this\npaper, a regional vocabulary-based application-oriented Neural Machine\nTranslation (NMT) model is proposed over the data set of emails used at the\nUniversity for communication over a period of three years. A state-of-the-art\nSequence-to-Sequence Neural Network for ML -> EN and EN -> ML translations is\ncompared with Google Translate using Gated Recurrent Unit Recurrent Neural\nNetwork machine translation model with attention decoder. The low BLEU score of\nGoogle Translation in comparison to our model indicates that the application\nbased regional models are better. The low BLEU score of EN -> ML of our model\nand Google Translation indicates that the Malay Language has complex language\nfeatures corresponding to English.\n', '  Many language pairs are low resource, meaning the amount and/or quality of\navailable parallel data is not sufficient to train a neural machine translation\n(NMT) model which can reach an acceptable standard of accuracy. Many works have\nexplored using the readily available monolingual data in either or both of the\nlanguages to improve the standard of translation models in low, and even high,\nresource languages. One of the most successful of such works is the\nback-translation that utilizes the translations of the target language\nmonolingual data to increase the amount of the training data. The quality of\nthe backward model which is trained on the available parallel data has been\nshown to determine the performance of the back-translation approach. Despite\nthis, only the forward model is improved on the monolingual target data in\nstandard back-translation. A previous study proposed an iterative\nback-translation approach for improving both models over several iterations.\nBut unlike in the traditional back-translation, it relied on both the target\nand source monolingual data. This work, therefore, proposes a novel approach\nthat enables both the backward and forward models to benefit from the\nmonolingual target data through a hybrid of self-learning and back-translation\nrespectively. Experimental results have shown the superiority of the proposed\napproach over the traditional back-translation method on English-German low\nresource neural machine translation. We also proposed an iterative\nself-learning approach that outperforms the iterative back-translation while\nalso relying only on the monolingual target data and require the training of\nless models.\n', '  An effective method to generate a large number of parallel sentences for\ntraining improved neural machine translation (NMT) systems is the use of the\nback-translations of the target-side monolingual data. The standard\nback-translation method has been shown to be unable to efficiently utilize the\navailable huge amount of existing monolingual data because of the inability of\ntranslation models to differentiate between the authentic and synthetic\nparallel data during training. Tagging, or using gates, has been used to enable\ntranslation models to distinguish between synthetic and authentic data,\nimproving standard back-translation and also enabling the use of iterative\nback-translation on language pairs that underperformed using standard\nback-translation. In this work, we approach back-translation as a domain\nadaptation problem, eliminating the need for explicit tagging. In the approach\n-- \\emph{tag-less back-translation} -- the synthetic and authentic parallel\ndata are treated as out-of-domain and in-domain data respectively and, through\npre-training and fine-tuning, the translation model is shown to be able to\nlearn more efficiently from them during training. Experimental results have\nshown that the approach outperforms the standard and tagged back-translation\napproaches on low resource English-Vietnamese and English-German neural machine\ntranslation.\n']"
71,222,71_manifold_manifold learning_manifolds_dimensional,"['manifold', 'manifold learning', 'manifolds', 'dimensional', 'data', 'dimensionality', 'isomap', 'dimension', 'embedding', 'reduction']","['manifold learning', 'data manifold', 'manifold structure', 'nonlinear dimensionality', 'manifolds', 'dimensionality reduction', 'manifold', 'dimensional manifold', 'intrinsic dimensionality', 'diffusion maps']","['Manifold learning', '', '', '', '', '', '', '', '', '']","['Manifold learning', '', '', '', '', '', '', '', '', '']","['  Modern sample points in many applications no longer comprise real vectors in\na real vector space but sample points of much more complex structures, which\nmay be represented as points in a space with a certain underlying geometric\nstructure, namely a manifold. Manifold learning is an emerging field for\nlearning the underlying structure. The study of manifold learning can be split\ninto two main branches: dimension reduction and manifold fitting. With the aim\nof combining statistics and geometry, we address the problem of manifold\nfitting in the ambient space. Inspired by the relation between the eigenvalues\nof the Laplace-Beltrami operator and the geometry of a manifold, we aim to find\na small set of points that preserve the geometry of the underlying manifold.\nFrom this relationship, we extend the idea of subsampling to sample points in\nhigh-dimensional space and employ the Moving Least Squares (MLS) approach to\napproximate the underlying manifold. We analyze the two core steps in our\nproposed method theoretically and also provide the bounds for the MLS approach.\nOur simulation results and theoretical analysis demonstrate the superiority of\nour method in estimating the underlying manifold.\n', '  Manifold learning is a hot research topic in the field of computer science\nand has many applications in the real world. A main drawback of manifold\nlearning methods is, however, that there is no explicit mappings from the input\ndata manifold to the output embedding. This prohibits the application of\nmanifold learning methods in many practical problems such as classification and\ntarget detection. Previously, in order to provide explicit mappings for\nmanifold learning methods, many methods have been proposed to get an\napproximate explicit representation mapping with the assumption that there\nexists a linear projection between the high-dimensional data samples and their\nlow-dimensional embedding. However, this linearity assumption may be too\nrestrictive. In this paper, an explicit nonlinear mapping is proposed for\nmanifold learning, based on the assumption that there exists a polynomial\nmapping between the high-dimensional data samples and their low-dimensional\nrepresentations. As far as we know, this is the first time that an explicit\nnonlinear mapping for manifold learning is given. In particular, we apply this\nto the method of Locally Linear Embedding (LLE) and derive an explicit\nnonlinear manifold learning algorithm, named Neighborhood Preserving Polynomial\nEmbedding (NPPE). Experimental results on both synthetic and real-world data\nshow that the proposed mapping is much more effective in preserving the local\nneighborhood information and the nonlinear geometry of the high-dimensional\ndata samples than previous work.\n', '  Traditional manifold learning algorithms assumed that the embedded manifold\nis globally or locally isometric to Euclidean space. Under this assumption,\nthey divided manifold into a set of overlapping local patches which are locally\nisometric to linear subsets of Euclidean space. By analyzing the global or\nlocal isometry assumptions it can be shown that the learnt manifold is a flat\nmanifold with zero Riemannian curvature tensor. In general, manifolds may not\nsatisfy these hypotheses. One major limitation of traditional manifold learning\nis that it does not consider the curvature information of manifold. In order to\nremove these limitations, we present our curvature-aware manifold learning\nalgorithm called CAML. The purpose of our algorithm is to break the local\nisometry assumption and to reduce the dimension of the general manifold which\nis not isometric to Euclidean space. Thus, our method adds the curvature\ninformation to the process of manifold learning. The experiments have shown\nthat our method CAML is more stable than other manifold learning algorithms by\ncomparing the neighborhood preserving ratios.\n']"
72,220,72_svm_support_support vector_svms,"['svm', 'support', 'support vector', 'svms', 'vector', 'vector machine', 'vector machines', 'machines', 'support vectors', 'classification']","['svm algorithm', 'svm models', 'svm model', 'svm training', 'svms', 'vector machines', 'vector machine', 'weighted svm', 'svm', 'machines svms']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['NESVM', '', '', '', '', '', '', '', '', '']","['  In this paper, we have considered general k-piece-wise linear convex loss\nfunctions in SVM model for measuring the empirical risk. The resulting\nk-Piece-wise Linear loss Support Vector Machine (k-PL-SVM) model is an adaptive\nSVM model which can learn a suitable piece-wise linear loss function according\nto nature of the given training set. The k-PL-SVM models are general SVM models\nand existing popular SVM models, like C-SVM, LS-SVM and Pin-SVM models, are\ntheir particular cases. We have performed the extensive numerical experiments\nwith k-PL-SVM models for k = 2 and 3 and shown that they are improvement over\nexisting SVM models.\n', '  The generalization error bound of support vector machine (SVM) depends on the\nratio of radius and margin, while standard SVM only considers the maximization\nof the margin but ignores the minimization of the radius. Several approaches\nhave been proposed to integrate radius and margin for joint learning of feature\ntransformation and SVM classifier. However, most of them either require the\nform of the transformation matrix to be diagonal, or are non-convex and\ncomputationally expensive. In this paper, we suggest a novel approximation for\nthe radius of minimum enclosing ball (MEB) in feature space, and then propose a\nconvex radius-margin based SVM model for joint learning of feature\ntransformation and SVM classifier, i.e., F-SVM. An alternating minimization\nmethod is adopted to solve the F-SVM model, where the feature transformation is\nupdatedvia gradient descent and the classifier is updated by employing the\nexisting SVM solver. By incorporating with kernel principal component analysis,\nF-SVM is further extended for joint learning of nonlinear transformation and\nclassifier. Experimental results on the UCI machine learning datasets and the\nLFW face datasets show that F-SVM outperforms the standard SVM and the existing\nradius-margin based SVMs, e.g., RMM, R-SVM+ and R-SVM+{\\mu}.\n', '  Support vector machines (SVMs) are invaluable tools for many practical\napplications in artificial intelligence, e.g., classification and event\nrecognition. However, popular SVM solvers are not sufficiently efficient for\napplications with a great deal of samples as well as a large number of\nfeatures. In this paper, thus, we present NESVM, a fast gradient SVM solver\nthat can optimize various SVM models, e.g., classical SVM, linear programming\nSVM and least square SVM. Compared against SVM-Perf\n\\cite{SVM_Perf}\\cite{PerfML} (its convergence rate in solving the dual SVM is\nupper bounded by $\\mathcal O(1/\\sqrt{k})$, wherein $k$ is the number of\niterations.) and Pegasos \\cite{Pegasos} (online SVM that converges at rate\n$\\mathcal O(1/k)$ for the primal SVM), NESVM achieves the optimal convergence\nrate at $\\mathcal O(1/k^{2})$ and a linear time complexity. In particular,\nNESVM smoothes the non-differentiable hinge loss and $\\ell_1$-norm in the\nprimal SVM. Then the optimal gradient method without any line search is adopted\nto solve the optimization. In each iteration round, the current gradient and\nhistorical gradients are combined to determine the descent direction, while the\nLipschitz constant determines the step size. Only two matrix-vector\nmultiplications are required in each iteration round. Therefore, NESVM is more\nefficient than existing SVM solvers. In addition, NESVM is available for both\nlinear and nonlinear kernels. We also propose ""homotopy NESVM"" to accelerate\nNESVM by dynamically decreasing the smooth parameter and using the continuation\nmethod. Our experiments on census income categorization, indoor/outdoor scene\nclassification, event recognition and scene recognition suggest the efficiency\nand the effectiveness of NESVM. The MATLAB code of NESVM will be available on\nour website for further assessment.\n']"
73,220,73_feature selection_feature_selection_features,"['feature selection', 'feature', 'selection', 'features', 'selection methods', 'data', 'methods', 'method', 'subset', 'proposed']","['feature selection', 'feature subsets', 'unsupervised feature', 'subset features', 'selection algorithms', 'feature subset', 'big data', 'data mining', 'feature extraction', 'unsupervised']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['feature selection', '', '', '', '', '', '', '', '', '']","[""  In machine learning and pattern recognition, feature selection has been a hot\ntopic in the literature. Unsupervised feature selection is challenging due to\nthe loss of labels which would supply the related information.How to define an\nappropriate metric is the key for feature selection. We propose a filter method\nfor unsupervised feature selection which is based on the Confidence Machine.\nConfidence Machine offers an estimation of confidence on a feature'reliability.\nIn this paper, we provide the math model of Confidence Machine in the context\nof feature selection, which maximizes the relevance and minimizes the\nredundancy of the selected feature. We compare our method against classic\nfeature selection methods Laplacian Score, Pearson Correlation and Principal\nComponent Analysis on benchmark data sets. The experimental results demonstrate\nthe efficiency and effectiveness of our method.\n"", '  Feature selection, as a data preprocessing strategy, has been proven to be\neffective and efficient in preparing data (especially high-dimensional data)\nfor various data mining and machine learning problems. The objectives of\nfeature selection include: building simpler and more comprehensible models,\nimproving data mining performance, and preparing clean, understandable data.\nThe recent proliferation of big data has presented some substantial challenges\nand opportunities to feature selection. In this survey, we provide a\ncomprehensive and structured overview of recent advances in feature selection\nresearch. Motivated by current challenges and opportunities in the era of big\ndata, we revisit feature selection research from a data perspective and review\nrepresentative feature selection algorithms for conventional data, structured\ndata, heterogeneous data and streaming data. Methodologically, to emphasize the\ndifferences and similarities of most existing feature selection algorithms for\nconventional data, we categorize them into four main groups: similarity based,\ninformation theoretical based, sparse learning based and statistical based\nmethods. To facilitate and promote the research in this community, we also\npresent an open-source feature selection repository that consists of most of\nthe popular feature selection algorithms\n(\\url{http://featureselection.asu.edu/}). Also, we use it as an example to show\nhow to evaluate feature selection algorithms. At the end of the survey, we\npresent a discussion about some open problems and challenges that require more\nattention in future research.\n', '  We are surrounded by huge amounts of large-scale high dimensional data. It is\ndesirable to reduce the dimensionality of data for many learning tasks due to\nthe curse of dimensionality. Feature selection has shown its effectiveness in\nmany applications by building simpler and more comprehensive model, improving\nlearning performance, and preparing clean, understandable data. Recently, some\nunique characteristics of big data such as data velocity and data variety\npresent challenges to the feature selection problem. In this paper, we envision\nthese challenges of feature selection for big data analytics. In particular, we\nfirst give a brief introduction about feature selection and then detail the\nchallenges of feature selection for structured, heterogeneous and streaming\ndata as well as its scalability and stability issues. At last, to facilitate\nand promote the feature selection research, we present an open-source feature\nselection repository (scikit-feature), which consists of most of current\npopular feature selection algorithms.\n']"
74,208,74_bert_language_fine_pre,"['bert', 'language', 'fine', 'pre', 'fine tuning', 'nlp', 'tuning', 'tasks', 'models', 'pre trained']","['models bert', 'bert based', 'language models', 'bert model', 'language processing', 'trained models', 'trained language', 'language inference', 'language model', 'processing nlp']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['Pre-trained language models', '', '', '', '', '', '', '', '', '']","['  Fine-tuning pre-trained contextualized embedding models has become an\nintegral part of the NLP pipeline. At the same time, probing has emerged as a\nway to investigate the linguistic knowledge captured by pre-trained models.\nVery little is, however, understood about how fine-tuning affects the\nrepresentations of pre-trained models and thereby the linguistic knowledge they\nencode. This paper contributes towards closing this gap. We study three\ndifferent pre-trained models: BERT, RoBERTa, and ALBERT, and investigate\nthrough sentence-level probing how fine-tuning affects their representations.\nWe find that for some probing tasks fine-tuning leads to substantial changes in\naccuracy, possibly suggesting that fine-tuning introduces or even removes\nlinguistic knowledge from a pre-trained model. These changes, however, vary\ngreatly across different models, fine-tuning and probing tasks. Our analysis\nreveals that while fine-tuning indeed changes the representations of a\npre-trained model and these changes are typically larger for higher layers,\nonly in very few cases, fine-tuning has a positive effect on probing accuracy\nthat is larger than just using the pre-trained model with a strong pooling\nmethod. Based on our findings, we argue that both positive and negative effects\nof fine-tuning on probing require a careful interpretation.\n', '  Fine-tuning pre-trained transformer-based language models such as BERT has\nbecome a common practice dominating leaderboards across various NLP benchmarks.\nDespite the strong empirical performance of fine-tuned models, fine-tuning is\nan unstable process: training the same model with multiple random seeds can\nresult in a large variance of the task performance. Previous literature (Devlin\net al., 2019; Lee et al., 2020; Dodge et al., 2020) identified two potential\nreasons for the observed instability: catastrophic forgetting and small size of\nthe fine-tuning datasets. In this paper, we show that both hypotheses fail to\nexplain the fine-tuning instability. We analyze BERT, RoBERTa, and ALBERT,\nfine-tuned on commonly used datasets from the GLUE benchmark, and show that the\nobserved instability is caused by optimization difficulties that lead to\nvanishing gradients. Additionally, we show that the remaining variance of the\ndownstream task performance can be attributed to differences in generalization\nwhere fine-tuned models with the same training loss exhibit noticeably\ndifferent test performance. Based on our analysis, we present a simple but\nstrong baseline that makes fine-tuning BERT-based models significantly more\nstable than the previously proposed approaches. Code to reproduce our results\nis available online: https://github.com/uds-lsv/bert-stable-fine-tuning.\n', ""  Pre-trained language models have been dominating the field of natural\nlanguage processing in recent years, and have led to significant performance\ngains for various complex natural language tasks. One of the most prominent\npre-trained language models is BERT, which was released as an English as well\nas a multilingual version. Although multilingual BERT performs well on many\ntasks, recent studies show that BERT models trained on a single language\nsignificantly outperform the multilingual version. Training a Dutch BERT model\nthus has a lot of potential for a wide range of Dutch NLP tasks. While previous\napproaches have used earlier implementations of BERT to train a Dutch version\nof BERT, we used RoBERTa, a robustly optimized BERT approach, to train a Dutch\nlanguage model called RobBERT. We measured its performance on various tasks as\nwell as the importance of the fine-tuning dataset size. We also evaluated the\nimportance of language-specific tokenizers and the model's fairness. We found\nthat RobBERT improves state-of-the-art results for various tasks, and\nespecially significantly outperforms other models when dealing with smaller\ndatasets. These results indicate that it is a powerful pre-trained model for a\nlarge variety of Dutch language tasks. The pre-trained and fine-tuned models\nare publicly available to support further downstream Dutch NLP applications.\n""]"
75,207,75_students_student_course_courses,"['students', 'student', 'course', 'courses', 'educational', 'knowledge', 'education', 'knowledge tracing', 'tracing', 'questions']","['student knowledge', 'academic performance', 'student performance', 'learning', 'learners', 'predictive', 'predicting', 'students performance', 'research', 'knowledge state']","['Knowledge Tracing in MOOCs', '', '', '', '', '', '', '', '', '']","['Science/Tech', '', '', '', '', '', '', '', '', '']","[""  The past decade has seen a growth in the development and deployment of\neducational technologies for assisting college-going students in choosing\nmajors, selecting courses and acquiring feedback based on past academic\nperformance. Grade prediction methods seek to estimate a grade that a student\nmay achieve in a course that she may take in the future (e.g., next term).\nAccurate and timely prediction of students' academic grades is important for\ndeveloping effective degree planners and early warning systems, and ultimately\nimproving educational outcomes. Existing grade pre- diction methods mostly\nfocus on modeling the knowledge components associated with each course and\nstudent, and often overlook other factors such as the difficulty of each\nknowledge component, course instructors, student interest, capabilities and\neffort. In this paper, we propose additive latent effect models that\nincorporate these factors to predict the student next-term grades.\nSpecifically, the proposed models take into account four factors: (i) student's\nacademic level, (ii) course instructors, (iii) student global latent factor,\nand (iv) latent knowledge factors. We compared the new models with several\nstate-of-the-art methods on students of various characteristics (e.g., whether\na student transferred in or not). The experimental results demonstrate that the\nproposed methods significantly outperform the baselines on grade prediction\nproblem. Moreover, we perform a thorough analysis on the importance of\ndifferent factors and how these factors can practically assist students in\ncourse selection, and finally improve their academic performance.\n"", ""  Digital technologies are becoming increasingly prevalent in education,\nenabling personalized, high quality education resources to be accessible by\nstudents across the world. Importantly, among these resources are diagnostic\nquestions: the answers that the students give to these questions reveal key\ninformation about the specific nature of misconceptions that the students may\nhold. Analyzing the massive quantities of data stemming from students'\ninteractions with these diagnostic questions can help us more accurately\nunderstand the students' learning status and thus allow us to automate learning\ncurriculum recommendations. In this competition, participants will focus on the\nstudents' answer records to these multiple-choice diagnostic questions, with\nthe aim of 1) accurately predicting which answers the students provide; 2)\naccurately predicting which questions have high quality; and 3) determining a\npersonalized sequence of questions for each student that best predicts the\nstudent's answers. These tasks closely mimic the goals of a real-world\neducational platform and are highly representative of the educational\nchallenges faced today. We provide over 20 million examples of students'\nanswers to mathematics questions from Eedi, a leading educational platform\nwhich thousands of students interact with daily around the globe. Participants\nto this competition have a chance to make a lasting, real-world impact on the\nquality of personalized education for millions of students across the world.\n"", ""  Massive open online courses are becoming a modish way for education, which\nprovides a large-scale and open-access learning opportunity for students to\ngrasp the knowledge. To attract students' interest, the recommendation system\nis applied by MOOCs providers to recommend courses to students. However, as a\ncourse usually consists of a number of video lectures, with each one covering\nsome specific knowledge concepts, directly recommending courses overlook\nstudents'interest to some specific knowledge concepts. To fill this gap, in\nthis paper, we study the problem of knowledge concept recommendation. We\npropose an end-to-end graph neural network-based approach\ncalledAttentionalHeterogeneous Graph Convolutional Deep Knowledge\nRecommender(ACKRec) for knowledge concept recommendation in MOOCs. Like other\nrecommendation problems, it suffers from sparsity issues. To address this\nissue, we leverage both content information and context information to learn\nthe representation of entities via graph convolution network. In addition to\nstudents and knowledge concepts, we consider other types of entities (e.g.,\ncourses, videos, teachers) and construct a heterogeneous information network to\ncapture the corresponding fruitful semantic relationships among different types\nof entities and incorporate them into the representation learning process.\nSpecifically, we use meta-path on the HIN to guide the propagation of students'\npreferences. With the help of these meta-paths, the students' preference\ndistribution with respect to a candidate knowledge concept can be captured.\nFurthermore, we propose an attention mechanism to adaptively fuse the context\ninformation from different meta-paths, in order to capture the different\ninterests of different students. The promising experiment results show that the\nproposedACKRecis able to effectively recommend knowledge concepts to students\npursuing online learning in MOOCs.\n""]"
76,198,76_ecg_heart_electrocardiogram_signals,"['ecg', 'heart', 'electrocardiogram', 'signals', 'arrhythmia', 'electrocardiogram ecg', 'signal', 'ecg signals', 'cardiac', 'ppg']","['electrocardiogram ecg', 'electrocardiogram', 'ecg recordings', 'ecg signals', 'ecg data', 'ecg signal', 'ecg', 'heartbeats', 'cardiac', 'atrial fibrillation']","['ECG', '', '', '', '', '', '', '', '', '']","['Electrocardiogram', '', '', '', '', '', '', '', '', '']","['  Heartbeat classification using electrocardiogram (ECG) data is a vital\nassistive technology for wearable health solutions. We propose heartbeat\nfeature classification based on a novel sparse representation using\ntime-frequency joint distribution of ECG. Fundamental to this is a multi-layer\nperceptron, which incorporates these signatures to detect cardiac arrhythmia.\nThis approach is validated with ECG data from MIT-BIH arrhythmia database.\nResults show that our approach has an average 95.7% accuracy, an improvement of\n22% over state-of-the-art approaches. Additionally, ECG sparse distributed\nrepresentations generates only 3.7% false negatives, reduction of 89% with\nrespect to existing ECG signal classification techniques.\n', '  Machine learning shows great performance in various problems of\nelectrocardiography (ECG) signal analysis. However, collecting a dataset for\nbiomedical engineering is a very difficult task. Any dataset for ECG processing\ncontains from 100 to 10,000 times fewer cases than datasets for image or text\nanalysis. This issue is especially important because of physiological phenomena\nthat can significantly change the morphology of heartbeats in ECG signals. In\nthis preliminary study, we analyze the effects of lead choice from the standard\nECG recordings, variation of ECG during 24-hours, and the effects of\nQT-prolongation agents on the performance of machine learning methods for ECG\nprocessing. We choose the problem of subject identification for analysis,\nbecause this problem may be solved for almost any available dataset of ECG\ndata. In a discussion, we compare our findings with observations from other\nworks that use machine learning for ECG processing with different problem\nstatements. Our results show the importance of training dataset enrichment with\nECG signals acquired in specific physiological conditions for obtaining good\nperformance of ECG processing for real applications.\n', '  Effective and powerful methods for denoising real electrocardiogram (ECG)\nsignals are important for wearable sensors and devices. Deep Learning (DL)\nmodels have been used extensively in image processing and other domains with\ngreat success but only very recently have been used in processing ECG signals.\nThis paper presents several DL models namely Convolutional Neural Networks\n(CNNs), Long Short-Term Memory (LSTM), Restricted Boltzmann Machine (RBM)\ntogether with the more conventional filtering methods (low pass filtering, high\npass filtering, Notch filtering) and the standard wavelet-based technique for\ndenoising EEG signals. These methods are trained, tested and evaluated on\ndifferent synthetic and real ECG datasets taken from the MIT PhysioNet database\nand for different simulation conditions (i.e. various lengths of the ECG\nsignals, single or multiple records). The results show the CNN model is a\nperformant model that can be used for off-line denoising ECG applications where\nit is satisfactory to train on a clean part of an ECG signal from an ECG\nrecord, and then to test on the same ECG signal, which would have some high\nlevel of noise added to it. However, for real-time applications or near-real\ntime applications, this task becomes more cumbersome, as the clean part of an\nECG signal is very probable to be very limited in size. Therefore the solution\nput forth in this work is to train a CNN model on 1 second ECG noisy artificial\nmultiple heartbeat data (i.e. ECG at effort), which was generated in a first\ninstance based on few sequences of real signal heartbeat ECG data (i.e. ECG at\nrest). Afterwards it would be possible to use the trained CNN model in real\nlife situations to denoise the ECG signal.\n']"
77,193,77_metric_metric learning_distance_similarity,"['metric', 'metric learning', 'distance', 'similarity', 'distance metric', 'learning', 'dml', 'triplet', 'data', 'deep metric']","['metric learning', 'deep metric', 'similarity learning', 'learning similarity', 'distance similarity', 'distance metric', 'similarity measure', 'local metric', 'metric space', 'learning algorithms']","['metric learning', '', '', '', '', '', '', '', '', '']","['Metric learning', '', '', '', '', '', '', '', '', '']","['  Distance metric learning has attracted much attention in recent years, where\nthe goal is to learn a distance metric based on user feedback. Conventional\napproaches to metric learning mainly focus on learning the Mahalanobis distance\nmetric on data attributes. Recent research on metric learning has been extended\nto sequential data, where we only have structural information in the sequences,\nbut no attribute is available. However, real-world applications often involve\nattributed sequence data (e.g., clickstreams), where each instance consists of\nnot only a set of attributes (e.g., user session context) but also a sequence\nof categorical items (e.g., user actions). In this paper, we study the problem\nof metric learning on attributed sequences. Unlike previous work on metric\nlearning, we now need to go beyond the Mahalanobis distance metric in the\nattribute feature space while also incorporating the structural information in\nsequences. We propose a deep learning framework, called MLAS (Metric Learning\non Attributed Sequences), to learn a distance metric that effectively measures\ndissimilarities between attributed sequences. Empirical results on real-world\ndatasets demonstrate that the proposed MLAS framework significantly improves\nthe performance of metric learning compared to state-of-the-art methods on\nattributed sequences.\n', '  Distance metric learning aims to learn from the given training data a valid\ndistance metric, with which the similarity between data samples can be more\neffectively evaluated for classification. Metric learning is often formulated\nas a convex or nonconvex optimization problem, while many existing metric\nlearning algorithms become inefficient for large scale problems. In this paper,\nwe formulate metric learning as a kernel classification problem, and solve it\nby iterated training of support vector machines (SVM). The new formulation is\neasy to implement, efficient in training, and tractable for large-scale\nproblems. Two novel metric learning models, namely Positive-semidefinite\nConstrained Metric Learning (PCML) and Nonnegative-coefficient Constrained\nMetric Learning (NCML), are developed. Both PCML and NCML can guarantee the\nglobal optimality of their solutions. Experimental results on UCI dataset\nclassification, handwritten digit recognition, face verification and person\nre-identification demonstrate that the proposed metric learning methods achieve\nhigher classification accuracy than state-of-the-art methods and they are\nsignificantly more efficient in training.\n', '  The need for appropriate ways to measure the distance or similarity between\ndata is ubiquitous in machine learning, pattern recognition and data mining,\nbut handcrafting such good metrics for specific problems is generally\ndifficult. This has led to the emergence of metric learning, which aims at\nautomatically learning a metric from data and has attracted a lot of interest\nin machine learning and related fields for the past ten years. This survey\npaper proposes a systematic review of the metric learning literature,\nhighlighting the pros and cons of each approach. We pay particular attention to\nMahalanobis distance metric learning, a well-studied and successful framework,\nbut additionally present a wide range of methods that have recently emerged as\npowerful alternatives, including nonlinear metric learning, similarity learning\nand local metric learning. Recent trends and extensions, such as\nsemi-supervised metric learning, metric learning for histogram data and the\nderivation of generalization guarantees, are also covered. Finally, this survey\naddresses metric learning for structured data, in particular edit distance\nlearning, and attempts to give an overview of the remaining challenges in\nmetric learning for the years to come.\n']"
78,191,78_distillation_teacher_student_knowledge distillation,"['distillation', 'teacher', 'student', 'knowledge distillation', 'knowledge', 'kd', 'student network', 'teacher student', 'teacher model', 'student model']","['knowledge distillation', 'knowledge teacher', 'knowledge transfer', 'transfer knowledge', 'transferring knowledge', 'teacher model', 'learning', 'neural networks', 'student models', 'teacher network']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['Knowledge distillation', '', '', '', '', '', '', '', '', '']","['  Knowledge distillation provides an effective way to transfer knowledge via\nteacher-student learning, where most existing distillation approaches apply a\nfixed pre-trained model as teacher to supervise the learning of student\nnetwork. This manner usually brings in a big capability gap between teacher and\nstudent networks during learning. Recent researches have observed that a small\nteacher-student capability gap can facilitate knowledge transfer. Inspired by\nthat, we propose an evolutionary knowledge distillation approach to improve the\ntransfer effectiveness of teacher knowledge. Instead of a fixed pre-trained\nteacher, an evolutionary teacher is learned online and consistently transfers\nintermediate knowledge to supervise student network learning on-the-fly. To\nenhance intermediate knowledge representation and mimicking, several simple\nguided modules are introduced between corresponding teacher-student blocks. In\nthis way, the student can simultaneously obtain rich internal knowledge and\ncapture its growth process, leading to effective student network learning.\nExtensive experiments clearly demonstrate the effectiveness of our approach as\nwell as good adaptability in the low-resolution and few-sample visual\nrecognition scenarios.\n', ""  Knowledge distillation is to transfer the knowledge from the data learned by\nthe teacher network to the student network, so that the student has the\nadvantage of less parameters and less calculations, and the accuracy is close\nto the teacher. In this paper, we propose a new distillation method, which\ncontains two transfer distillation strategies and a loss decay strategy. The\nfirst transfer strategy is based on channel-wise attention, called Channel\nDistillation (CD). CD transfers the channel information from the teacher to the\nstudent. The second is Guided Knowledge Distillation (GKD). Unlike Knowledge\nDistillation (KD), which allows the student to mimic each sample's prediction\ndistribution of the teacher, GKD only enables the student to mimic the correct\noutput of the teacher. The last part is Early Decay Teacher (EDT). During the\ntraining process, we gradually decay the weight of the distillation loss. The\npurpose is to enable the student to gradually control the optimization rather\nthan the teacher. Our proposed method is evaluated on ImageNet and CIFAR100. On\nImageNet, we achieve 27.68% of top-1 error with ResNet18, which outperforms\nstate-of-the-art methods. On CIFAR100, we achieve surprising result that the\nstudent outperforms the teacher. Code is available at\nhttps://github.com/zhouzaida/channel-distillation.\n"", '  The crux of knowledge distillation is to effectively train a resource-limited\nstudent model with the guide of a pre-trained larger teacher model. However,\nwhen there is a large difference between the model complexities of teacher and\nstudent (i.e., capacity gap), knowledge distillation loses its strength in\ntransferring knowledge from the teacher to the student, thus training a weaker\nstudent. To mitigate the impact of the capacity gap, we introduce knowledge\ndistillation via intermediate heads. By extending the intermediate layers of\nthe teacher (at various depths) with classifier heads, we cheaply acquire a\ncohort of heterogeneous pre-trained teachers. The intermediate classifier heads\ncan all together be efficiently learned while freezing the backbone of the\npre-trained teacher. The cohort of teachers (including the original teacher)\nco-teach the student simultaneously. Our experiments on various teacher-student\npairs and datasets have demonstrated that the proposed approach outperforms the\ncanonical knowledge distillation approach and its extensions.\n']"
79,188,79_submodular_submodular functions_functions_approximation,"['submodular', 'submodular functions', 'functions', 'approximation', 'maximization', 'submodular function', 'greedy', 'submodularity', 'submodular maximization', 'monotone']","['submodular minimization', 'submodular maximization', 'submodular optimization', 'submodularity', 'submodular functions', 'constrained submodular', 'submodular function', 'adaptive submodular', 'function minimization', 'monotone submodular']","['Continuous submodularity', '', '', '', '', '', '', '', '', '']","['Submodularity and optimization', '', '', '', '', '', '', '', '', '']","['  Submodular functions are relevant to machine learning for at least two\nreasons: (1) some problems may be expressed directly as the optimization of\nsubmodular functions and (2) the lovasz extension of submodular functions\nprovides a useful set of regularization functions for supervised and\nunsupervised learning. In this monograph, we present the theory of submodular\nfunctions from a convex analysis perspective, presenting tight links between\ncertain polyhedra, combinatorial optimization and convex optimization problems.\nIn particular, we show how submodular function minimization is equivalent to\nsolving a wide variety of convex optimization problems. This allows the\nderivation of new efficient algorithms for approximate and exact submodular\nfunction minimization with theoretical guarantees and good practical\nperformance. By listing many examples of submodular functions, we review\nvarious applications to machine learning, such as clustering, experimental\ndesign, sensor placement, graphical model structure learning or subset\nselection, as well as a family of structured sparsity-inducing norms that can\nbe derived and used from submodular functions.\n', '  Continuous submodular functions are a category of generally\nnon-convex/non-concave functions with a wide spectrum of applications. The\ncelebrated property of this class of functions - continuous submodularity -\nenables both exact minimization and approximate maximization in poly. time.\nContinuous submodularity is obtained by generalizing the notion of\nsubmodularity from discrete domains to continuous domains. It intuitively\ncaptures a repulsive effect amongst different dimensions of the defined\nmultivariate function.\n  In this paper, we systematically study continuous submodularity and a class\nof non-convex optimization problems: continuous submodular function\nmaximization. We start by a thorough characterization of the class of\ncontinuous submodular functions, and show that continuous submodularity is\nequivalent to a weak version of the diminishing returns (DR) property. Thus we\nalso derive a subclass of continuous submodular functions, termed continuous\nDR-submodular functions, which enjoys the full DR property. Then we present\noperations that preserve continuous (DR-)submodularity, thus yielding general\nrules for composing new submodular functions. We establish intriguing\nproperties for the problem of constrained DR-submodular maximization, such as\nthe local-global relation. We identify several applications of continuous\nsubmodular optimization, ranging from influence maximization, MAP inference for\nDPPs to provable mean field inference. For these applications, continuous\nsubmodularity formalizes valuable domain knowledge relevant for optimizing this\nclass of objectives. We present inapproximability results and provable\nalgorithms for two problem settings: constrained monotone DR-submodular\nmaximization and constrained non-monotone DR-submodular maximization. Finally,\nwe extensively evaluate the effectiveness of the proposed algorithms.\n', '  Submodular functions are a broad class of set functions, which naturally\narise in diverse areas. Many algorithms have been suggested for the\nmaximization of these functions. Unfortunately, once the function deviates from\nsubmodularity, the known algorithms may perform arbitrarily poorly. Amending\nthis issue, by obtaining approximation results for set functions generalizing\nsubmodular functions, has been the focus of recent works.\n  One such class, known as weakly submodular functions, has received a lot of\nattention. A key result proved by Das and Kempe (2011) showed that the\napproximation ratio of the greedy algorithm for weakly submodular maximization\nsubject to a cardinality constraint degrades smoothly with the distance from\nsubmodularity. However, no results have been obtained for maximization subject\nto constraints beyond cardinality. In particular, it is not known whether the\ngreedy algorithm achieves any non-trivial approximation ratio for such\nconstraints.\n  In this paper, we prove that a randomized version of the greedy algorithm\n(previously used by Buchbinder et al. (2014) for a different problem) achieves\nan approximation ratio of $(1 + 1/\\gamma)^{-2}$ for the maximization of a\nweakly submodular function subject to a general matroid constraint, where\n$\\gamma$ is a parameter measuring the distance of the function from\nsubmodularity. Moreover, we also experimentally compare the performance of this\nversion of the greedy algorithm on real world problems against natural\nbenchmarks, and show that the algorithm we study performs well also in\npractice. To the best of our knowledge, this is the first algorithm with a\nnon-trivial approximation guarantee for maximizing a weakly submodular function\nsubject to a constraint other than the simple cardinality constraint. In\nparticular, it is the first algorithm with such a guarantee for the important\nand broad class of matroid constraints.\n']"
80,188,80_belief propagation_belief_graphical_bp,"['belief propagation', 'belief', 'graphical', 'bp', 'propagation', 'inference', 'bethe', 'graphical models', 'ising', 'markov random']","['belief propagation', 'approximate inference', 'loopy belief', 'map inference', 'propagation bp', 'bethe approximation', 'inference', 'approximations', 'graphical models', 'approximation']","['Belief propagation', '', '', '', '', '', '', '', '', '']","['Belief propagation', '', '', '', '', '', '', '', '', '']","['  We present a novel inference algorithm for arbitrary, binary, undirected\ngraphs. Unlike loopy belief propagation, which iterates fixed point equations,\nwe directly descend on the Bethe free energy. The algorithm consists of two\nphases, first we update the pairwise probabilities, given the marginal\nprobabilities at each unit,using an analytic expression. Next, we update the\nmarginal probabilities, given the pairwise probabilities by following the\nnegative gradient of the Bethe free energy. Both steps are guaranteed to\ndecrease the Bethe free energy, and since it is lower bounded, the algorithm is\nguaranteed to converge to a local minimum. We also show that the Bethe free\nenergy is equal to the TAP free energy up to second order in the weights. In\nexperiments we confirm that when belief propagation converges it usually finds\nidentical solutions as our belief optimization method. However, in cases where\nbelief propagation fails to converge, belief optimization continues to converge\nto reasonable beliefs. The stable nature of belief optimization makes it\nideally suited for learning graphical models from data.\n', ""  Loopy belief propagation performs approximate inference on graphical models\nwith loops. One might hope to compensate for the approximation by adjusting\nmodel parameters. Learning algorithms for this purpose have been explored\npreviously, and the claim has been made that every set of locally consistent\nmarginals can arise from belief propagation run on a graphical model. On the\ncontrary, here we show that many probability distributions have marginals that\ncannot be reached by belief propagation using any set of model parameters or\nany learning algorithm. We call such marginals `unbelievable.' This problem\noccurs whenever the Hessian of the Bethe free energy is not positive-definite\nat the target marginals. All learning algorithms for belief propagation\nnecessarily fail in these cases, producing beliefs or sets of beliefs that may\neven be worse than the pre-learning approximation. We then show that averaging\ninaccurate beliefs, each obtained from belief propagation using model\nparameters perturbed about some learned mean values, can achieve the\nunbelievable marginals.\n"", '  Belief propagation is a fundamental message-passing algorithm for\nprobabilistic reasoning and inference in graphical models. While it is known to\nbe exact on trees, in most applications belief propagation is run on graphs\nwith cycles. Understanding the behavior of ""loopy"" belief propagation has been\na major challenge for researchers in machine learning, and positive convergence\nresults for BP are known under strong assumptions which imply the underlying\ngraphical model exhibits decay of correlations. We show that under a natural\ninitialization, BP converges quickly to the global optimum of the Bethe free\nenergy for Ising models on arbitrary graphs, as long as the Ising model is\n\\emph{ferromagnetic} (i.e. neighbors prefer to be aligned). This holds even\nthough such models can exhibit long range correlations and may have multiple\nsuboptimal BP fixed points. We also show an analogous result for iterating the\n(naive) mean-field equations; perhaps surprisingly, both results are\ndimension-free in the sense that a constant number of iterations already\nprovides a good estimate to the Bethe/mean-field free energy.\n']"
81,181,81_ranking_pairwise_items_rank,"['ranking', 'pairwise', 'items', 'rank', 'comparisons', 'preference', 'pairwise comparisons', 'rankings', 'luce', 'preferences']","['bipartite ranking', 'ranking algorithms', 'rank aggregation', 'pairwise comparisons', 'ranking data', 'pairwise comparison', 'learning rank', 'partial ranking', 'ranking', 'problem ranking']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['A method for ranking objects based on pairwise comparisons', '', '', '', '', '', '', '', '', '']","['  This paper studies the problem of finding the exact ranking from noisy\ncomparisons. A comparison over a set of $m$ items produces a noisy outcome\nabout the most preferred item, and reveals some information about the ranking.\nBy repeatedly and adaptively choosing items to compare, we want to fully rank\nthe items with a certain confidence, and use as few comparisons as possible.\nDifferent from most previous works, in this paper, we have three main\nnovelties: (i) compared to prior works, our upper bounds (algorithms) and lower\nbounds on the sample complexity (aka number of comparisons) require the minimal\nassumptions on the instances, and are not restricted to specific models; (ii)\nwe give lower bounds and upper bounds on instances with unequal noise levels;\nand (iii) this paper aims at the exact ranking without knowledge on the\ninstances, while most of the previous works either focus on approximate\nrankings or study exact ranking but require prior knowledge. We first derive\nlower bounds for pairwise ranking (i.e., compare two items each time), and then\npropose (nearly) optimal pairwise ranking algorithms. We further make\nextensions to listwise ranking (i.e., comparing multiple items each time).\nNumerical results also show our improvements against the state of the art.\n', '  We consider sequential or active ranking of a set of n items based on noisy\npairwise comparisons. Items are ranked according to the probability that a\ngiven item beats a randomly chosen item, and ranking refers to partitioning the\nitems into sets of pre-specified sizes according to their scores. This notion\nof ranking includes as special cases the identification of the top-k items and\nthe total ordering of the items. We first analyze a sequential ranking\nalgorithm that counts the number of comparisons won, and uses these counts to\ndecide whether to stop, or to compare another pair of items, chosen based on\nconfidence intervals specified by the data collected up to that point. We prove\nthat this algorithm succeeds in recovering the ranking using a number of\ncomparisons that is optimal up to logarithmic factors. This guarantee does not\nrequire any structural properties of the underlying pairwise probability\nmatrix, unlike a significant body of past work on pairwise ranking based on\nparametric models such as the Thurstone or Bradley-Terry-Luce models. It has\nbeen a long-standing open question as to whether or not imposing these\nparametric assumptions allows for improved ranking algorithms. For stochastic\ncomparison models, in which the pairwise probabilities are bounded away from\nzero, our second contribution is to resolve this issue by proving a lower bound\nfor parametric models. This shows, perhaps surprisingly, that these popular\nparametric modeling choices offer at most logarithmic gains for stochastic\ncomparisons.\n', '  We describe a seriation algorithm for ranking a set of items given pairwise\ncomparisons between these items. Intuitively, the algorithm assigns similar\nrankings to items that compare similarly with all others. It does so by\nconstructing a similarity matrix from pairwise comparisons, using seriation\nmethods to reorder this matrix and construct a ranking. We first show that this\nspectral seriation algorithm recovers the true ranking when all pairwise\ncomparisons are observed and consistent with a total order. We then show that\nranking reconstruction is still exact when some pairwise comparisons are\ncorrupted or missing, and that seriation based spectral ranking is more robust\nto noise than classical scoring methods. Finally, we bound the ranking error\nwhen only a random subset of the comparions are observed. An additional benefit\nof the seriation formulation is that it allows us to solve semi-supervised\nranking problems. Experiments on both synthetic and real datasets demonstrate\nthat seriation based spectral ranking achieves competitive and in some cases\nsuperior performance compared to classical ranking methods.\n']"
82,178,82_bayesian_bayesian networks_bayesian network_structure,"['bayesian', 'bayesian networks', 'bayesian network', 'structure', 'structure learning', 'variables', 'network', 'networks', 'learning bayesian', 'structures']","['bayesian networks', 'bayesian network', 'structure bayesian', 'structure learning', 'learning structure', 'markov networks', 'learning bayesian', 'markov network', 'dynamic bayesian', 'network structures']","['Bayesian network structure learning', '', '', '', '', '', '', '', '', '']","['Bayesian network structure learning', '', '', '', '', '', '', '', '', '']","['  We give a new consistent scoring function for structure learning of Bayesian\nnetworks. In contrast to traditional approaches to scorebased structure\nlearning, such as BDeu or MDL, the complexity penalty that we propose is\ndata-dependent and is given by the probability that a conditional independence\ntest correctly shows that an edge cannot exist. What really distinguishes this\nnew scoring function from earlier work is that it has the property of becoming\ncomputationally easier to maximize as the amount of data increases. We prove a\npolynomial sample complexity result, showing that maximizing this score is\nguaranteed to correctly learn a structure with no false edges and a\ndistribution close to the generating distribution, whenever there exists a\nBayesian network which is a perfect map for the data generating distribution.\nAlthough the new score can be used with any search algorithm, we give empirical\nresults showing that it is particularly effective when used together with a\nlinear programming relaxation approach to Bayesian network structure learning.\n', '  Learning Bayesian networks from raw data can help provide insights into the\nrelationships between variables. While real data often contains a mixture of\ndiscrete and continuous-valued variables, many Bayesian network structure\nlearning algorithms assume all random variables are discrete. Thus, continuous\nvariables are often discretized when learning a Bayesian network. However, the\nchoice of discretization policy has significant impact on the accuracy, speed,\nand interpretability of the resulting models. This paper introduces a\nprincipled Bayesian discretization method for continuous variables in Bayesian\nnetworks with quadratic complexity instead of the cubic complexity of other\nstandard techniques. Empirical demonstrations show that the proposed method is\nsuperior to the established minimum description length algorithm. In addition,\nthis paper shows how to incorporate existing methods into the structure\nlearning process to discretize all continuous variables and simultaneously\nlearn Bayesian network structures.\n', '  Bayesian network structure learning is the notoriously difficult problem of\ndiscovering a Bayesian network that optimally represents a given set of\ntraining data. In this paper we study the computational worst-case complexity\nof exact Bayesian network structure learning under graph theoretic restrictions\non the (directed) super-structure. The super-structure is an undirected graph\nthat contains as subgraphs the skeletons of solution networks. We introduce the\ndirected super-structure as a natural generalization of its undirected\ncounterpart. Our results apply to several variants of score-based Bayesian\nnetwork structure learning where the score of a network decomposes into local\nscores of its nodes. Results: We show that exact Bayesian network structure\nlearning can be carried out in non-uniform polynomial time if the\nsuper-structure has bounded treewidth, and in linear time if in addition the\nsuper-structure has bounded maximum degree. Furthermore, we show that if the\ndirected super-structure is acyclic, then exact Bayesian network structure\nlearning can be carried out in quadratic time. We complement these positive\nresults with a number of hardness results. We show that both restrictions\n(treewidth and degree) are essential and cannot be dropped without loosing\nuniform polynomial time tractability (subject to a complexity-theoretic\nassumption). Similarly, exact Bayesian network structure learning remains\nNP-hard for ""almost acyclic"" directed super-structures. Furthermore, we show\nthat the restrictions remain essential if we do not search for a globally\noptimal network but aim to improve a given network by means of at most k arc\nadditions, arc deletions, or arc reversals (k-neighborhood local search).\n']"
83,174,83_automl_ml_machine learning_machine,"['automl', 'ml', 'machine learning', 'machine', 'pipelines', 'pipeline', 'learning', 'data', 'automated', 'automated machine']","['learning automl', 'learning ml', 'automl systems', 'machine learning', 'ml models', 'auto sklearn', 'sklearn', 'automl frameworks', 'automl', 'learning pipeline']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['Automated Machine Learning', '', '', '', '', '', '', '', '', '']","['  There are currently many barriers that prevent non-experts from exploiting\nmachine learning solutions ranging from the lack of intuition on statistical\nlearning techniques to the trickiness of hyperparameter tuning. Such barriers\nhave led to an explosion of interest in automated machine learning (AutoML),\nwhereby an off-the-shelf system can take care of many of the steps for\nend-users without the need for expertise in machine learning. This paper\npresents Ensemble Squared (Ensemble$^2$), an AutoML system that ensembles the\nresults of state-of-the-art open-source AutoML systems. Ensemble$^2$ exploits\nthe diversity of existing AutoML systems by leveraging the differences in their\nmodel search space and heuristics. Empirically, we show that diversity of each\nAutoML system is sufficient to justify ensembling at the AutoML system level.\nIn demonstrating this, we also establish new state-of-the-art AutoML results on\nthe OpenML tabular classification benchmark.\n', ""  As big data becomes ubiquitous across domains, and more and more stakeholders\naspire to make the most of their data, demand for machine learning tools has\nspurred researchers to explore the possibilities of automated machine learning\n(AutoML). AutoML tools aim to make machine learning accessible for non-machine\nlearning experts (domain experts), to improve the efficiency of machine\nlearning, and to accelerate machine learning research. But although automation\nand efficiency are among AutoML's main selling points, the process still\nrequires human involvement at a number of vital steps, including understanding\nthe attributes of domain-specific data, defining prediction problems, creating\na suitable training data set, and selecting a promising machine learning\ntechnique. These steps often require a prolonged back-and-forth that makes this\nprocess inefficient for domain experts and data scientists alike, and keeps\nso-called AutoML systems from being truly automatic. In this review article, we\nintroduce a new classification system for AutoML systems, using a seven-tiered\nschematic to distinguish these systems based on their level of autonomy. We\nbegin by describing what an end-to-end machine learning pipeline actually looks\nlike, and which subtasks of the machine learning pipeline have been automated\nso far. We highlight those subtasks which are still done manually - generally\nby a data scientist - and explain how this limits domain experts' access to\nmachine learning. Next, we introduce our novel level-based taxonomy for AutoML\nsystems and define each level according to the scope of automation support\nprovided. Finally, we lay out a roadmap for the future, pinpointing the\nresearch required to further automate the end-to-end machine learning pipeline\nand discussing important challenges that stand in the way of this ambitious\ngoal.\n"", '  Machine learning (ML) has become a vital part in many aspects of our daily\nlife. However, building well performing machine learning applications requires\nhighly specialized data scientists and domain experts. Automated machine\nlearning (AutoML) aims to reduce the demand for data scientists by enabling\ndomain experts to build machine learning applications automatically without\nextensive knowledge of statistics and machine learning. This paper is a\ncombination of a survey on current AutoML methods and a benchmark of popular\nAutoML frameworks on real data sets. Driven by the selected frameworks for\nevaluation, we summarize and review important AutoML techniques and methods\nconcerning every step in building an ML pipeline. The selected AutoML\nframeworks are evaluated on 137 data sets from established AutoML benchmark\nsuits.\n']"
84,171,84_hashing_hash_codes_retrieval,"['hashing', 'hash', 'codes', 'retrieval', 'search', 'lsh', 'binary', 'hash codes', 'similarity', 'hash functions']","['deep hashing', 'supervised hashing', 'learning hash', 'semantic hashing', 'hashing', 'minwise hashing', 'sensitive hashing', 'hashing lsh', 'modal hashing', 'similarity search']","['A adversarial hashing algorithm', '', '', '', '', '', '', '', '', '']","['A novel adversarial hashing algorithm', '', '', '', '', '', '', '', '', '']","['  With the growth of image on the web, research on hashing which enables\nhigh-speed image retrieval has been actively studied. In recent years, various\nhashing methods based on deep neural networks have been proposed and achieved\nhigher precision than the other hashing methods. In these methods, multiple\nlosses for hash codes and the parameters of neural networks are defined. They\ngenerate hash codes that minimize the weighted sum of the losses. Therefore, an\nexpert has to tune the weights for the losses heuristically, and the\nprobabilistic optimality of the loss function cannot be explained. In order to\ngenerate explainable hash codes without weight tuning, we theoretically derive\na single loss function with no hyperparameters for the hash code from the\nprobability distribution of the images. By generating hash codes that minimize\nthis loss function, highly accurate image retrieval with probabilistic\noptimality is performed. We evaluate the performance of hashing using MNIST,\nCIFAR-10, SVHN and show that the proposed method outperforms the\nstate-of-the-art hashing methods.\n', '  Image hashing is one of the fundamental problems that demand both efficient\nand effective solutions for various practical scenarios. Adversarial\nautoencoders are shown to be able to implicitly learn a robust,\nlocality-preserving hash function that generates balanced and high-quality hash\ncodes. However, the existing adversarial hashing methods are inefficient to be\nemployed for large-scale image retrieval applications. Specifically, they\nrequire an exponential number of samples to be able to generate optimal hash\ncodes and a significantly high computational cost to train. In this paper, we\nshow that the high sample-complexity requirement often results in sub-optimal\nretrieval performance of the adversarial hashing methods. To address this\nchallenge, we propose a new adversarial-autoencoder hashing approach that has a\nmuch lower sample requirement and computational cost. Specifically, by\nexploiting the desired properties of the hash function in the low-dimensional,\ndiscrete space, our method efficiently estimates a better variant of\nWasserstein distance by averaging a set of easy-to-compute one-dimensional\nWasserstein distances. The resulting hashing approach has an order-of-magnitude\nbetter sample complexity, thus better generalization property, compared to the\nother adversarial hashing methods. In addition, the computational cost is\nsignificantly reduced using our approach. We conduct experiments on several\nreal-world datasets and show that the proposed method outperforms the competing\nhashing methods, achieving up to 10% improvement over the current\nstate-of-the-art image hashing methods. The code accompanying this paper is\navailable on Github (https://github.com/khoadoan/adversarial-hashing).\n', '  Semantic hashing represents documents as compact binary vectors (hash codes)\nand allows both efficient and effective similarity search in large-scale\ninformation retrieval. The state of the art has primarily focused on learning\nhash codes that improve similarity search effectiveness, while assuming a\nbrute-force linear scan strategy for searching over all the hash codes, even\nthough much faster alternatives exist. One such alternative is multi-index\nhashing, an approach that constructs a smaller candidate set to search over,\nwhich depending on the distribution of the hash codes can lead to sub-linear\nsearch time. In this work, we propose Multi-Index Semantic Hashing (MISH), an\nunsupervised hashing model that learns hash codes that are both effective and\nhighly efficient by being optimized for multi-index hashing. We derive novel\ntraining objectives, which enable to learn hash codes that reduce the candidate\nsets produced by multi-index hashing, while being end-to-end trainable. In\nfact, our proposed training objectives are model agnostic, i.e., not tied to\nhow the hash codes are generated specifically in MISH, and are straight-forward\nto include in existing and future semantic hashing models. We experimentally\ncompare MISH to state-of-the-art semantic hashing baselines in the task of\ndocument similarity search. We find that even though multi-index hashing also\nimproves the efficiency of the baselines compared to a linear scan, they are\nstill upwards of 33% slower than MISH, while MISH is still able to obtain\nstate-of-the-art effectiveness.\n']"
85,170,85_fault_rul_maintenance_industrial,"['fault', 'rul', 'maintenance', 'industrial', 'fault diagnosis', 'data', 'faults', 'equipment', 'failure', 'fault detection']","['predictive maintenance', 'fault diagnosis', 'fault detection', 'condition monitoring', 'deep learning', 'models', 'machine learning', 'neural network', 'data driven', 'transfer learning']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['  In Prognostics and Health Management (PHM) sufficient prior observed\ndegradation data is usually critical for Remaining Useful Lifetime (RUL)\nprediction. Most previous data-driven prediction methods assume that training\n(source) and testing (target) condition monitoring data have similar\ndistributions. However, due to different operating conditions, fault modes,\nnoise and equipment updates distribution shift exists across different data\ndomains. This shift reduces the performance of predictive models previously\nbuilt to specific conditions when no observed run-to-failure data is available\nfor retraining. To address this issue, this paper proposes a new data-driven\napproach for domain adaptation in prognostics using Long Short-Term Neural\nNetworks (LSTM). We use a time window approach to extract temporal information\nfrom time-series data in a source domain with observed RUL values and a target\ndomain containing only sensor information. We propose a Domain Adversarial\nNeural Network (DANN) approach to learn domain-invariant features that can be\nused to predict the RUL in the target domain. The experimental results show\nthat the proposed method can provide more reliable RUL predictions under\ndatasets with different operating conditions and fault modes. These results\nsuggest that the proposed method offers a promising approach to performing\ndomain adaptation in practical PHM applications.\n', ""  The monitoring of rotating machinery is an essential task in today's\nproduction processes. Currently, several machine learning and deep\nlearning-based modules have achieved excellent results in fault detection and\ndiagnosis. Nevertheless, to further increase user adoption and diffusion of\nsuch technologies, users and human experts must be provided with explanations\nand insights by the modules. Another issue is related, in most cases, with the\nunavailability of labeled historical data that makes the use of supervised\nmodels unfeasible. Therefore, a new approach for fault detection and diagnosis\nin rotating machinery is here proposed. The methodology consists of three\nparts: feature extraction, fault detection and fault diagnosis. In the first\npart, the vibration features in the time and frequency domains are extracted.\nSecondly, in the fault detection, the presence of fault is verified in an\nunsupervised manner based on anomaly detection algorithms. The modularity of\nthe methodology allows different algorithms to be implemented. Finally, in\nfault diagnosis, Shapley Additive Explanations (SHAP), a technique to interpret\nblack-box models, is used. Through the feature importance ranking obtained by\nthe model explainability, the fault diagnosis is performed. Two tools for\ndiagnosis are proposed, namely: unsupervised classification and root cause\nanalysis. The effectiveness of the proposed approach is shown on three datasets\ncontaining different mechanical faults in rotating machinery. The study also\npresents a comparison between models used in machine learning explainability:\nSHAP and Local Depth-based Feature Importance for the Isolation Forest (Local-\nDIFFI). Lastly, an analysis of several state-of-art anomaly detection\nalgorithms in rotating machinery is included.\n"", '  Data-driven fault classification is complicated by imbalanced training data\nand unknown fault classes. Fault diagnosis of dynamic systems is done by\ndetecting changes in time-series data, for example residuals, caused by faults\nor system degradation. Different fault classes can result in similar residual\noutputs, especially for small faults which can be difficult to distinguish from\nnominal system operation. Analyzing how easy it is to distinguish data from\ndifferent fault classes is crucial during the design process of a diagnosis\nsystem to evaluate if classification performance requirements can be met. Here,\na data-driven model of different fault classes is used based on the\nKullback-Leibler divergence. This is used to develop a framework for\nquantitative fault diagnosis performance analysis and open set fault\nclassification. A data-driven fault classification algorithm is proposed which\ncan handle unknown faults and also estimate the fault size using training data\nfrom known fault scenarios. To illustrate the usefulness of the proposed\nmethods, data have been collected from an engine test bench to illustrate the\ndesign process of a data-driven diagnosis system, including quantitative fault\ndiagnosis analysis and evaluation of the developed open set fault\nclassification algorithm.\n']"
86,168,86_face_facial_face recognition_recognition,"['face', 'facial', 'face recognition', 'recognition', 'faces', 'gender', 'age', 'images', 'identity', 'image']","['facial recognition', 'face verification', 'face recognition', 'facial images', 'face images', 'face image', 'face analysis', 'face detection', '3d face', 'faces']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['Face Recognition', '', '', '', '', '', '', '', '', '']","['  Face occlusions, covering either the majority or discriminative parts of the\nface, can break facial perception and produce a drastic loss of information.\nBiometric systems such as recent deep face recognition models are not immune to\nobstructions or other objects covering parts of the face. While most of the\ncurrent face recognition methods are not optimized to handle occlusions, there\nhave been a few attempts to improve robustness directly in the training stage.\nUnlike those, we propose to study the effect of generative face completion on\nthe recognition. We offer a face completion encoder-decoder, based on a\nconvolutional operator with a gating mechanism, trained with an ample set of\nface occlusions. To systematically evaluate the impact of realistic occlusions\non recognition, we propose to play the occlusion game: we render 3D objects\nonto different face parts, providing precious knowledge of what the impact is\nof effectively removing those occlusions. Extensive experiments on the Labeled\nFaces in the Wild (LFW), and its more difficult variant LFW-BLUFR, testify that\nface completion is able to partially restore face perception in machine vision\nsystems for improved recognition.\n', ""  Recently, Convolutional Neural Networks (CNNs) have achieved tremendous\nperformances on face recognition, and one popular perspective regarding CNNs'\nsuccess is that CNNs could learn discriminative face representations from face\nimages with complex image feature encoding. However, it is still unclear what\nis the intrinsic mechanism of face representation in CNNs. In this work, we\ninvestigate this problem by formulating face images as points in a\nshape-appearance parameter space, and our results demonstrate that: (i) The\nencoding and decoding of the neuron responses (representations) to face images\nin CNNs could be achieved under a linear model in the parameter space, in\nagreement with the recent discovery in primate IT face neurons, but different\nfrom the aforementioned perspective on CNNs' face representation with complex\nimage feature encoding; (ii) The linear model for face encoding and decoding in\nthe parameter space could achieve close or even better performances on face\nrecognition and verification than state-of-the-art CNNs, which might provide\nnew lights on the design strategies for face recognition systems; (iii) The\nneuron responses to face images in CNNs could not be adequately modelled by the\naxis model, a model recently proposed on face modelling in primate IT cortex.\nAll these results might shed some lights on the often complained blackbox\nnature behind CNNs' tremendous performances on face recognition.\n"", '  Despite the remarkable progress in face recognition related technologies,\nreliably recognizing faces across ages still remains a big challenge. The\nappearance of a human face changes substantially over time, resulting in\nsignificant intra-class variations. As opposed to current techniques for\nage-invariant face recognition, which either directly extract age-invariant\nfeatures for recognition, or first synthesize a face that matches target age\nbefore feature extraction, we argue that it is more desirable to perform both\ntasks jointly so that they can leverage each other. To this end, we propose a\ndeep Age-Invariant Model (AIM) for face recognition in the wild with three\ndistinct novelties. First, AIM presents a novel unified deep architecture\njointly performing cross-age face synthesis and recognition in a mutual\nboosting way. Second, AIM achieves continuous face rejuvenation/aging with\nremarkable photorealistic and identity-preserving properties, avoiding the\nrequirement of paired data and the true age of testing samples. Third, we\ndevelop effective and novel training strategies for end-to-end learning the\nwhole deep architecture, which generates powerful age-invariant face\nrepresentations explicitly disentangled from the age variation. Moreover, we\npropose a new large-scale Cross-Age Face Recognition (CAFR) benchmark dataset\nto facilitate existing efforts and push the frontiers of age-invariant face\nrecognition research. Extensive experiments on both our CAFR and several other\ncross-age datasets (MORPH, CACD and FG-NET) demonstrate the superiority of the\nproposed AIM model over the state-of-the-arts. Benchmarking our model on one of\nthe most popular unconstrained face recognition datasets IJB-C additionally\nverifies the promising generalizability of AIM in recognizing faces in the\nwild.\n']"
87,166,87_transport_optimal transport_ot_wasserstein,"['transport', 'optimal transport', 'ot', 'wasserstein', 'optimal', 'distance', 'sinkhorn', 'wasserstein distance', 'barycenter', 'distributions']","['optimal transport', 'entropic regularization', 'divergence', 'gaussian measures', 'regularization', 'optimization', 'sinkhorn algorithm', 'transport problem', 'stochastic', 'transport maps']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['Optimal transport', '', '', '', '', '', '', '', '', '']","['  Mini-batch optimal transport (m-OT) has been successfully used in practical\napplications that involve probability measures with a very high number of\nsupports. The m-OT solves several smaller optimal transport problems and then\nreturns the average of their costs and transportation plans. Despite its\nscalability advantage, the m-OT does not consider the relationship between\nmini-batches which leads to undesirable estimation. Moreover, the m-OT does not\napproximate a proper metric between probability measures since the identity\nproperty is not satisfied. To address these problems, we propose a novel\nmini-batch scheme for optimal transport, named Batch of Mini-batches Optimal\nTransport (BoMb-OT), that finds the optimal coupling between mini-batches and\nit can be seen as an approximation to a well-defined distance on the space of\nprobability measures. Furthermore, we show that the m-OT is a limit of the\nentropic regularized version of the BoMb-OT when the regularized parameter goes\nto infinity. Finally, we carry out experiments on various applications\nincluding deep generative models, deep domain adaptation, approximate Bayesian\ncomputation, color transfer, and gradient flow to show that the BoMb-OT can be\nwidely applied and performs well in various applications.\n', '  Optimal Transport (OT) naturally arises in many machine learning\napplications, yet the heavy computational burden limits its wide-spread uses.\nTo address the scalability issue, we propose an implicit generative\nlearning-based framework called SPOT (Scalable Push-forward of Optimal\nTransport). Specifically, we approximate the optimal transport plan by a\npushforward of a reference distribution, and cast the optimal transport problem\ninto a minimax problem. We then can solve OT problems efficiently using primal\ndual stochastic gradient-type algorithms. We also show that we can recover the\ndensity of the optimal transport plan using neural ordinary differential\nequations. Numerical experiments on both synthetic and real datasets illustrate\nthat SPOT is robust and has favorable convergence behavior. SPOT also allows us\nto efficiently sample from the optimal transport plan, which benefits\ndownstream applications such as domain adaptation.\n', '  Optimal transport induces the Earth Mover\'s (Wasserstein) distance between\nprobability distributions, a geometric divergence that is relevant to a wide\nrange of problems. Over the last decade, two relaxations of optimal transport\nhave been studied in depth: unbalanced transport, which is robust to the\npresence of outliers and can be used when distributions don\'t have the same\ntotal mass; entropy-regularized transport, which is robust to sampling noise\nand lends itself to fast computations using the Sinkhorn algorithm. This paper\ncombines both lines of work to put robust optimal transport on solid ground.\nOur main contribution is a generalization of the Sinkhorn algorithm to\nunbalanced transport: our method alternates between the standard Sinkhorn\nupdates and the pointwise application of a contractive function. This implies\nthat entropic transport solvers on grid images, point clouds and sampled\ndistributions can all be modified easily to support unbalanced transport, with\na proof of linear convergence that holds in all settings. We then show how to\nuse this method to define pseudo-distances on the full space of positive\nmeasures that satisfy key geometric axioms: (unbalanced) Sinkhorn divergences\nare differentiable, positive, definite, convex, statistically robust and avoid\nany ""entropic bias"" towards a shrinkage of the measures\' supports.\n']"
88,163,88_handwritten_text_character_recognition,"['handwritten', 'text', 'character', 'recognition', 'characters', 'handwriting', 'ocr', 'documents', 'dataset', 'character recognition']","['handwriting recognition', 'character recognition', 'handwritten text', 'recognition ocr', 'text recognition', 'ocr', 'handwritten', 'handwriting', 'text detection', 'writer identification']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['Bangla character recognition', '', '', '', '', '', '', '', '', '']","['  Handling large corpuses of documents is of significant importance in many\nfields, no more so than in the areas of crime investigation and defence, where\nan organisation may be presented with a large volume of scanned documents which\nneed to be processed in a finite time. However, this problem is exacerbated\nboth by the volume, in terms of scanned documents and the complexity of the\npages, which need to be processed. Often containing many different elements,\nwhich each need to be processed and understood. Text recognition, which is a\nprimary task of this process, is usually dependent upon the type of text, being\neither handwritten or machine-printed. Accordingly, the recognition involves\nprior classification of the text category, before deciding on the recognition\nmethod to be applied. This poses a more challenging task if a document contains\nboth handwritten and machine-printed text. In this work, we present a generic\nprocess flow for text recognition in scanned documents containing mixed\nhandwritten and machine-printed text without the need to classify text in\nadvance. We realize the proposed process flow using several open-source image\nprocessing and text recognition packages1. The evaluation is performed using a\nspecially developed variant, presented in this work, of the IAM handwriting\ndatabase, where we achieve an average transcription accuracy of nearly 80% for\npages containing both printed and handwritten text.\n', '  A novel approach for recognition of handwritten compound Bangla characters,\nalong with the Basic characters of Bangla alphabet, is presented here. Compared\nto English like Roman script, one of the major stumbling blocks in Optical\nCharacter Recognition (OCR) of handwritten Bangla script is the large number of\ncomplex shaped character classes of Bangla alphabet. In addition to 50 basic\ncharacter classes, there are nearly 160 complex shaped compound character\nclasses in Bangla alphabet. Dealing with such a large varieties of handwritten\ncharacters with a suitably designed feature set is a challenging problem.\nUncertainty and imprecision are inherent in handwritten script. Moreover, such\na large varieties of complex shaped characters, some of which have close\nresemblance, makes the problem of OCR of handwritten Bangla characters more\ndifficult. Considering the complexity of the problem, the present approach\nmakes an attempt to identify compound character classes from most frequently to\nless frequently occurred ones, i.e., in order of importance. This is to develop\na frame work for incrementally increasing the number of learned classes of\ncompound characters from more frequently occurred ones to less frequently\noccurred ones along with Basic characters. On experimentation, the technique is\nobserved produce an average recognition rate of 79.25 after three fold cross\nvalidation of data with future scope of improvement and extension.\n', '  Optical Character Recognition (OCR) is the process of extracting digitized\ntext from images of scanned documents. While OCR systems have already matured\nin many languages, they still have shortcomings in cursive languages with\noverlapping letters such as the Arabic language. This paper proposes a complete\nArabic OCR system that takes a scanned image of Arabic Naskh script as an input\nand generates a corresponding digital document. Our Arabic OCR system consists\nof the following modules: Pre-processing, Word-level Feature Extraction,\nCharacter Segmentation, Character Recognition, and Post-processing. This paper\nalso proposes an improved font-independent character segmentation algorithm\nthat outperforms the state-of-the-art segmentation algorithms. Lastly, the\npaper proposes a neural network model for the character recognition task. The\nsystem has experimented on several open Arabic corpora datasets with an average\ncharacter segmentation accuracy 98.06%, character recognition accuracy 99.89%,\nand overall system accuracy 97.94% achieving outstanding results compared to\nthe state-of-the-art Arabic OCR systems.\n']"
89,159,89_pricing_revenue_price_auctions,"['pricing', 'revenue', 'price', 'auctions', 'regret', 'seller', 'auction', 'products', 'demand', 'prices']","['dynamic pricing', 'choice model', 'price auctions', 'assortment planning', 'auction design', 'multinomial logit', 'allocation', 'pricing policy', 'expected revenue', 'model']","['Science/Tech', '', '', '', '', '', '', '', '', '']","['A model for dynamic pricing', '', '', '', '', '', '', '', '', '']","['  We consider a dynamic pricing problem for repeated contextual second-price\nauctions with strategic buyers whose goals are to maximize their long-term time\ndiscounted utility. The seller has very limited information about buyers\'\noverall demand curves, which depends on $d$-dimensional context vectors\ncharacterizing auctioned items, and a non-parametric market noise distribution\nthat captures buyers\' idiosyncratic tastes. The noise distribution and the\nrelationship between the context vectors and buyers\' demand curves are both\nunknown to the seller. We focus on designing the seller\'s learning policy to\nset contextual reserve prices where the seller\'s goal is to minimize his regret\nfor revenue. We first propose a pricing policy when buyers are truthful and\nshow that it achieves a $T$-period regret bound of\n$\\tilde{\\mathcal{O}}(\\sqrt{dT})$ against a clairvoyant policy that has full\ninformation of the buyers\' demand. Next, under the setting where buyers bid\nstrategically to maximize their long-term discounted utility, we develop a\nvariant of our first policy that is robust to strategic (corrupted) bids. This\npolicy incorporates randomized ""isolation"" periods, during which a buyer is\nrandomly chosen to solely participate in the auction. We show that this design\nallows the seller to control the number of periods in which buyers\nsignificantly corrupt their bids. Because of this nice property, our robust\npolicy enjoys a $T$-period regret of $\\tilde{\\mathcal{O}}(\\sqrt{dT})$, matching\nthat under the truthful setting up to a constant factor that depends on the\nutility discount factor.\n', ""  Motivated by pricing in ad exchange markets, we consider the problem of\nrobust learning of reserve prices against strategic buyers in repeated\ncontextual second-price auctions. Buyers' valuations for an item depend on the\ncontext that describes the item. However, the seller is not aware of the\nrelationship between the context and buyers' valuations, i.e., buyers'\npreferences. The seller's goal is to design a learning policy to set reserve\nprices via observing the past sales data, and her objective is to minimize her\nregret for revenue, where the regret is computed against a clairvoyant policy\nthat knows buyers' heterogeneous preferences. Given the seller's goal,\nutility-maximizing buyers have the incentive to bid untruthfully in order to\nmanipulate the seller's learning policy. We propose learning policies that are\nrobust to such strategic behavior. These policies use the outcomes of the\nauctions, rather than the submitted bids, to estimate the preferences while\ncontrolling the long-term effect of the outcome of each auction on the future\nreserve prices. When the market noise distribution is known to the seller, we\npropose a policy called Contextual Robust Pricing (CORP) that achieves a\nT-period regret of $O(d\\log(Td) \\log (T))$, where $d$ is the dimension of {the}\ncontextual information. When the market noise distribution is unknown to the\nseller, we propose two policies whose regrets are sublinear in $T$.\n"", ""  We consider the problem of multi-product dynamic pricing, in a contextual\nsetting, for a seller of differentiated products. In this environment, the\ncustomers arrive over time and products are described by high-dimensional\nfeature vectors. Each customer chooses a product according to the widely used\nMultinomial Logit (MNL) choice model and her utility depends on the product\nfeatures as well as the prices offered. The seller a-priori does not know the\nparameters of the choice model but can learn them through interactions with\ncustomers. The seller's goal is to design a pricing policy that maximizes her\ncumulative revenue. This model is motivated by online marketplaces such as\nAirbnb platform and online advertising. We measure the performance of a pricing\npolicy in terms of regret, which is the expected revenue loss with respect to a\nclairvoyant policy that knows the parameters of the choice model in advance and\nalways sets the revenue-maximizing prices. We propose a pricing policy, named\nM3P, that achieves a $T$-period regret of $O(\\log(Td) ( \\sqrt{T}+ d\\log(T)))$\nunder heterogeneous price sensitivity for products with features of dimension\n$d$. We also use tools from information theory to prove that no policy can\nachieve worst-case $T$-regret better than $\\Omega(\\sqrt{T})$.\n""]"
